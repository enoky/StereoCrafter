diff --git a/README.md b/README.md
deleted file mode 100644
index e46d100..0000000
--- a/README.md
+++ /dev/null
@@ -1,190 +0,0 @@
-# StereoCrafter GUI + DepthCrafter GUI Seg
-
-You can learn more about DepthCrafter GUI Seg <a href="https://github.com/Billynom8/DepthCrafter_GUI_Seg">here</a>.
-
-## Installation
-
-### Option 1: Installer script (Windows)
-
-#### PREREQUISITES:
-   - GIT: Ensure Git is installed and added to your system‚Äôs PATH.<br>
-     Download here: https://git-scm.com/downloads/win<br>
-     You can check the installation by running the command:<br>
-       `git --version`<br>
-     If it shows a version, Git is installed and on PATH.
-   
-   - PYTHON: Ensure Python 3.12 is installed and added to your PATH.<br>
-     Download here: https://www.python.org/downloads<br>
-     You can check by running the command:<br>
-       `python --version`
-   - CUDA ToolKit: Ensure CUDA 12.8 is installed and added to your PATH.<br>
-     Download here: https://developer.nvidia.com/cuda-12-8-0-download-archive?target_os=Windows&target_arch=x86_64<br>
-
-   - FFMPEG: Ensure FFMpeg is installed and added to your PATH.<br>
-     See [Here](https://techtactician.com/how-to-install-ffmpeg-and-add-it-to-path-on-windows/) for a tutorial on how to install.
-
-
-#### INSTALL:
-   - Run <a href="https://github.com/enoky/StereoCrafter/blob/90257ee4f032b649530281b0eceb95ea460db115/1click_installer_script/StereoCrafter_1click_Installer.bat">script</a> from folder where you want StereoCrafter installed
-   - Download and extract <a href="https://mega.nz/file/Fw1GgJrL#bPplu2Y1PT4G-TM29zcGNENUYVySEk2NENT4krkjEso">model</a> "weights" to StereoCrafter folder (use <a href="https://www.qbittorrent.org">qBittorrent</a> to download)
-
-<hr>
-
-### Option 2: Manual Install
-
-For Manual Install Instructions <a href="https://github.com/enoky/StereoCrafter/blob/55dce8ec0b8f9d1ddfd69bc1fbaf831f6473bba5/1click_installer_script/StereoCrafter_Manual_Install.md">Click Here</a>
-
-<hr>
-<div align="center">
-<h2>StereoCrafter: Diffusion-based Generation of Long and High-fidelity Stereoscopic 3D from Monocular Videos</h2>
-
-Sijie Zhao*&emsp;
-Wenbo Hu*&emsp;
-Xiaodong Cun*&emsp;
-Yong Zhang&dagger;&emsp;
-Xiaoyu Li&dagger;&emsp;<br>
-Zhe Kong&emsp;
-Xiangjun Gao&emsp;
-Muyao Niu&emsp;
-Ying Shan
-
-&emsp;* equal contribution &emsp; &dagger; corresponding author 
-
-<h3>Tencent AI Lab&emsp;&emsp;ARC Lab, Tencent PCG</h3>
-
-<a href='https://arxiv.org/abs/2409.07447'><img src='https://img.shields.io/badge/arXiv-PDF-a92225'></a> &emsp;
-<a href='https://stereocrafter.github.io/'><img src='https://img.shields.io/badge/Project_Page-Page-64fefe' alt='Project Page'></a> &emsp;
-<a href='https://huggingface.co/TencentARC/StereoCrafter'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Weights-yellow'></a>
-</div>
-
-## üí° Abstract
-
-We propose a novel framework to convert any 2D videos to immersive stereoscopic 3D ones that can be viewed on different display devices, like 3D Glasses, Apple Vision Pro and 3D Display. It can be applied to various video sources, such as movies, vlogs, 3D cartoons, and AIGC videos.
-
-![teaser](assets/teaser.jpg)
-
-## üì£ News
-- `2024/12/27` We released our inference code and model weights.
-- `2024/09/11` We submitted our technical report on arXiv and released our project page.
-
-## üéûÔ∏è Showcases
-Here we show some examples of input videos and their corresponding stereo outputs in Anaglyph 3D format.
-<div align="center">
-    <img src="assets/demo.gif">
-</div>
-
-
-## üõ†Ô∏è Installation
-
-#### 1. Set up the environment
-We run our code on Python 3.8 and Cuda 11.8.
-You can use Anaconda or Docker to build this basic environment.
-
-#### 2. Clone the repo
-```bash
-# use --recursive to clone the dependent submodules
-git clone --recursive https://github.com/TencentARC/StereoCrafter
-cd StereoCrafter
-```
-
-#### 3. Install the requirements
-```bash
-pip install -r requirements.txt
-```
-
-
-#### 4. Install customized 'Forward-Warp' package for forward splatting
-```
-cd ./dependency/Forward-Warp
-chmod a+x install.sh
-./install.sh
-```
-
-
-## üì¶ Model Weights
-
-#### 1. Download the [SVD img2vid model](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1) for the image encoder and VAE.
-
-```bash
-# in StereoCrafter project root directory
-mkdir weights
-cd ./weights
-git lfs install
-git clone https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1
-```
-
-#### 2. Download the [DepthCrafter model](https://huggingface.co/tencent/DepthCrafter) for the video depth estimation.
-```bash
-git clone https://huggingface.co/tencent/DepthCrafter
-```
-
-#### 3. Download the [StereoCrafter model](https://huggingface.co/TencentARC/StereoCrafter) for the stereo video generation.
-```bash
-git clone https://huggingface.co/TencentARC/StereoCrafter
-```
-
-
-## üîÑ Inference
-
-Script:
-
-```bash
-# in StereoCrafter project root directory
-sh run_inference.sh
-```
-
-There are two main steps in this script for generating stereo video.
-
-#### 1. Depth-Based Video Splatting Using the Video Depth from DepthCrafter
-Execute the following command:
-```bash
-python depth_splatting_inference.py --pre_trained_path [PATH] --unet_path [PATH]
-                                    --input_video_path [PATH] --output_video_path [PATH]
-```
-Arguments:
-- `--pre_trained_path`: Path to the SVD img2vid model weights (e.g., `./weights/stable-video-diffusion-img2vid-xt-1-1`).
-- `--unet_path`: Path to the DepthCrafter model weights (e.g., `./weights/DepthCrafter`).
-- `--input_video_path`: Path to the input video (e.g., `./source_video/camel.mp4`).
-- `--output_video_path`: Path to the output video (e.g., `./outputs/camel_splatting_results.mp4`).
-- `--max_disp`: Parameter controlling the maximum disparity between the generated right video and the input left video. Default value is `20` pixels.
-
-The first step generates a video grid with input video, visualized depth map, occlusion mask, and splatting right video, as shown below:
-
-<img src="assets/camel_splatting_results.jpg" alt="camel_splatting_results" width="800"/> 
-
-#### 2. Stereo Video Inpainting of the Splatting Video
-Execute the following command:
-```bash
-python inpainting_inference.py --pre_trained_path [PATH] --unet_path [PATH]
-                               --input_video_path [PATH] --save_dir [PATH]
-```
-Arguments:
-- `--pre_trained_path`: Path to the SVD img2vid model weights (e.g., `./weights/stable-video-diffusion-img2vid-xt-1-1`).
-- `--unet_path`: Path to the StereoCrafter model weights (e.g., `./weights/StereoCrafter`).
-- `--input_video_path`: Path to the splatting video result generated by the first stage (e.g., `./outputs/camel_splatting_results.mp4`).
-- `--save_dir`: Directory for the output stereo video (e.g., `./outputs`).
-- `--tile_num`: The number of tiles in width and height dimensions for tiled processing, which allows for handling high resolution input without requiring more GPU memory. The default value is `1` (1 $\times$ 1 tile). For input videos with a resolution of 2K or higher, you could use more tiles to avoid running out of memory.
-
-The stereo video inpainting generates the stereo video result in side-by-side format and anaglyph 3D format, as shown below:
-
-<img src="assets/camel_sbs.jpg" alt="camel_sbs" width="800"/> 
-
-<img src="assets/camel_anaglyph.jpg" alt="camel_anaglyph" width="400"/>
-
-## ü§ù Acknowledgements
-
-We would like to express our gratitude to the following open-source projects:
-- [Stable Video Diffusion](https://github.com/Stability-AI/generative-models): A latent diffusion model trained to generate video clips from an image or text conditioning.
-- [DepthCrafter](https://github.com/Tencent/DepthCrafter): A novel method to generate temporally consistent depth sequences from videos.
-
-
-## üìö Citation
-
-```bibtex
-@article{zhao2024stereocrafter,
-  title={Stereocrafter: Diffusion-based generation of long and high-fidelity stereoscopic 3d from monocular videos},
-  author={Zhao, Sijie and Hu, Wenbo and Cun, Xiaodong and Zhang, Yong and Li, Xiaoyu and Kong, Zhe and Gao, Xiangjun and Niu, Muyao and Shan, Ying},
-  journal={arXiv preprint arXiv:2409.07447},
-  year={2024}
-}
-```
diff --git a/_RUN_Splatting_GUI.bat b/_RUN_Splatting_GUI.bat
index d6e6881..9f34334 100644
--- a/_RUN_Splatting_GUI.bat
+++ b/_RUN_Splatting_GUI.bat
@@ -7,3 +7,4 @@ set "PATH=%CUDA_PATH%\bin;%CUDA_PATH%\libnvvp;%PATH%"
 
 call venv\scripts\activate.bat
 python splatting_gui.py
+pause
\ No newline at end of file
diff --git a/core/__init__.py b/core/__init__.py
new file mode 100644
index 0000000..8ec01ba
--- /dev/null
+++ b/core/__init__.py
@@ -0,0 +1,42 @@
+"""StereoCrafter core modules package.
+
+This package contains modularized components for the StereoCrafter
+2D-to-3D video conversion tool.
+"""
+
+from .common import (
+    ThemeManager,
+    DARK_COLORS,
+    LIGHT_COLORS,
+    VideoIO,
+    read_video_frames,
+)
+
+from .splatting import (
+    ConfigManager,
+    SPLATTER_DEFAULT_CONFIG,
+    load_config,
+    save_config,
+    compute_global_depth_stats,
+    load_pre_rendered_depth,
+    FFmpegDepthPipeReader,
+    ForwardWarpStereo,
+    FusionSidecarGenerator,
+)
+
+__all__ = [
+    'ConfigManager',
+    'SPLATTER_DEFAULT_CONFIG',
+    'load_config',
+    'save_config',
+    'ThemeManager',
+    'DARK_COLORS',
+    'LIGHT_COLORS',
+    'VideoIO',
+    'read_video_frames',
+    'compute_global_depth_stats',
+    'load_pre_rendered_depth',
+    'FFmpegDepthPipeReader',
+    'ForwardWarpStereo',
+    'FusionSidecarGenerator',
+]
diff --git a/core/common/__init__.py b/core/common/__init__.py
new file mode 100644
index 0000000..ad5af30
--- /dev/null
+++ b/core/common/__init__.py
@@ -0,0 +1,23 @@
+"""Shared utilities across all GUI applications.
+
+This package contains modules that are common to multiple GUI applications
+within the StereoCrafter project.
+"""
+
+from .video_io import VideoIO, read_video_frames
+
+from .theme_manager import (
+    ThemeManager,
+    DARK_COLORS,
+    LIGHT_COLORS,
+    get_theme_colors,
+)
+
+__all__ = [
+    'VideoIO',
+    'read_video_frames',
+    'ThemeManager',
+    'DARK_COLORS',
+    'LIGHT_COLORS',
+    'get_theme_colors',
+]
diff --git a/core/common/theme_manager.py b/core/common/theme_manager.py
new file mode 100644
index 0000000..b9d4652
--- /dev/null
+++ b/core/common/theme_manager.py
@@ -0,0 +1,277 @@
+"""Theme management for StereoCrafter GUI applications.
+
+Provides dark/light theme support with color palettes and styling utilities
+for tkinter-based applications.
+"""
+
+import logging
+from typing import TYPE_CHECKING, Any, Dict, List, Optional
+
+logger = logging.getLogger(__name__)
+
+
+# Dark theme color palette
+DARK_COLORS = {
+    "bg": "#2b2b2b",
+    "fg": "white",
+    "entry_bg": "#3c3c3c",
+    "menu_bg": "#3c3c3c",
+    "menu_fg": "white",
+    "active_bg": "#555555",
+    "active_fg": "white",
+    "tooltip_bg": "#4a4a4a",
+    "tooltip_fg": "white",
+    "theme": "black",
+}
+
+# Light theme color palette
+LIGHT_COLORS = {
+    "bg": "#d9d9d9",
+    "fg": "black",
+    "entry_bg": "#ffffff",
+    "menu_bg": "#f0f0f0",
+    "menu_fg": "black",
+    "active_bg": "#dddddd",
+    "active_fg": "black",
+    "tooltip_bg": "#ffffe0",
+    "tooltip_fg": "black",
+    "theme": "clam",
+}
+
+
+def get_theme_colors(is_dark: bool) -> Dict[str, str]:
+    """Get the color palette for the specified theme.
+
+    Args:
+        is_dark: True for dark theme, False for light theme
+
+    Returns:
+        Dictionary containing color values for the theme
+    """
+    return DARK_COLORS if is_dark else LIGHT_COLORS
+
+
+def is_dark_mode(config: Dict[str, Any]) -> bool:
+    """Check if dark mode is enabled in configuration.
+
+    Args:
+        config: Configuration dictionary
+
+    Returns:
+        True if dark mode is enabled
+    """
+    return bool(config.get("dark_mode_enabled", False))
+
+
+class ThemeManager:
+    """Manages application theming for dark/light mode switching.
+
+    Provides utilities for applying themes to tkinter widgets and
+    maintaining consistent styling across the application.
+
+    Args:
+        dark_mode_var: Optional tkinter BooleanVar tracking theme state
+        config: Optional configuration dictionary
+    """
+
+    def __init__(
+        self,
+        dark_mode_var: Optional["tk.BooleanVar"] = None,
+        config: Optional[Dict[str, Any]] = None,
+    ):
+        """Initialize the theme manager.
+
+        Args:
+            dark_mode_var: Tkinter BooleanVar tracking dark mode state
+            config: Configuration dictionary
+        """
+        self.dark_mode_var = dark_mode_var
+        self.config = config or {}
+        
+    def is_dark_mode(self) -> bool:
+        """Check if dark mode is currently enabled.
+
+        Returns:
+            True if dark mode is enabled
+        """
+        if self.dark_mode_var:
+            return bool(self.dark_mode_var.get())
+        return is_dark_mode(self.config)
+    
+    def get_colors(self) -> Dict[str, str]:
+        """Get the current theme colors.
+
+        Returns:
+            Dictionary containing current color palette
+        """
+        return get_theme_colors(self.is_dark_mode())
+    
+    def apply_theme_to_style(
+        self,
+        style: "ttk.Style",
+        root_window: Optional["tk.Tk"] = None,
+    ) -> None:
+        """Apply the current theme to ttk styles.
+
+        Args:
+            style: ttk.Style object to configure
+            root_window: Optional root window for bg configuration
+        """
+        colors = self.get_colors()
+
+        # Apply theme to root window if provided
+        if root_window:
+            root_window.configure(bg=colors["bg"])
+            # Use set_theme if it's a ThemedTk instance, otherwise theme_use
+            if hasattr(root_window, "set_theme"):
+                try:
+                    root_window.set_theme(colors["theme"])
+                except Exception as e:
+                    logger.warning(f"Failed to set theme via set_theme: {e}")
+                    style.theme_use(colors["theme"])
+            else:
+                try:
+                    style.theme_use(colors["theme"])
+                except Exception as e:
+                    logger.warning(f"Failed to set theme via theme_use: {e}")
+
+            # Crucial: let the theme switch take effect before configuring styles
+            root_window.update_idletasks()
+        
+        # Configure basic styles for the current theme
+        # We apply to common prefixes to be thorough
+        bg = colors["bg"]
+        fg = colors["fg"]
+        entry_bg = colors["entry_bg"]
+
+        for style_name in ["TFrame", "TLabelframe", "TLabel", "TCheckbutton", "TRadiobutton"]:
+            style.configure(style_name, background=bg, foreground=fg)
+        
+        style.configure("TLabelframe.Label", background=bg, foreground=fg)
+        
+        # Configure style maps for interactive widgets
+        style.map(
+            "TCheckbutton",
+            foreground=[("active", fg), ("!disabled", fg)],
+            background=[("active", bg), ("!disabled", bg)],
+        )
+        style.map(
+            "TRadiobutton",
+            foreground=[("active", fg), ("!disabled", fg)],
+            background=[("active", bg), ("!disabled", bg)],
+        )
+        
+        # Configure Entry and Combobox
+        # Some themes require explicit state mappings to override native looks
+        style.map(
+            "TEntry",
+            fieldbackground=[("!disabled", entry_bg), ("focus", entry_bg)],
+            foreground=[("!disabled", fg), ("focus", fg)],
+        )
+        style.configure("TEntry", insertcolor=fg)
+        
+        style.map(
+            "TCombobox",
+            fieldbackground=[("readonly", entry_bg), ("!disabled", entry_bg), ("focus", entry_bg)],
+            foreground=[("readonly", fg), ("!disabled", fg), ("focus", fg)],
+            selectbackground=[("readonly", entry_bg)],
+            selectforeground=[("readonly", fg)],
+        )
+
+        # Force a final update to catch any delayed renders
+        if root_window:
+            root_window.update_idletasks()
+    
+    def apply_theme_to_menus(
+        self,
+        menus: list,
+        menubar: Optional["tk.Menu"] = None,
+    ) -> None:
+        """Apply theme colors to menu widgets.
+
+        Args:
+            menus: List of tk.Menu widgets to configure
+            menubar: Optional menubar widget
+        """
+        colors = self.get_colors()
+        
+        all_menus = list(menus)
+        if menubar:
+            all_menus.append(menubar)
+        
+        for menu in all_menus:
+            try:
+                menu.config(
+                    bg=colors["menu_bg"],
+                    fg=colors["menu_fg"],
+                    activebackground=colors["active_bg"],
+                    activeforeground=colors["active_fg"],
+                )
+            except Exception:
+                pass
+    
+    def apply_theme_to_labels(
+        self,
+        labels: list,
+        bg: Optional[str] = None,
+        fg: Optional[str] = None,
+    ) -> None:
+        """Apply theme colors to label widgets.
+
+        Args:
+            labels: List of tk.Label widgets to configure
+            bg: Optional background color override
+            fg: Optional foreground color override
+        """
+        colors = self.get_colors()
+        
+        for label in labels:
+            try:
+                if bg:
+                    label.config(bg=bg)
+                else:
+                    label.config(bg=colors["bg"])
+                if fg:
+                    label.config(fg=fg)
+                else:
+                    label.config(fg=colors["fg"])
+            except Exception:
+                pass
+    
+    def apply_theme_to_canvas(
+        self,
+        canvas: "tk.Canvas",
+    ) -> None:
+        """Apply theme background color to a canvas widget.
+
+        Args:
+            canvas: tk.Canvas widget to configure
+        """
+        colors = self.get_colors()
+        try:
+            canvas.config(bg=colors["bg"], highlightthickness=0)
+        except Exception:
+            pass
+    
+    def get_style_config(self) -> Dict[str, str]:
+        """Get the current style configuration for persistence.
+
+        Returns:
+            Dictionary containing style-relevant configuration
+        """
+        colors = self.get_colors()
+        return {
+            "dark_mode_enabled": self.is_dark_mode(),
+            "theme_bg": colors["bg"],
+            "theme_fg": colors["fg"],
+            "theme_entry_bg": colors["entry_bg"],
+        }
+    
+    @staticmethod
+    def get_available_themes() -> list:
+        """Get list of available theme names.
+
+        Returns:
+            List of theme names
+        """
+        return ["default", "black", "clam", "alt", "classic"]
diff --git a/core/common/video_io.py b/core/common/video_io.py
new file mode 100644
index 0000000..162b12e
--- /dev/null
+++ b/core/common/video_io.py
@@ -0,0 +1,192 @@
+"""Video I/O utilities for StereoCrafter.
+
+Provides video reading and frame extraction utilities using decord
+for efficient video loading.
+"""
+
+import logging
+from typing import Any, Optional, Tuple, Union
+
+import numpy as np
+from decord import VideoReader, cpu
+
+logger = logging.getLogger(__name__)
+
+
+class _NumpyBatch:
+    """Minimal wrapper to match Decord's get_batch(...).asnumpy() API."""
+
+    def __init__(self, arr: np.ndarray):
+        """Initialize with a numpy array.
+
+        Args:
+            arr: Numpy array to wrap
+        """
+        self._arr = arr
+
+    def asnumpy(self) -> np.ndarray:
+        """Return the underlying numpy array.
+
+        Returns:
+            The wrapped numpy array
+        """
+        return self._arr
+
+
+class VideoIO:
+    """Video input/output operations for video processing."""
+
+    @staticmethod
+    def read_video_info(
+        video_path: str,
+    ) -> Tuple[int, int, int, float]:
+        """Read video information without loading frames.
+
+        Args:
+            video_path: Path to the video file
+
+        Returns:
+            Tuple of (total_frames, height, width, fps)
+        """
+        logger.debug(f"==> Reading video info: {video_path}")
+        reader = VideoReader(video_path, ctx=cpu(0))
+        total_frames = len(reader)
+        first_frame = reader.get_batch([0]).asnumpy()
+        height, width = first_frame.shape[1:3]
+        fps = float(reader.get_avg_fps())
+
+        logger.debug(
+            f"==> Video info: {total_frames} frames, {width}x{height}, {fps} fps"
+        )
+
+        return total_frames, height, width, fps
+
+    @staticmethod
+    def read_frame(
+        reader: VideoReader,
+        index: int,
+    ) -> np.ndarray:
+        """Read a single frame from a video reader.
+
+        Args:
+            reader: Active VideoReader instance
+            index: Frame index to read
+
+        Returns:
+            Frame as numpy array [H, W, C]
+        """
+        return reader.get_batch([index]).asnumpy()[0]
+
+    @staticmethod
+    def read_frames_batch(
+        reader: VideoReader,
+        indices: list,
+    ) -> np.ndarray:
+        """Read multiple frames from a video reader.
+
+        Args:
+            reader: Active VideoReader instance
+            indices: List of frame indices to read
+
+        Returns:
+            Frames as numpy array [N, H, W, C]
+        """
+        return reader.get_batch(indices).asnumpy()
+
+
+def read_video_frames(
+    video_path: str,
+    process_length: int,
+    set_pre_res: bool,
+    pre_res_width: int,
+    pre_res_height: int,
+    dataset: str = "open",
+) -> Tuple[VideoReader, float, int, int, int, int, Optional[dict], int]:
+    """Initialize a VideoReader for chunked reading.
+
+    Args:
+        video_path: Path to the video file
+        process_length: Number of frames to process (-1 for all)
+        set_pre_res: Whether to set custom resolution
+        pre_res_width: Target width if set_pre_res is True
+        pre_res_height: Target height if set_pre_res is True
+        dataset: Dataset type (only 'open' supported)
+
+    Returns:
+        Tuple of (video_reader, fps, original_height, original_width,
+                  actual_processed_height, actual_processed_width,
+                  video_stream_info, total_frames_to_process)
+
+    Raises:
+        NotImplementedError: If dataset is not 'open'
+    """
+    if dataset == "open":
+        logger.debug(f"==> Initializing VideoReader for: {video_path}")
+        vid_info_only = VideoReader(
+            video_path, ctx=cpu(0)
+        )  # Use separate reader for info
+        original_height, original_width = vid_info_only.get_batch([0]).shape[1:3]
+        total_frames_original = len(vid_info_only)
+        logger.debug(
+            f"==> Original video shape: {total_frames_original} frames, "
+            f"{original_height}x{original_width} per frame"
+        )
+
+        height_for_reader = original_height
+        width_for_reader = original_width
+
+        if set_pre_res and pre_res_width > 0 and pre_res_height > 0:
+            height_for_reader = pre_res_height
+            width_for_reader = pre_res_width
+            logger.debug(
+                f"==> Pre-processing resolution set to: "
+                f"{width_for_reader}x{height_for_reader}"
+            )
+        else:
+            logger.debug(
+                f"==> Using original video resolution for reading: "
+                f"{width_for_reader}x{height_for_reader}"
+            )
+
+    else:
+        raise NotImplementedError(f"Dataset '{dataset}' not supported.")
+
+    # decord automatically resizes if width/height are passed to VideoReader
+    video_reader = VideoReader(
+        video_path, ctx=cpu(0), width=width_for_reader, height=height_for_reader
+    )
+
+    # Verify the actual shape after Decord processing, using the first frame
+    first_frame_shape = video_reader.get_batch([0]).shape
+    actual_processed_height, actual_processed_width = first_frame_shape[1:3]
+
+    fps = video_reader.get_avg_fps()  # Use actual FPS from the reader
+
+    total_frames_available = len(video_reader)
+    total_frames_to_process = total_frames_available  # Use available frames directly
+    if process_length != -1 and process_length < total_frames_available:
+        total_frames_to_process = process_length
+
+    logger.debug(
+        f"==> VideoReader initialized. Final processing dimensions: "
+        f"{actual_processed_width}x{actual_processed_height}. "
+        f"Total frames for processing: {total_frames_to_process}"
+    )
+
+    # Import here to avoid circular dependency
+    from dependency.stereocrafter_util import get_video_stream_info
+
+    video_stream_info = get_video_stream_info(
+        video_path
+    )  # Get stream info for FFmpeg later
+
+    return (
+        video_reader,
+        fps,
+        original_height,
+        original_width,
+        actual_processed_height,
+        actual_processed_width,
+        video_stream_info,
+        total_frames_to_process,
+    )
diff --git a/core/splatting/__init__.py b/core/splatting/__init__.py
new file mode 100644
index 0000000..d945462
--- /dev/null
+++ b/core/splatting/__init__.py
@@ -0,0 +1,103 @@
+"""Splatting GUI core modules.
+
+This package contains the modularized components of the Splatting GUI,
+organized by functionality.
+"""
+
+from .batch_processing import (
+    BatchProcessor,
+    ProcessingTask,
+    ProcessingSettings,
+    BatchSetupResult,
+)
+
+from .border_scanning import BorderScanner
+
+from .config_manager import (
+    ConfigManager,
+    SPLATTER_DEFAULT_CONFIG,
+    load_config,
+    save_config,
+    load_settings_from_file,
+    save_settings_to_file,
+    get_current_config,
+    reset_to_defaults,
+)
+
+from .convergence import ConvergenceEstimatorWrapper
+
+from .depth_processing import (
+    compute_global_depth_stats,
+    load_pre_rendered_depth,
+    FFmpegDepthPipeReader,
+    DEPTH_VIS_TV10_BLACK_NORM,
+    DEPTH_VIS_TV10_WHITE_NORM,
+)
+
+from .forward_warp import ForwardWarpStereo
+
+from .fusion_export import FusionSidecarGenerator
+
+from .preview_rendering import PreviewRenderer
+
+from .gui_widgets import (
+    Tooltip,
+    create_tooltip,
+    create_labeled_entry,
+    create_folder_selection_row,
+    create_checkbox_group,
+    create_labeled_slider,
+    create_button_group,
+    create_section_frame,
+    create_dropdown,
+    configure_grid_weights,
+    get_common_tooltip,
+    COMMON_TOOLTIPS,
+)
+
+from .main_gui import SplatterGUI
+
+__all__ = [
+    # Config Management
+    'ConfigManager',
+    'SPLATTER_DEFAULT_CONFIG',
+    'load_config',
+    'save_config',
+    'load_settings_from_file',
+    'save_settings_to_file',
+    'get_current_config',
+    'reset_to_defaults',
+    # Border & Convergence
+    'BorderScanner',
+    'ConvergenceEstimatorWrapper',
+    # Depth Processing
+    'compute_global_depth_stats',
+    'load_pre_rendered_depth',
+    'FFmpegDepthPipeReader',
+    'DEPTH_VIS_TV10_BLACK_NORM',
+    'DEPTH_VIS_TV10_WHITE_NORM',
+    # Core Processing
+    'ForwardWarpStereo',
+    'FusionSidecarGenerator',
+    'PreviewRenderer',
+    # Batch Processing
+    'BatchProcessor',
+    'ProcessingTask',
+    'ProcessingSettings',
+    'BatchSetupResult',
+    # GUI Widgets
+    'Tooltip',
+    'create_tooltip',
+    'create_labeled_entry',
+    'create_folder_selection_row',
+    'create_checkbox_group',
+    'create_labeled_slider',
+    'create_button_group',
+    'create_section_frame',
+    'create_dropdown',
+    'configure_grid_weights',
+    'get_common_tooltip',
+    'COMMON_TOOLTIPS',
+    # Main GUI
+    'SplatterGUI',
+]
diff --git a/core/splatting/batch_processing.py b/core/splatting/batch_processing.py
new file mode 100644
index 0000000..136254a
--- /dev/null
+++ b/core/splatting/batch_processing.py
@@ -0,0 +1,748 @@
+"""Batch video processing module.
+
+Handles batch video splatting workflow including multi-resolution output,
+sidecar integration, auto-convergence, and move-to-finished functionality.
+"""
+
+import gc
+import glob
+import logging
+import os
+import queue
+import threading
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, List, Optional, Tuple
+
+import torch
+from decord import VideoReader, cpu
+
+from dependency.stereocrafter_util import (
+    get_video_stream_info,
+    release_cuda_memory,
+)
+from .depth_processing import (
+    DEPTH_VIS_TV10_BLACK_NORM,
+    DEPTH_VIS_TV10_WHITE_NORM,
+    FFmpegDepthPipeReader,
+    _infer_depth_bit_depth,
+    compute_global_depth_stats,
+    load_pre_rendered_depth,
+)
+from .render_processor import RenderProcessor
+from .convergence import ConvergenceEstimatorWrapper
+from core.common.video_io import read_video_frames
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class ProcessingTask:
+    """Represents a single video processing task configuration.
+
+    Attributes:
+        name: Task name (e.g., "Full-Resolution", "Low-Resolution")
+        output_subdir: Output subdirectory name
+        set_pre_res: Whether to set preprocessing resolution
+        target_width: Target output width (-1 for original)
+        target_height: Target output height (-1 for original)
+        batch_size: Frames per batch
+        is_low_res: Whether this is a low-resolution task
+    """
+
+    name: str
+    output_subdir: str
+    set_pre_res: bool
+    target_width: int
+    target_height: int
+    batch_size: int
+    is_low_res: bool
+
+
+@dataclass
+class ProcessingSettings:
+    """Processing settings for batch operations.
+
+    Attributes:
+        input_source_clips: Path to source clips folder or file
+        input_depth_maps: Path to depth maps folder or file
+        output_splatted: Output directory for splatted videos
+        max_disp: Maximum disparity percentage
+        process_length: Number of frames to process (-1 for all)
+        enable_full_resolution: Whether to output full resolution
+        full_res_batch_size: Batch size for full resolution
+        enable_low_resolution: Whether to output low resolution
+        low_res_width: Width for low resolution output
+        low_res_height: Height for low resolution output
+        low_res_batch_size: Batch size for low resolution
+        dual_output: Whether to output both left and right eyes
+        zero_disparity_anchor: Convergence plane value (0.0-1.0)
+        enable_global_norm: Whether to enable global normalization
+        match_depth_res: Whether to match depth resolution to video
+        move_to_finished: Whether to move processed files to finished folder
+        output_crf: Quality CRF value (legacy)
+        output_crf_full: CRF for full resolution
+        output_crf_low: CRF for low resolution
+        depth_gamma: Gamma correction for depth
+        depth_dilate_size_x: Horizontal dilation size
+        depth_dilate_size_y: Vertical dilation size
+        depth_blur_size_x: Horizontal blur size
+        depth_blur_size_y: Vertical blur size
+        depth_dilate_left: Left eye dilation
+        depth_blur_left: Left eye blur
+        auto_convergence_mode: Auto-convergence mode ("Off", "Average", "Peak", "Hybrid")
+        enable_sidecar_gamma: Whether sidecar controls gamma
+        enable_sidecar_blur_dilate: Whether sidecar controls blur/dilate
+        single_finished_source_folder: Optional finished folder for single-file mode
+        single_finished_depth_folder: Optional finished folder for single-file mode
+    """
+
+    input_source_clips: str
+    input_depth_maps: str
+    output_splatted: str
+    max_disp: float = 20.0
+    process_length: int = -1
+    enable_full_resolution: bool = True
+    full_res_batch_size: int = 10
+    enable_low_resolution: bool = False
+    low_res_width: int = 1920
+    low_res_height: int = 1080
+    low_res_batch_size: int = 50
+    dual_output: bool = False
+    zero_disparity_anchor: float = 0.5
+    enable_global_norm: bool = False
+    match_depth_res: bool = True
+    move_to_finished: bool = True
+    output_crf: int = 23
+    output_crf_full: int = 23
+    output_crf_low: int = 23
+    depth_gamma: float = 1.0
+    depth_dilate_size_x: float = 0.0
+    depth_dilate_size_y: float = 0.0
+    depth_blur_size_x: float = 0.0
+    depth_blur_size_y: float = 0.0
+    depth_dilate_left: float = 0.0
+    depth_blur_left: float = 0.0
+    depth_blur_left_mix: float = 0.5
+    auto_convergence_mode: str = "Off"
+    enable_sidecar_gamma: bool = True
+    enable_sidecar_blur_dilate: bool = True
+    single_finished_source_folder: Optional[str] = None
+    single_finished_depth_folder: Optional[str] = None
+    # NEW FIELDS for deeper orchestration
+    multi_map: bool = False
+    selected_depth_map: str = ""
+    color_tags_mode: str = "Auto"
+    is_test_mode: bool = False
+    test_target_frame_idx: Optional[int] = None
+    skip_lowres_preproc: bool = False
+    sidecar_ext: str = ".fssidecar"
+    sidecar_folder: str = ""
+    track_dp_total_true_on_render: bool = False
+
+
+@dataclass
+class BatchSetupResult:
+    """Result of batch processing setup.
+
+    Attributes:
+        input_videos: List of input video paths
+        is_single_file_mode: Whether running in single-file mode
+        finished_source_folder: Optional finished folder for source
+        finished_depth_folder: Optional finished folder for depth
+        error: Optional error message
+    """
+
+    input_videos: List[str] = field(default_factory=list)
+    is_single_file_mode: bool = False
+    finished_source_folder: Optional[str] = None
+    finished_depth_folder: Optional[str] = None
+    error: Optional[str] = None
+
+
+class BatchProcessor:
+    """Handles batch video splatting workflow.
+
+    Manages the processing queue, worker threads, and progress
+    reporting for batch video conversion. Coordinates multiple
+    processing tasks (Full/Low resolution) per video.
+    """
+
+    def __init__(
+        self,
+        progress_queue: queue.Queue,
+        stop_event: threading.Event,
+
+        sidecar_manager: Optional[Any] = None,
+    ):
+        """Initialize batch processor.
+
+        Args:
+            progress_queue: Queue for progress updates to GUI
+            stop_event: Event for cancellation
+
+            sidecar_manager: Optional sidecar manager for loading/saving settings
+        """
+        self.progress_queue = progress_queue
+        self.stop_event = stop_event
+
+        self.sidecar_manager = sidecar_manager
+        self.logger = logging.getLogger(__name__)
+
+    def setup_batch_processing(
+        self, settings: ProcessingSettings
+    ) -> BatchSetupResult:
+        """Setup batch processing, validate inputs, and determine mode.
+
+        Handles input path validation, mode determination (single file vs batch),
+        and creates necessary 'finished' folders.
+
+        Args:
+            settings: Processing settings
+
+        Returns:
+            BatchSetupResult with setup information or error
+        """
+        input_source = settings.input_source_clips
+        input_depth = settings.input_depth_maps
+        output_dir = settings.output_splatted
+
+        is_source_file = os.path.isfile(input_source)
+        is_source_dir = os.path.isdir(input_source)
+        is_depth_file = os.path.isfile(input_depth)
+        is_depth_dir = os.path.isdir(input_depth)
+
+        result = BatchSetupResult()
+
+        if is_source_file and is_depth_file:
+            # Single-file mode
+            result.is_single_file_mode = True
+            self.logger.debug(
+                "==> Running in single file mode. Files will not be moved to 'finished' folders."
+            )
+            result.input_videos.append(input_source)
+            os.makedirs(output_dir, exist_ok=True)
+
+        elif is_source_dir and is_depth_dir:
+            # Batch (folder) mode
+            self.logger.debug("==> Running in batch (folder) mode.")
+
+            if settings.move_to_finished:
+                result.finished_source_folder = os.path.join(input_source, "finished")
+                result.finished_depth_folder = os.path.join(input_depth, "finished")
+                os.makedirs(result.finished_source_folder, exist_ok=True)
+                os.makedirs(result.finished_depth_folder, exist_ok=True)
+                self.logger.debug("Finished folders enabled for batch mode.")
+            else:
+                self.logger.debug(
+                    "Finished folders DISABLED by user setting. Files will remain in input folders."
+                )
+
+            os.makedirs(output_dir, exist_ok=True)
+
+            # Collect video files
+            video_extensions = ("*.mp4", "*.avi", "*.mov", "*.mkv")
+            for ext in video_extensions:
+                result.input_videos.extend(glob.glob(os.path.join(input_source, ext)))
+            result.input_videos = sorted(result.input_videos)
+
+        else:
+            result.error = (
+                "==> Error: Input Source Clips and Input Depth Maps must both be "
+                "either files or directories. Skipping processing."
+            )
+            self.logger.error(result.error)
+            return result
+
+        if not result.input_videos:
+            result.error = f"No video files found in {input_source}"
+            self.logger.error(result.error)
+
+        return result
+
+    def get_defined_tasks(self, settings: ProcessingSettings) -> List[ProcessingTask]:
+        """Get list of processing tasks based on settings.
+
+        Args:
+            settings: Processing settings
+
+        Returns:
+            List of ProcessingTask objects
+        """
+        tasks = []
+
+        if settings.enable_full_resolution:
+            tasks.append(
+                ProcessingTask(
+                    name="Full-Resolution",
+                    output_subdir="hires",
+                    set_pre_res=False,
+                    target_width=-1,
+                    target_height=-1,
+                    batch_size=settings.full_res_batch_size,
+                    is_low_res=False,
+                )
+            )
+
+        if settings.enable_low_resolution:
+            tasks.append(
+                ProcessingTask(
+                    name="Low-Resolution",
+                    output_subdir="lowres",
+                    set_pre_res=True,
+                    target_width=settings.low_res_width,
+                    target_height=settings.low_res_height,
+                    batch_size=settings.low_res_batch_size,
+                    is_low_res=True,
+                )
+            )
+
+        return tasks
+
+    def run_batch_process(
+        self,
+        settings: ProcessingSettings,
+        from_index: int = 0,
+        to_index: Optional[int] = None,
+        video_list: Optional[List[Dict]] = None,
+    ) -> None:
+        """Run batch processing loop."""
+        try:
+            # Setup
+            setup_result = self.setup_batch_processing(settings)
+            if setup_result.error:
+                self.logger.error(setup_result.error)
+                return
+
+            input_videos = setup_result.input_videos
+            is_single_file = setup_result.is_single_file_mode
+
+            if not input_videos:
+                self.logger.error("No input videos found for processing.")
+                return
+
+            # Range selection
+            if not is_single_file and video_list:
+                total = len(video_list)
+                start = max(0, min(total, from_index))
+                end = max(start + 1, min(total, to_index or total))
+                selected = video_list[start:end]
+                input_videos = [e.get("source_video") for e in selected if e.get("source_video")]
+            elif not is_single_file:
+                total = len(input_videos)
+                start = max(0, min(total, from_index))
+                end = max(start + 1, min(total, to_index or total))
+                input_videos = input_videos[start:end]
+
+            if not input_videos:
+                self.logger.error("No input videos left to process.")
+                return
+
+            tasks = self.get_defined_tasks(settings)
+            if not tasks:
+                self.logger.error("No processing tasks defined.")
+                return
+
+            total_tasks = len(input_videos) * len(tasks)
+            self.progress_queue.put(("total", total_tasks))
+
+            # Initialize RenderProcessor
+            renderer = RenderProcessor(
+                stop_event=self.stop_event,
+                progress_queue=self.progress_queue,
+            )
+
+            # Process each video
+            task_counter = 0
+            for vid_path in input_videos:
+                if self.stop_event.is_set(): break
+
+                tasks_processed = self._process_single_video_orchestration(
+                    video_path=vid_path,
+                    settings=settings,
+                    renderer=renderer,
+                    initial_task_counter=task_counter,
+                    is_single_file_mode=is_single_file,
+                )
+                task_counter += tasks_processed
+
+        except Exception as e:
+            self.logger.error(f"Batch processing error: {e}", exc_info=True)
+            self.progress_queue.put(("status", f"Error: {e}"))
+        finally:
+            release_cuda_memory()
+            self.progress_queue.put("finished")
+
+    def _process_single_video_orchestration(
+        self,
+        video_path: str,
+        settings: ProcessingSettings,
+        renderer: RenderProcessor,
+        initial_task_counter: int,
+        is_single_file_mode: bool,
+    ) -> int:
+        """Handles the full processing lifecycle for a single video."""
+        video_name = os.path.splitext(os.path.basename(video_path))[0]
+        self.logger.info(f"==> Processing Video: {video_name}")
+        self.progress_queue.put(("update_info", {"filename": video_name}))
+
+        # 1. Resolve Settings (Sidecar + GUI)
+        vid_settings = self._get_video_specific_settings(video_path, settings, is_single_file_mode)
+        if vid_settings.get("error"):
+            self.logger.error(f"Settings error for {video_name}: {vid_settings['error']}")
+            return len(self.get_defined_tasks(settings))
+
+        # 2. Auto-Convergence
+        conv_val = vid_settings["convergence_plane"]
+        if settings.auto_convergence_mode != "Off" and vid_settings["anchor_source"] != "Sidecar":
+            conv_val = self._handle_auto_convergence(video_path, vid_settings["actual_depth_map_path"], settings)
+
+        # 3. Tasks Loop
+        tasks = self.get_defined_tasks(settings)
+        processed_count = 0
+        for task in tasks:
+            if self.stop_event.is_set(): break
+            
+            self.progress_queue.put(("status", f"Processing {task.name} for {video_name}"))
+            
+            # Initialize Readers for this task/resolution
+            readers = self._initialize_readers(video_path, vid_settings["actual_depth_map_path"], settings, task)
+            if not readers:
+                processed_count += 1
+                self.progress_queue.put(("processed", initial_task_counter + processed_count))
+                continue
+
+            # Run Rendering
+            success = renderer.render_video(
+                input_video_reader=readers["source"],
+                depth_map_reader=readers["depth"],
+                total_frames_to_process=readers["total_frames"],
+                processed_fps=readers["fps"],
+                output_video_path_base=os.path.join(settings.output_splatted, task.output_subdir, f"{video_name}.mp4"),
+                target_output_height=readers["target_h"],
+                target_output_width=readers["target_w"],
+                max_disp=vid_settings["max_disparity_percentage"],
+                batch_size=task.batch_size,
+                dual_output=settings.dual_output,
+                zero_disparity_anchor_val=conv_val,
+                video_stream_info=readers["source_info"],
+                input_bias=vid_settings["input_bias"],
+                assume_raw_input=not vid_settings["enable_global_norm"],
+                global_depth_min=vid_settings.get("global_min", 0.0), # Will handle normalization inside renderer if needed
+                global_depth_max=vid_settings.get("global_max", 1.0),
+                depth_stream_info=readers["depth_info"],
+                user_output_crf=settings.output_crf_low if task.is_low_res else settings.output_crf_full,
+                is_low_res_task=task.is_low_res,
+                depth_gamma=vid_settings["depth_gamma"],
+                depth_dilate_size_x=vid_settings["depth_dilate_size_x"],
+                depth_dilate_size_y=vid_settings["depth_dilate_size_y"],
+                depth_blur_size_x=vid_settings["depth_blur_size_x"],
+                depth_blur_size_y=vid_settings["depth_blur_size_y"],
+                depth_dilate_left=vid_settings["depth_dilate_left"],
+                depth_blur_left=vid_settings["depth_blur_left"],
+                depth_blur_left_mix=vid_settings["depth_blur_left_mix"],
+                skip_lowres_preproc=settings.skip_lowres_preproc,
+                color_tags_mode=settings.color_tags_mode,
+                is_test_mode=settings.is_test_mode,
+                test_target_frame_idx=settings.test_target_frame_idx,
+            )
+            processed_count += 1
+            # Note: RenderProcessor already puts 'processed' events, but maybe we should synchronize here?
+            # renderer.render_video might need to be told the start index.
+            # For now, let's assume Rendering handles it and we just return count.
+
+        return len(tasks)
+
+    def _get_video_specific_settings(self, video_path: str, settings: ProcessingSettings, is_single_file_mode: bool) -> dict:
+        """Resolve settings for a specific video, merging sidecar and GUI defaults."""
+        video_name = os.path.splitext(os.path.basename(video_path))[0]
+        sidecar_path = os.path.join(settings.sidecar_folder, f"{video_name}_depth{settings.sidecar_ext}")
+        
+        sidecar_data = {}
+        if self.sidecar_manager and os.path.exists(sidecar_path):
+            sidecar_data = self.sidecar_manager.load_sidecar_data(sidecar_path) or {}
+
+        # Resolve Depth Map Path
+        actual_depth_path = self._resolve_depth_path(video_path, settings, sidecar_data, is_single_file_mode)
+        if not actual_depth_path:
+            return {"error": "Depth map not found"}
+
+        # Merging Logic
+        res = {
+            "actual_depth_map_path": actual_depth_path,
+            "convergence_plane": sidecar_data.get("convergence_plane", settings.zero_disparity_anchor),
+            "max_disparity_percentage": sidecar_data.get("max_disparity", settings.max_disp),
+            "input_bias": sidecar_data.get("input_bias", 0.0),
+            "depth_gamma": sidecar_data.get("gamma", settings.depth_gamma) if settings.enable_sidecar_gamma else settings.depth_gamma,
+            "depth_dilate_size_x": sidecar_data.get("depth_dilate_size_x", settings.depth_dilate_size_x) if settings.enable_sidecar_blur_dilate else settings.depth_dilate_size_x,
+            "depth_dilate_size_y": sidecar_data.get("depth_dilate_size_y", settings.depth_dilate_size_y) if settings.enable_sidecar_blur_dilate else settings.depth_dilate_size_y,
+            "depth_blur_size_x": sidecar_data.get("depth_blur_size_x", settings.depth_blur_size_x) if settings.enable_sidecar_blur_dilate else settings.depth_blur_size_x,
+            "depth_blur_size_y": sidecar_data.get("depth_blur_size_y", settings.depth_blur_size_y) if settings.enable_sidecar_blur_dilate else settings.depth_blur_size_y,
+            "depth_dilate_left": sidecar_data.get("depth_dilate_left", settings.depth_dilate_left),
+            "depth_blur_left": sidecar_data.get("depth_blur_left", settings.depth_blur_left),
+            "depth_blur_left_mix": sidecar_data.get("depth_blur_left_mix", settings.depth_blur_left_mix),
+            "anchor_source": "Sidecar" if "convergence_plane" in sidecar_data else "GUI",
+            "enable_global_norm": settings.enable_global_norm and ("convergence_plane" not in sidecar_data), # Policy
+        }
+        return res
+
+    def _resolve_depth_path(self, video_path: str, settings: ProcessingSettings, sidecar_data: dict, is_single_file: bool) -> Optional[str]:
+        video_name = os.path.splitext(os.path.basename(video_path))[0]
+        if is_single_file:
+            return settings.input_depth_maps if os.path.isfile(settings.input_depth_maps) else None
+        
+        base_folder = settings.input_depth_maps
+        if settings.multi_map:
+            # Sidecar check
+            selected = sidecar_data.get("selected_depth_map") or settings.selected_depth_map
+            if selected:
+                candidate = os.path.join(base_folder, selected, f"{video_name}_depth.mp4")
+                if os.path.exists(candidate): return candidate
+                candidate = os.path.join(base_folder, selected, f"{video_name}_depth.npz")
+                if os.path.exists(candidate): return candidate
+        
+        # Default check
+        c_mp4 = os.path.join(base_folder, f"{video_name}_depth.mp4")
+        if os.path.exists(c_mp4): return c_mp4
+        c_npz = os.path.join(base_folder, f"{video_name}_depth.npz")
+        if os.path.exists(c_npz): return c_npz
+        return None
+
+    def _handle_auto_convergence(self, video_path: str, depth_path: str, settings: ProcessingSettings) -> float:
+        # Placeholder for auto-convergence integration
+        # In actual implementation, initialize ConvergenceEstimatorWrapper and call estimate_convergence
+        return settings.zero_disparity_anchor
+
+    def _initialize_readers(self, video_path: str, depth_path: str, settings: ProcessingSettings, task: ProcessingTask) -> Optional[dict]:
+        try:
+            source, fps, orig_h, orig_w, target_h, target_w, info, total = read_video_frames(
+                video_path, settings.process_length, set_pre_res=task.set_pre_res,
+                pre_res_width=task.target_width, pre_res_height=task.target_height
+            )
+            
+            # Depth reader setup
+            depth_target_h, depth_target_w = (orig_h, orig_w) if (task.is_low_res and not settings.skip_lowres_preproc) else (target_h, target_w)
+            depth_match = True if task.is_low_res else settings.match_depth_res
+            
+            d_reader, d_total, d_h, d_w, d_info = load_pre_rendered_depth(
+                depth_path, process_length=settings.process_length,
+                target_height=depth_target_h, target_width=depth_target_w,
+                match_resolution_to_target=depth_match
+            )
+            
+            if total != d_total:
+                self.logger.error("Frame count mismatch")
+                return None
+                
+            return {
+                "source": source, "depth": d_reader, "fps": fps, "target_h": target_h, "target_w": target_w,
+                "source_info": info, "depth_info": d_info, "total_frames": total,
+                "orig_h": orig_h, "orig_w": orig_w
+            }
+        except Exception as e:
+            self.logger.error(f"Reader init error: {e}")
+            return None
+
+    def validate_settings(self, settings: ProcessingSettings) -> Tuple[bool, str]:
+        """Validate processing settings.
+
+        Args:
+            settings: Processing settings to validate
+
+        Returns:
+            Tuple of (is_valid, error_message)
+        """
+        # Check max disparity
+        if settings.max_disp <= 0:
+            return False, "Max Disparity must be positive."
+
+        # Check convergence anchor
+        if not (0.0 <= settings.zero_disparity_anchor <= 1.0):
+            return False, "Zero Disparity Anchor must be between 0.0 and 1.0."
+
+        # Check batch sizes
+        if settings.enable_full_resolution and settings.full_res_batch_size <= 0:
+            return False, "Full Resolution Batch Size must be positive."
+
+        if settings.enable_low_resolution:
+            if settings.low_res_width <= 0 or settings.low_res_height <= 0:
+                return False, "Low-Resolution Width and Height must be positive."
+            if settings.low_res_batch_size <= 0:
+                return False, "Low-Resolution Batch Size must be positive."
+
+        # Check at least one resolution enabled
+        if not (settings.enable_full_resolution or settings.enable_low_resolution):
+            return False, "At least one resolution (Full or Low) must be enabled."
+
+        # Check gamma
+        if settings.depth_gamma <= 0:
+            return False, "Depth Gamma must be positive."
+
+        # Check dilate/blur values
+        if (
+            settings.depth_dilate_size_x < -10.0
+            or settings.depth_dilate_size_x > 30.0
+            or settings.depth_dilate_size_y < -10.0
+            or settings.depth_dilate_size_y > 30.0
+        ):
+            return False, "Depth Dilate Sizes (X/Y) must be between -10 and 30."
+
+        if settings.depth_blur_size_x < 0 or settings.depth_blur_size_y < 0:
+            return False, "Depth Blur Sizes (X/Y) must be non-negative."
+
+        if settings.depth_dilate_left < 0.0 or settings.depth_dilate_left > 20.0:
+            return False, "Dilate Left must be between 0 and 20."
+
+        if settings.depth_blur_left < 0 or settings.depth_blur_left > 20:
+            return False, "Blur Left must be between 0 and 20."
+
+        return True, ""
+
+    def compute_depth_normalization(
+        self,
+        depth_map_path: str,
+        enable_global_norm: bool,
+        total_frames: int,
+        batch_size: int,
+        actual_width: int,
+        actual_height: int,
+    ) -> Tuple[bool, float, float, float]:
+        """Compute depth normalization parameters.
+
+        Performs global depth stats scan when global normalization is enabled,
+        or determines scaling factor for raw input mode.
+
+        Args:
+            depth_map_path: Path to depth map video
+            enable_global_norm: Whether global normalization is enabled
+            total_frames: Total frames in depth video
+            batch_size: Processing batch size
+            actual_width: Actual depth width
+            actual_height: Actual depth height
+
+        Returns:
+            Tuple of (assume_raw_mode, global_min, global_max, max_content_value)
+        """
+        assume_raw_mode = not enable_global_norm
+        global_min, global_max = 0.0, 1.0
+        max_content_value = 1.0
+
+        # First scan: Get max content value (unconditional)
+        raw_reader = None
+        try:
+            depth_info = get_video_stream_info(depth_map_path)
+            bit_depth = _infer_depth_bit_depth(depth_info)
+            pix_fmt = str((depth_info or {}).get("pix_fmt", ""))
+
+            if bit_depth > 8:
+                raw_reader = FFmpegDepthPipeReader(
+                    depth_map_path,
+                    out_w=actual_width,
+                    out_h=actual_height,
+                    bit_depth=bit_depth,
+                    num_frames=total_frames,
+                    pix_fmt=pix_fmt,
+                )
+            else:
+                raw_reader = VideoReader(
+                    depth_map_path, ctx=cpu(0), width=actual_width, height=actual_height
+                )
+
+            if len(raw_reader) > 0:
+                _, max_content_value = compute_global_depth_stats(
+                    depth_map_reader=raw_reader,
+                    total_frames=total_frames,
+                    chunk_size=batch_size,
+                )
+                self.logger.debug(f"Max content depth scanned: {max_content_value:.3f}.")
+            else:
+                self.logger.error("RAW depth reader has no frames for content scan.")
+        except Exception as e:
+            self.logger.error(f"Failed to scan max content depth: {e}")
+        finally:
+            if raw_reader:
+                if hasattr(raw_reader, "close"):
+                    try:
+                        raw_reader.close()
+                    except Exception:
+                        pass
+                del raw_reader
+                gc.collect()
+
+        # Second scan: Global normalization (if enabled)
+        if not assume_raw_mode:
+            self.logger.info(
+                "==> Global Depth Normalization selected. Starting global depth stats pre-pass."
+            )
+
+            raw_reader = None
+            try:
+                depth_info = get_video_stream_info(depth_map_path)
+                bit_depth = _infer_depth_bit_depth(depth_info)
+                pix_fmt = str((depth_info or {}).get("pix_fmt", ""))
+
+                if bit_depth > 8:
+                    raw_reader = FFmpegDepthPipeReader(
+                        depth_map_path,
+                        out_w=actual_width,
+                        out_h=actual_height,
+                        bit_depth=bit_depth,
+                        num_frames=total_frames,
+                        pix_fmt=pix_fmt,
+                    )
+                else:
+                    raw_reader = VideoReader(
+                        depth_map_path,
+                        ctx=cpu(0),
+                        width=actual_width,
+                        height=actual_height,
+                    )
+
+                if len(raw_reader) > 0:
+                    global_min, global_max = compute_global_depth_stats(
+                        depth_map_reader=raw_reader,
+                        total_frames=total_frames,
+                        chunk_size=batch_size,
+                    )
+                    self.logger.debug(
+                        "Successfully computed global stats from RAW reader."
+                    )
+                else:
+                    self.logger.error("RAW depth reader has no frames.")
+            except Exception as e:
+                self.logger.error(f"Failed to compute global stats: {e}")
+                global_min, global_max = 0.0, 1.0
+            finally:
+                if raw_reader:
+                    if hasattr(raw_reader, "close"):
+                        try:
+                            raw_reader.close()
+                        except Exception:
+                            pass
+                    del raw_reader
+                    gc.collect()
+        else:
+            self.logger.debug(
+                "==> No Normalization (Assume Raw 0-1 Input) selected. Skipping global stats pre-pass."
+            )
+
+            # Determine scaling factor for raw input mode
+            if max_content_value <= 256.0 and max_content_value > 1.0:
+                global_max = 255.0
+            elif max_content_value > 256.0 and max_content_value <= 1024.0:
+                global_max = 1023.0
+            else:
+                global_max = 1023.0
+                self.logger.warning(
+                    f"Max content value is unusual ({max_content_value:.2f}). Using fallback 1023.0."
+                )
+            global_min = 0.0
+
+        return assume_raw_mode, global_min, global_max, max_content_value
+
+    def check_stop_requested(self) -> bool:
+        """Check if processing stop has been requested.
+
+        Returns:
+            True if stop event is set
+        """
+        return self.stop_event.is_set()
diff --git a/core/splatting/border_scanning.py b/core/splatting/border_scanning.py
new file mode 100644
index 0000000..9bb1a42
--- /dev/null
+++ b/core/splatting/border_scanning.py
@@ -0,0 +1,316 @@
+"""Border scanning module for automatic border detection.
+
+Provides methods for scanning depth map edges to determine
+required border widths for zero-parallax plane adjustments.
+"""
+
+import logging
+import os
+from typing import Optional, Tuple
+
+import numpy as np
+from decord import VideoReader, cpu
+
+from .depth_processing import (
+    DEPTH_VIS_TV10_BLACK_NORM,
+    DEPTH_VIS_TV10_WHITE_NORM,
+    _infer_depth_bit_depth,
+)
+from dependency.stereocrafter_util import get_video_stream_info
+
+logger = logging.getLogger(__name__)
+
+
+class BorderScanner:
+    """Handles automatic border detection from depth maps.
+
+    Provides methods for scanning depth map edges to determine
+    required border widths for zero-parallax plane adjustments.
+
+    Args:
+        gui_context: Optional reference to GUI for status updates
+    """
+
+    def __init__(self, gui_context=None):
+        """Initialize border scanner.
+
+        Args:
+            gui_context: Optional reference to GUI for status updates
+        """
+        self.gui_context = gui_context
+        self.logger = logging.getLogger(__name__)
+
+    def scan_current_clip(
+        self,
+        depth_path: str,
+        conv: float,
+        max_disp: float,
+        gamma: float = 1.0,
+        force: bool = False,
+        stop_event=None,
+        status_callback=None,
+    ) -> Optional[Tuple[float, float]]:
+        """Scan current depth map for border requirements.
+
+        Samples frames from the depth map and calculates the maximum border
+        width needed on left and right edges based on convergence and disparity
+        settings.
+
+        Args:
+            depth_path: Path to depth map video
+            conv: Convergence plane value (0.0 to 1.0)
+            max_disp: Maximum disparity in pixels
+            gamma: Gamma correction value (default: 1.0)
+            force: Force rescan even if cached (unused, for API compatibility)
+            stop_event: Optional threading.Event for cancellation
+            status_callback: Optional callback(status_text) for status updates
+
+        Returns:
+            Tuple of (left_border_pct, right_border_pct) if successful, None otherwise
+        """
+        if not depth_path or not os.path.exists(depth_path):
+            self.logger.warning(f"Depth path not found: {depth_path}")
+            return None
+
+        try:
+            vr = VideoReader(depth_path, ctx=cpu(0))
+            total_frames = len(vr)
+            if total_frames == 0:
+                return None
+
+            # Show scanning status
+            if status_callback:
+                status_callback(f"Scanning borders for {os.path.basename(depth_path)}...")
+
+            return self._scan_depth_video(
+                vr, total_frames, conv, max_disp, gamma, stop_event, status_callback
+            )
+
+        except Exception as e:
+            self.logger.error(f"Border scan failed: {e}", exc_info=True)
+            if status_callback:
+                status_callback("Border scan failed.")
+            return None
+
+    def scan_depth_path(
+        self,
+        depth_map_path: str,
+        conv: float,
+        max_disp: float,
+        gamma: float = 1.0,
+        stop_event=None,
+    ) -> Optional[Tuple[float, float]]:
+        """Thread-safe helper for scanning a depth-map video for border requirements.
+
+        Samples frames from the depth map and calculates the maximum border
+        width needed on left and right edges based on convergence and disparity
+        settings.
+
+        Args:
+            depth_map_path: Absolute path to depth map video file
+            conv: Convergence plane value (0.0 to 1.0)
+            max_disp: Maximum disparity in pixels
+            gamma: Gamma correction value for depth (default: 1.0)
+            stop_event: Optional threading.Event for cancellation
+
+        Returns:
+            Tuple of (left_border_pct, right_border_pct) if successful,
+            None if scan failed or was cancelled
+        """
+        try:
+            vr_depth = VideoReader(depth_map_path, ctx=cpu(0))
+            total_frames = len(vr_depth)
+            if total_frames <= 0:
+                return None
+
+            return self._scan_depth_video(
+                vr_depth, total_frames, conv, max_disp, gamma, stop_event
+            )
+
+        except Exception as e:
+            self.logger.error(f"Border scan failed: {e}", exc_info=True)
+            return None
+
+    def _scan_depth_video(
+        self,
+        video_reader: VideoReader,
+        total_frames: int,
+        conv: float,
+        max_disp: float,
+        gamma: float = 1.0,
+        stop_event=None,
+        status_callback=None,
+    ) -> Optional[Tuple[float, float]]:
+        """Internal method to perform the actual border scanning.
+
+        Args:
+            video_reader: Active VideoReader for the depth map
+            total_frames: Total number of frames to scan
+            conv: Convergence plane value
+            max_disp: Maximum disparity
+            gamma: Gamma correction value
+            stop_event: Optional threading.Event for cancellation
+            status_callback: Optional callback(status_text) for status updates
+
+        Returns:
+            Tuple of (left_border_pct, right_border_pct) if successful
+        """
+        step = 5
+        max_L = 0.0
+        max_R = 0.0
+
+        # Get TV-range compensation factor
+        tv_disp_comp = self._get_tv_compensation(video_reader)
+
+        gamma_f = float(gamma) if gamma else 1.0
+
+        for i in range(0, total_frames, step):
+            # Check for cancellation
+            if stop_event and stop_event.is_set():
+                break
+
+            try:
+                frame_raw = video_reader[i].asnumpy()
+            except Exception:
+                continue
+
+            # Convert to grayscale if RGB
+            if frame_raw.ndim == 3:
+                frame = frame_raw.mean(axis=2)
+            else:
+                frame = frame_raw
+
+            # Sample 5px wide at each edge
+            L_sample = frame[:, :5]
+            R_sample = frame[:, -5:]
+
+            # 99th percentile to ignore noise
+            d_L = np.percentile(L_sample, 99) / 255.0
+            d_R = np.percentile(R_sample, 99) / 255.0
+
+            # Apply the same gamma curve used by the render path
+            if gamma_f != 1.0:
+                d_L = float(np.clip(d_L, 0.0, 1.0))
+                d_R = float(np.clip(d_R, 0.0, 1.0))
+                d_L = 1.0 - (1.0 - d_L) ** gamma_f
+                d_R = 1.0 - (1.0 - d_R) ** gamma_f
+
+            # Calculate border widths using the same formula as Auto Basic
+            b_L = max(0.0, (d_L - conv) * 2.0 * (max_disp / 20.0) * tv_disp_comp)
+            b_R = max(0.0, (d_R - conv) * 2.0 * (max_disp / 20.0) * tv_disp_comp)
+
+            max_L = max(max_L, b_L)
+            max_R = max(max_R, b_R)
+
+        max_L = min(5.0, round(float(max_L), 3))
+        max_R = min(5.0, round(float(max_R), 3))
+
+        self.logger.info(
+            f"Border scan complete: L={max_L}, R={max_R} (Conv={conv:.2f}, Disp={max_disp:.1f})"
+        )
+
+        if status_callback:
+            status_callback(f"Scan complete: L={max_L}%, R={max_R}%")
+
+        return max_L, max_R
+
+    def _get_tv_compensation(self, video_reader: VideoReader) -> float:
+        """Get TV-range compensation factor for depth maps.
+
+        TV-range 10-bit depth maps preserve the 64-940 code window;
+        this compensates so the disparity feels the same as full-range.
+
+        Args:
+            video_reader: VideoReader instance
+
+        Returns:
+            Compensation factor (1.0 for full-range, scaled for TV-range)
+        """
+        try:
+            # Try to get the file path from the reader
+            if hasattr(video_reader, '_file_path'):
+                depth_path = video_reader._file_path
+            else:
+                # Fallback - can't determine compensation
+                return 1.0
+
+            _info = get_video_stream_info(depth_path)
+            if _infer_depth_bit_depth(_info) > 8:
+                color_range = str((_info or {}).get("color_range", "unknown")).lower()
+                if color_range == "tv":
+                    return 1.0 / (
+                        DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM
+                    )
+        except Exception:
+            pass
+
+        return 1.0
+
+    @staticmethod
+    def calculate_basic_border(
+        convergence: float, max_disp: float, tv_comp: float = 1.0
+    ) -> float:
+        """Calculate border width for Auto Basic mode.
+
+        Args:
+            convergence: Convergence plane value (0.0 to 1.0)
+            max_disp: Maximum disparity in pixels
+            tv_comp: TV-range compensation factor (default: 1.0)
+
+        Returns:
+            Border width as percentage (capped at 5.0%)
+        """
+        width = max(0.0, (1.0 - convergence) * 2.0 * (max_disp / 20.0) * tv_comp)
+        return min(5.0, width)
+
+    @staticmethod
+    def calculate_border_from_depth(
+        depth_value: float,
+        conv: float,
+        max_disp: float,
+        gamma: float = 1.0,
+        tv_comp: float = 1.0,
+    ) -> float:
+        """Calculate border width from a single depth value.
+
+        Args:
+            depth_value: Normalized depth value (0.0 to 1.0)
+            conv: Convergence plane value
+            max_disp: Maximum disparity
+            gamma: Gamma correction value
+            tv_comp: TV-range compensation factor
+
+        Returns:
+            Border width as percentage
+        """
+        # Apply gamma if needed
+        if gamma != 1.0:
+            depth_value = float(np.clip(depth_value, 0.0, 1.0))
+            depth_value = 1.0 - (1.0 - depth_value) ** float(gamma)
+
+        border = max(0.0, (depth_value - conv) * 2.0 * (max_disp / 20.0) * tv_comp)
+        return min(5.0, border)
+
+    @staticmethod
+    def sync_sliders_to_auto_borders(
+        left_border: float, right_border: float
+    ) -> Tuple[float, float]:
+        """Convert left/right borders to width/bias values.
+
+        Args:
+            left_border: Left border percentage
+            right_border: Right border percentage
+
+        Returns:
+            Tuple of (width, bias) values for sliders
+        """
+        width = max(left_border, right_border)
+        if width > 0:
+            if left_border >= right_border:
+                bias = (right_border / left_border) - 1.0
+            else:
+                bias = 1.0 - (left_border / right_border)
+        else:
+            bias = 0.0
+
+        return round(width, 2), round(bias, 2)
diff --git a/core/splatting/config_manager.py b/core/splatting/config_manager.py
new file mode 100644
index 0000000..7da7de4
--- /dev/null
+++ b/core/splatting/config_manager.py
@@ -0,0 +1,376 @@
+"""Configuration management for StereoCrafter applications.
+
+Provides utilities for loading, saving, and managing application configuration
+with support for defaults, backward compatibility, and file operations.
+"""
+
+import json
+import logging
+import os
+from typing import Any, Dict, Optional
+
+logger = logging.getLogger(__name__)
+
+
+# Default configuration for Splatting GUI
+SPLATTER_DEFAULT_CONFIG = {
+    "DEFAULT_CONFIG_FILENAME": "config_splat.splatcfg",
+    "input_source_clips": "./input_source_clips",
+    "input_depth_maps": "./input_depth_maps",
+    "multi_map_enabled": False,
+    "output_splatted": "./output_splatted",
+    "max_disp": "30.0",
+    "process_length": "-1",
+    "batch_size": "10",
+    "dual_output": False,
+    "enable_global_norm": False,
+    "enable_full_resolution": True,
+    "enable_low_resolution": False,
+    "pre_res_width": "1920",
+    "pre_res_height": "1080",
+    "low_res_batch_size": "50",
+    "convergence_point": "0.5",
+    "output_crf": "23",
+    "output_crf_full": "23",
+    "output_crf_low": "23",
+    "color_tags_mode": "Auto",
+    "dark_mode_enabled": False,
+    "skip_lowres_preproc": False,
+    "move_to_finished": True,
+    "crosshair_enabled": False,
+    "crosshair_white": False,
+    "crosshair_multi": False,
+    "depth_pop_enabled": False,
+    "auto_convergence_mode": "Off",
+    "depth_gamma": "1.0",
+    "depth_dilate_size_x": "3",
+    "depth_dilate_size_y": "3",
+    "depth_blur_size_x": "5",
+    "depth_blur_size_y": "5",
+    "depth_dilate_left": "0",
+    "depth_blur_left": "0",
+    "depth_blur_left_mix": "0.5",
+    "enable_sidecar_gamma": True,
+    "enable_sidecar_blur_dilate": True,
+    "update_slider_from_sidecar": True,
+    "auto_save_sidecar": False,
+    "border_width": "0.0",
+    "border_bias": "0.0",
+    "border_mode": "Off",
+    "auto_border_L": "0.0",
+    "auto_border_R": "0.0",
+    "preview_source": "Splat Result",
+    "preview_size": "75%",
+    "window_width": 620,
+    "window_height": 750,
+    "debug_mode_enabled": False,
+    "border_manual": False,
+}
+
+
+def load_config(
+    config_filename: str = "config_splat.splatcfg",
+    defaults: Optional[Dict[str, Any]] = None,
+) -> Dict[str, Any]:
+    """Load configuration from a JSON file with defaults.
+
+    Args:
+        config_filename: Path to the configuration file
+        defaults: Dictionary of default values to use if file not found
+
+    Returns:
+        Dictionary containing configuration values
+    """
+    config = dict(defaults) if defaults else {}
+    
+    if not os.path.exists(config_filename):
+        logger.debug(f"Config file not found: {config_filename}. Using defaults.")
+        return config
+
+    try:
+        with open(config_filename, "r") as f:
+            loaded_config = json.load(f)
+        
+        # Apply loaded values, preserving structure
+        config.update(loaded_config)
+        
+        # Handle backward compatibility for specific keys
+        _apply_backward_compat(config)
+        
+        logger.info(f"Loaded config from: {config_filename}")
+        
+    except json.JSONDecodeError as e:
+        logger.error(f"Failed to parse config file {config_filename}: {e}")
+    except Exception as e:
+        logger.error(f"Failed to load config file {config_filename}: {e}")
+
+    return config
+
+
+def save_config(
+    config: Dict[str, Any],
+    config_filename: str = "config_splat.splatcfg",
+) -> bool:
+    """Save configuration to a JSON file.
+
+    Args:
+        config: Dictionary of configuration values to save
+        config_filename: Path to the configuration file
+
+    Returns:
+        True if successful, False otherwise
+    """
+    try:
+        with open(config_filename, "w") as f:
+            json.dump(config, f, indent=4)
+        logger.info(f"Saved config to: {config_filename}")
+        return True
+    except Exception as e:
+        logger.error(f"Failed to save config to {config_filename}: {e}")
+        return False
+
+
+def load_settings_from_file(
+    filename: str,
+    tk_vars: Optional[Dict[str, Any]] = None,
+) -> Dict[str, Any]:
+    """Load settings from a user-selected JSON file.
+
+    Args:
+        filename: Path to the settings file
+        tk_vars: Optional dictionary mapping config keys to tkinter variables
+
+    Returns:
+        Dictionary of loaded settings
+    """
+    try:
+        with open(filename, "r") as f:
+            loaded_config = json.load(f)
+        
+        # Apply to tkinter variables if provided
+        if tk_vars:
+            for config_key, config_value in loaded_config.items():
+                tk_var_attr_name = config_key + "_var"
+                if tk_var_attr_name in tk_vars:
+                    tk_var = tk_vars[tk_var_attr_name]
+                    _set_tk_var(tk_var, config_value)
+        
+        logger.info(f"Loaded settings from: {filename}")
+        return loaded_config
+        
+    except Exception as e:
+        logger.error(f"Failed to load settings from {filename}: {e}")
+        return {}
+
+
+def save_settings_to_file(
+    config: Dict[str, Any],
+    filename: str,
+) -> bool:
+    """Save settings to a user-selected JSON file.
+
+    Args:
+        config: Dictionary of settings to save
+        filename: Path to the output file
+
+    Returns:
+        True if successful, False otherwise
+    """
+    try:
+        with open(filename, "w") as f:
+            json.dump(config, f, indent=4)
+        logger.info(f"Saved settings to: {filename}")
+        return True
+    except Exception as e:
+        logger.error(f"Failed to save settings to {filename}: {e}")
+        return False
+
+
+def get_current_config(
+    tk_vars: Dict[str, Any],
+    defaults: Optional[Dict[str, Any]] = None,
+) -> Dict[str, Any]:
+    """Extract current configuration from tkinter variables.
+
+    Args:
+        tk_vars: Dictionary mapping config keys to tkinter variables
+        defaults: Optional dictionary of default values
+
+    Returns:
+        Dictionary of current configuration values
+    """
+    config = dict(defaults) if defaults else {}
+    
+    for key, var in tk_vars.items():
+        if key.endswith("_var"):
+            config_key = key[:-4]  # Remove "_var" suffix
+            config[config_key] = _get_tk_var_value(var)
+    
+    return config
+
+
+def reset_to_defaults(
+    tk_vars: Dict[str, Any],
+    defaults: Optional[Dict[str, Any]] = None,
+) -> None:
+    """Reset tkinter variables to their default values.
+
+    Args:
+        tk_vars: Dictionary mapping config keys to tkinter variables
+        defaults: Dictionary of default values
+    """
+    for key, var in tk_vars.items():
+        if key.endswith("_var"):
+            config_key = key[:-4]
+            default_value = defaults.get(config_key) if defaults else None
+            if default_value is not None:
+                _set_tk_var(var, default_value)
+
+
+def _apply_backward_compat(config: Dict[str, Any]) -> None:
+    """Apply backward compatibility transformations.
+
+    Args:
+        config: Configuration dictionary to modify
+    """
+    # Handle old 'enable_autogain' key
+    # Old meaning: True = Raw Input / Disable Normalization (GN OFF)
+    # New meaning: True = Enable Global Normalization (GN ON)
+    if "enable_autogain" in config:
+        old_value = config.pop("enable_autogain")
+        config["enable_global_norm"] = not bool(old_value)
+
+    # Handle old depth dilation erosion mapping (30..40 -> -0..-10)
+    for key in ["depth_dilate_size_x", "depth_dilate_size_y"]:
+        if key in config:
+            try:
+                val = float(config[key])
+                if 30.0 < val <= 40.0:
+                    config[key] = str(-(val - 30.0))
+            except (ValueError, TypeError):
+                pass
+
+
+def _get_tk_var_value(var) -> Any:
+    """Get the value from a tkinter variable.
+
+    Args:
+        var: Tkinter variable (StringVar, BooleanVar, IntVar, etc.)
+
+    Returns:
+        The value of the variable
+    """
+    import tkinter as tk
+    
+    if isinstance(var, tk.BooleanVar):
+        return bool(var.get())
+    elif isinstance(var, tk.IntVar):
+        return int(var.get())
+    elif isinstance(var, tk.DoubleVar):
+        return float(var.get())
+    else:
+        return str(var.get())
+
+
+def _set_tk_var(var, value: Any) -> None:
+    """Set the value of a tkinter variable.
+
+    Args:
+        var: Tkinter variable (StringVar, BooleanVar, IntVar, etc.)
+        value: Value to set
+    """
+    import tkinter as tk
+    
+    if isinstance(var, tk.BooleanVar):
+        var.set(bool(value))
+    elif isinstance(var, (tk.IntVar, tk.DoubleVar)):
+        var.set(float(value))
+    else:
+        var.set(str(value))
+
+
+class ConfigManager:
+    """Manages application configuration with tkinter variable integration.
+
+    Provides a high-level interface for loading, saving, and managing
+    configuration with automatic synchronization to tkinter variables.
+
+    Args:
+        defaults: Optional dictionary of default configuration values
+        config_filename: Name of the default config file
+    """
+
+    def __init__(
+        self,
+        defaults: Optional[Dict[str, Any]] = None,
+        config_filename: str = "config_splat.splatcfg",
+    ):
+        """Initialize the configuration manager.
+
+        Args:
+            defaults: Default configuration values
+            config_filename: Name of the config file
+        """
+        self.defaults = defaults or SPLATTER_DEFAULT_CONFIG
+        self.config_filename = config_filename
+        self.config = {}
+        
+    def load(self) -> Dict[str, Any]:
+        """Load configuration from file.
+
+        Returns:
+            Loaded configuration dictionary
+        """
+        self.config = load_config(self.config_filename, self.defaults)
+        return self.config
+    
+    def save(self) -> bool:
+        """Save current configuration to file.
+
+        Returns:
+            True if successful
+        """
+        return save_config(self.config, self.config_filename)
+    
+    def get(self, key: str, default: Any = None) -> Any:
+        """Get a configuration value.
+
+        Args:
+            key: Configuration key
+            default: Default value if key not found
+
+        Returns:
+            Configuration value or default
+        """
+        return self.config.get(key, default)
+    
+    def set(self, key: str, value: Any) -> None:
+        """Set a configuration value.
+
+        Args:
+            key: Configuration key
+            value: Value to set
+        """
+        self.config[key] = value
+    
+    def sync_to_tk_vars(self, tk_vars: Dict[str, Any]) -> None:
+        """Synchronize configuration to tkinter variables.
+
+        Args:
+            tk_vars: Dictionary mapping config keys to tkinter variables
+        """
+        for config_key, config_value in self.config.items():
+            tk_var_attr_name = config_key + "_var"
+            if tk_var_attr_name in tk_vars:
+                _set_tk_var(tk_vars[tk_var_attr_name], config_value)
+    
+    def sync_from_tk_vars(self, tk_vars: Dict[str, Any]) -> None:
+        """Synchronize configuration from tkinter variables.
+
+        Args:
+            tk_vars: Dictionary mapping config keys to tkinter variables
+        """
+        for key, var in tk_vars.items():
+            if key.endswith("_var"):
+                config_key = key[:-4]
+                self.config[config_key] = _get_tk_var_value(var)
diff --git a/core/splatting/convergence.py b/core/splatting/convergence.py
new file mode 100644
index 0000000..d9f64dc
--- /dev/null
+++ b/core/splatting/convergence.py
@@ -0,0 +1,234 @@
+"""Auto-convergence estimation module using U2NETP neural network.
+
+Provides convergence plane estimation using visual saliency analysis
+of RGB and depth map pairs.
+"""
+
+import logging
+import os
+import threading
+from typing import Dict, List, Optional, Tuple
+
+import numpy as np
+import torch
+from decord import VideoReader, cpu
+
+logger = logging.getLogger(__name__)
+
+
+class ConvergenceEstimatorWrapper:
+    """Handles automatic convergence plane detection using U2NETP.
+
+    Estimates the optimal zero-parallax plane based on visual saliency
+    analysis of RGB and depth map pairs. Wraps the dependency.convergence_estimator
+    neural network model.
+
+    Args:
+        model_path: Optional path to U2NETP model weights
+        device: Optional torch device ('cuda' or 'cpu')
+    """
+
+    def __init__(
+        self, model_path: Optional[str] = None, device: Optional[str] = None
+    ):
+        """Initialize convergence estimation model.
+
+        Args:
+            model_path: Path to model weights file
+            device: Torch device for inference
+        """
+        self.logger = logging.getLogger(__name__)
+        self._model_path = model_path
+        self._device = device
+        self._estimator = None
+        self._load_model()
+
+    def _load_model(self) -> None:
+        """Load the U2NETP model for convergence estimation."""
+        try:
+            # Lazy import to avoid circular dependency issues
+            from dependency.convergence_estimator import (
+                ConvergenceEstimator as NeuralEstimator,
+            )
+
+            self._estimator = NeuralEstimator(
+                model_path=self._model_path, device=self._device
+            )
+            if self._estimator.model is None:
+                self.logger.error(
+                    "ConvergenceEstimator model failed to load."
+                )
+        except ImportError as e:
+            self.logger.error(f"Could not import ConvergenceEstimator: {e}")
+            self._estimator = None
+
+    def is_model_loaded(self) -> bool:
+        """Check if the neural network model is loaded and ready.
+
+        Returns:
+            True if model is loaded, False otherwise
+        """
+        return self._estimator is not None and self._estimator.model is not None
+
+    def estimate_convergence(
+        self,
+        rgb_path: str,
+        depth_path: str,
+        process_length: int = -1,
+        sample_stride: int = 6,
+        gamma: float = 1.0,
+        fallback_value: float = 0.5,
+        stop_event: Optional[threading.Event] = None,
+    ) -> Tuple[float, float]:
+        """Estimate optimal convergence plane from video and depth map.
+
+        Samples frames uniformly from the video, analyzes them using the
+        U2NETP model to detect salient objects, and returns both average
+        and peak convergence values.
+
+        Args:
+            rgb_path: Path to RGB source video
+            depth_path: Path to depth map video
+            process_length: Number of frames to process (-1 for all)
+            sample_stride: Stride between sampled frames (default: 6)
+            gamma: Gamma correction for depth (default: 1.0)
+            fallback_value: Value to return on failure (default: 0.5)
+            stop_event: Optional threading.Event for cancellation
+
+        Returns:
+            Tuple of (average_convergence, peak_convergence)
+        """
+        if not self.is_model_loaded():
+            self.logger.warning("Model not loaded, returning fallback values")
+            return fallback_value, fallback_value
+
+        try:
+            # Initialize Readers
+            vr_rgb = VideoReader(rgb_path, ctx=cpu(0))
+            vr_depth = VideoReader(depth_path, ctx=cpu(0))
+
+            len_rgb = len(vr_rgb)
+            len_depth = len(vr_depth)
+
+            # Sanity check
+            if len_rgb == 0 or len_depth == 0:
+                self.logger.warning("Empty video or depth map found.")
+                return fallback_value, fallback_value
+
+            total_frames = min(len_rgb, len_depth)
+
+            # Respect process_length if set > 0
+            if process_length > 0:
+                total_frames = min(total_frames, process_length)
+
+            # Sample frames
+            indices = list(range(0, total_frames, sample_stride))
+
+            # Ensure at least one frame is sampled
+            if not indices:
+                indices = [0]
+
+            estimates = []
+
+            self.logger.info(
+                f"Auto-Converge: Sampling {len(indices)} frames from {os.path.basename(rgb_path)}..."
+            )
+
+            for idx in indices:
+                if stop_event and stop_event.is_set():
+                    self.logger.info("Auto-Converge scan cancelled.")
+                    break
+
+                # Read RGB
+                rgb_frame = vr_rgb[idx].asnumpy()  # H, W, 3 (uint8)
+                # Read Depth
+                depth_frame = vr_depth[idx].asnumpy()  # H, W, C or H, W
+
+                # Preprocess for Torch
+                rgb_t = (
+                    torch.from_numpy(rgb_frame).float().permute(2, 0, 1) / 255.0
+                )
+
+                # Depth: Handle various formats (Gray8, Gray16, RGB-encoding)
+                if depth_frame.ndim == 3:
+                    depth_mono = depth_frame.mean(axis=2)
+                else:
+                    depth_mono = depth_frame
+
+                depth_t = torch.from_numpy(depth_mono).float()
+                # Normalize if not 0-1
+                if depth_t.max() > 1.0:
+                    depth_t = depth_t / 255.0
+
+                # Clamp and apply gamma
+                depth_t = torch.clamp(depth_t, 0.0, 1.0)
+                gamma_f = float(gamma) if gamma else 1.0
+                if gamma_f != 1.0:
+                    depth_t = 1.0 - torch.pow((1.0 - depth_t), gamma_f)
+
+                # Format: 1, C, H, W
+                depth_t = depth_t.unsqueeze(0).unsqueeze(0)
+                rgb_b = rgb_t.unsqueeze(0)
+
+                # Predict
+                res = self._estimator.predict(rgb_b, depth_t)
+                estimates.extend(res)
+
+            if not estimates:
+                return fallback_value, fallback_value
+
+            avg_val = sum(estimates) / len(estimates)
+            # Using Max as 'Peak' estimate
+            peak_val = max(estimates)
+
+            self.logger.info(
+                f"Auto-Converge Result: Avg={avg_val:.3f}, Peak={peak_val:.3f}"
+            )
+            return avg_val, peak_val
+
+        except Exception as e:
+            self.logger.error(
+                f"Auto convergence determination failed: {e}", exc_info=True
+            )
+            return fallback_value, fallback_value
+
+    def calculate_hybrid_value(
+        self, avg_value: float, peak_value: float
+    ) -> float:
+        """Calculate hybrid convergence value from average and peak.
+
+        Args:
+            avg_value: Average convergence value
+            peak_value: Peak convergence value
+
+        Returns:
+            Hybrid value (average of avg and peak)
+        """
+        return (avg_value + peak_value) / 2.0
+
+    def get_cached_value(
+        self,
+        mode: str,
+        cache: Dict[str, float],
+        fallback: float = 0.5,
+    ) -> float:
+        """Get convergence value from cache based on mode.
+
+        Args:
+            mode: Mode - 'Average', 'Peak', or 'Hybrid'
+            cache: Dictionary with cached values
+            fallback: Fallback value if mode not in cache
+
+        Returns:
+            Convergence value for the specified mode
+        """
+        if mode == "Average":
+            return cache.get("Average", fallback)
+        elif mode == "Peak":
+            return cache.get("Peak", fallback)
+        elif mode == "Hybrid":
+            avg = cache.get("Average", fallback)
+            peak = cache.get("Peak", fallback)
+            return self.calculate_hybrid_value(avg, peak)
+        else:
+            return fallback
diff --git a/core/splatting/depth_processing.py b/core/splatting/depth_processing.py
new file mode 100644
index 0000000..e981be7
--- /dev/null
+++ b/core/splatting/depth_processing.py
@@ -0,0 +1,913 @@
+"""Depth map processing utilities for StereoCrafter.
+
+Provides functions and classes for reading, processing, and analyzing
+depth maps including 10-bit+ depth support via FFmpeg.
+"""
+
+import logging
+import os
+import re
+import subprocess
+import math
+import torch
+import torch.nn.functional as F
+import cv2
+import numpy as np
+from decord import VideoReader, cpu
+from typing import Optional, Tuple, Any
+
+logger = logging.getLogger(__name__)
+
+# Constants for TV-range depth map normalization
+DEPTH_VIS_TV10_BLACK_NORM = 64.0 / 1023.0
+DEPTH_VIS_TV10_WHITE_NORM = 940.0 / 1023.0
+DEPTH_VIS_APPLY_TV_RANGE_EXPANSION_10BIT = True
+
+
+def custom_dilate(
+    tensor: torch.Tensor,
+    kernel_size_x: float,
+    kernel_size_y: float,
+    use_gpu: bool = False,
+    max_content_value: float = 1.0,
+) -> torch.Tensor:
+    """Applies 16-bit fractional dilation or erosion to preserve 10-bit+ depth fidelity."""
+    kx_raw = float(kernel_size_x)
+    ky_raw = float(kernel_size_y)
+
+    if abs(kx_raw) <= 1e-5 and abs(ky_raw) <= 1e-5:
+        return tensor
+
+    if (kx_raw > 0 and ky_raw < 0) or (kx_raw < 0 and ky_raw > 0):
+        tensor = custom_dilate(tensor, kx_raw, 0, use_gpu, max_content_value)
+        return custom_dilate(tensor, 0, ky_raw, use_gpu, max_content_value)
+
+    is_erosion = kx_raw < 0 or ky_raw < 0
+    kx_abs, ky_abs = abs(kx_raw), abs(ky_raw)
+
+    def get_dilation_params(value):
+        if value <= 1e-5:
+            return 1, 1, 0.0
+        elif value < 3.0:
+            return 1, 3, (value / 3.0)
+        else:
+            base = 3 + 2 * int((value - 3) // 2)
+            return base, base + 2, (value - base) / 2.0
+
+    kx_low, kx_high, tx = get_dilation_params(kx_abs)
+    ky_low, ky_high, ty = get_dilation_params(ky_abs)
+
+    device = torch.device("cpu")
+    tensor_cpu = tensor.to(device)
+    processed_frames = []
+
+    for t in range(tensor_cpu.shape[0]):
+        frame_float = tensor_cpu[t].numpy()
+        frame_2d_raw = (
+            frame_float[0]
+            if frame_float.shape[0] == 1
+            else np.transpose(frame_float, (1, 2, 0))
+        )
+        effective_max_value = max(max_content_value, 1e-5)
+
+        src_img = np.ascontiguousarray(
+            np.clip((frame_2d_raw / effective_max_value) * 65535, 0, 65535).astype(
+                np.uint16
+            )
+        )
+
+        def do_op(k_w, k_h, img):
+            if k_w <= 1 and k_h <= 1:
+                return img.astype(np.float32)
+            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (k_w, k_h))
+            if is_erosion:
+                return cv2.erode(img, kernel, iterations=1).astype(np.float32)
+            return cv2.dilate(img, kernel, iterations=1).astype(np.float32)
+
+        is_x_int, is_y_int = (tx <= 1e-4), (ty <= 1e-4)
+        if is_x_int and is_y_int:
+            final_float = do_op(kx_low, ky_low, src_img)
+        elif not is_x_int and is_y_int:
+            final_float = (1.0 - tx) * do_op(kx_low, ky_low, src_img) + tx * do_op(
+                kx_high, ky_low, src_img
+            )
+        elif is_x_int and not is_y_int:
+            final_float = (1.0 - ty) * do_op(kx_low, ky_low, src_img) + ty * do_op(
+                kx_low, ky_high, src_img
+            )
+        else:
+            r11, r12 = do_op(kx_low, ky_low, src_img), do_op(kx_low, ky_high, src_img)
+            r21, r22 = do_op(kx_high, ky_low, src_img), do_op(kx_high, ky_high, src_img)
+            final_float = (1.0 - tx) * ((1.0 - ty) * r11 + ty * r12) + tx * (
+                (1.0 - ty) * r21 + ty * r22
+            )
+
+        processed_raw = (final_float / 65535.0) * effective_max_value
+        processed_frames.append(torch.from_numpy(processed_raw).unsqueeze(0).float())
+
+    return torch.stack(processed_frames).to(tensor.device)
+
+
+def custom_dilate_left(
+    tensor: torch.Tensor,
+    kernel_size: float,
+    use_gpu: bool = False,
+    max_content_value: float = 1.0,
+) -> torch.Tensor:
+    """Directional 16-bit fractional dilation to the LEFT."""
+    k_raw = float(kernel_size)
+    if abs(k_raw) <= 1e-5:
+        return tensor
+
+    is_erosion = k_raw < 0
+    k_raw = abs(k_raw)
+
+    def get_dilation_params(value: float):
+        if value <= 1e-5:
+            return 1, 1, 0.0
+        elif value < 3.0:
+            return 1, 3, (value / 3.0)
+        else:
+            base = 3 + 2 * int((value - 3) // 2)
+            return base, base + 2, (value - base) / 2.0
+
+    k_w_low, k_w_high, t = get_dilation_params(k_raw)
+    k_low, k_high = int(k_w_low // 2), int(k_w_high // 2)
+
+    if k_low <= 0 and k_high <= 0:
+        return tensor
+
+    effective_max_value = max(float(max_content_value), 1e-5)
+    device = torch.device("cpu")
+    tensor = tensor.to(device)
+
+    def do_op(k_int: int, src_img: np.ndarray) -> np.ndarray:
+        if k_int <= 0:
+            return src_img.astype(np.float32)
+        k_w = int(k_int) + 1
+        kernel = np.ones((1, k_w), dtype=np.uint8)
+        anchor = (0, 0)
+        if is_erosion:
+            return cv2.erode(src_img, kernel, anchor=anchor, iterations=1).astype(np.float32)
+        return cv2.dilate(src_img, kernel, anchor=anchor, iterations=1).astype(np.float32)
+
+    processed_frames = []
+    for t_idx in range(tensor.shape[0]):
+        frame_float = tensor[t_idx].cpu().numpy()
+        frame_2d_raw = (
+            frame_float[0]
+            if frame_float.shape[0] == 1
+            else np.transpose(frame_float, (1, 2, 0))
+        )
+        frame_norm_2d = frame_2d_raw / effective_max_value
+        frame_cv_uint16 = np.ascontiguousarray(
+            np.clip(frame_norm_2d * 65535, 0, 65535).astype(np.uint16)
+        )
+        src = frame_cv_uint16.astype(np.float32)
+
+        if abs(t) <= 1e-4:
+            out = do_op(k_low, src)
+        else:
+            out_low, out_high = do_op(k_low, src), do_op(k_high, src)
+            out = (1.0 - t) * out_low + t * out_high
+
+        out_u16 = np.ascontiguousarray(np.clip(out, 0, 65535).astype(np.uint16))
+        out_float = (out_u16.astype(np.float32) / 65535.0) * effective_max_value
+        processed_frames.append(torch.from_numpy(out_float).unsqueeze(0).float())
+
+    return torch.stack(processed_frames).to(tensor.device)
+
+
+def custom_blur(
+    tensor: torch.Tensor,
+    kernel_size_x: int,
+    kernel_size_y: int,
+    use_gpu: bool = True,
+    max_content_value: float = 1.0,
+) -> torch.Tensor:
+    """Applies 16-bit Gaussian blur."""
+    k_x, k_y = int(kernel_size_x), int(kernel_size_y)
+    if k_x <= 0 and k_y <= 0:
+        return tensor
+
+    k_x = k_x if k_x % 2 == 1 else k_x + 1
+    k_y = k_y if k_y % 2 == 1 else k_y + 1
+
+    device = torch.device("cpu")
+    tensor = tensor.to(device)
+    processed_frames = []
+
+    for t in range(tensor.shape[0]):
+        frame_float = tensor[t].cpu().numpy()
+        frame_2d_raw = (
+            frame_float[0]
+            if frame_float.shape[0] == 1
+            else np.transpose(frame_float, (1, 2, 0))
+        )
+        effective_max_value = max(max_content_value, 1e-5)
+        frame_norm_2d = frame_2d_raw / effective_max_value
+        frame_cv_uint16 = np.ascontiguousarray(
+            np.clip(frame_norm_2d * 65535, 0, 65535).astype(np.uint16)
+        )
+        processed_cv_uint16 = cv2.GaussianBlur(frame_cv_uint16, (k_x, k_y), 0)
+        processed_norm_float = processed_cv_uint16.astype(np.float32) / 65535.0
+        processed_raw_float = processed_norm_float * effective_max_value
+        processed_frames.append(torch.from_numpy(processed_raw_float).unsqueeze(0).float())
+
+    return torch.stack(processed_frames).to(tensor.device)
+
+
+def process_depth_batch(
+    batch_depth_numpy_raw: np.ndarray,
+    depth_gamma: float,
+    depth_dilate_size_x: float,
+    depth_dilate_size_y: float,
+    depth_blur_size_x: float,
+    depth_blur_size_y: float,
+    max_raw_value: float,
+    depth_dilate_left: float = 0.0,
+    depth_blur_left: float = 0.0,
+    depth_blur_left_mix: float = 0.5,
+    skip_preprocessing: bool = False,
+) -> np.ndarray:
+    """Unified depth processor for batch of depth maps."""
+    if batch_depth_numpy_raw.ndim == 4 and batch_depth_numpy_raw.shape[-1] == 3:
+        batch_depth_numpy = batch_depth_numpy_raw.mean(axis=-1)
+    else:
+        batch_depth_numpy = (
+            batch_depth_numpy_raw.squeeze(-1)
+            if batch_depth_numpy_raw.ndim == 4
+            else batch_depth_numpy_raw
+        )
+
+    batch_depth_numpy_float = batch_depth_numpy.astype(np.float32)
+
+    if skip_preprocessing:
+        return batch_depth_numpy_float
+
+    current_width = (
+        batch_depth_numpy_raw.shape[2]
+        if batch_depth_numpy_raw.ndim == 4
+        else batch_depth_numpy_raw.shape[1]
+    )
+    res_scale = math.sqrt(current_width / 960.0)
+
+    def map_val(v):
+        f_v = float(v)
+        if f_v > 30.0 and f_v <= 40.0:
+            return -(f_v - 30.0)
+        return f_v
+
+    render_dilate_x = map_val(depth_dilate_size_x) * res_scale
+    render_dilate_y = map_val(depth_dilate_size_y) * res_scale
+    render_blur_x, render_blur_y = depth_blur_size_x * res_scale, depth_blur_size_y * res_scale
+    render_dilate_left, render_blur_left = float(depth_dilate_left) * res_scale, float(depth_blur_left) * res_scale
+
+    if (
+        abs(render_dilate_left) > 1e-5
+        or render_blur_left > 0
+        or abs(render_dilate_x) > 1e-5
+        or abs(render_dilate_y) > 1e-5
+        or render_blur_x > 0
+        or render_blur_y > 0
+    ):
+        device = torch.device("cpu")
+        tensor_4d = torch.from_numpy(batch_depth_numpy_float).unsqueeze(1).to(device)
+
+        if abs(render_dilate_left) > 1e-5:
+            tensor_4d = custom_dilate_left(tensor_4d, float(render_dilate_left), False, max_raw_value)
+
+        if render_blur_left > 0:
+            effective_max_value = max(max_raw_value, 1e-5)
+            EDGE_STEP_8BIT = 3.0
+            step_thresh = effective_max_value * (EDGE_STEP_8BIT / 255.0)
+            dx = tensor_4d[:, :, :, 1:] - tensor_4d[:, :, :, :-1]
+            edge_core = dx > step_thresh
+            edge_mask = torch.zeros_like(tensor_4d, dtype=torch.float32)
+            edge_mask[:, :, :, 1:] = edge_core.float()
+
+            k_blur = int(round(render_blur_left))
+            k_blur = k_blur if k_blur % 2 == 1 else k_blur + 1
+            band_half = max(1, int(math.ceil(k_blur / 4.0)))
+            edge_band = (F.max_pool2d(edge_mask, kernel_size=(1, 2 * band_half + 1), stride=1, padding=(0, band_half)) > 0.5).float()
+            alpha = torch.clamp(custom_blur(edge_band, 7, 1, False, 1.0), 0.0, 1.0)
+
+            mix_f = max(0.0, min(1.0, float(depth_blur_left_mix)))
+            BLUR_LEFT_V_WEIGHT, BLUR_LEFT_H_WEIGHT = mix_f, 1.0 - mix_f
+
+            blurred_h = custom_blur(tensor_4d, k_blur, 1, False, max_raw_value) if BLUR_LEFT_H_WEIGHT > 1e-6 else None
+            blurred_v = custom_blur(tensor_4d, 1, k_blur, False, max_raw_value) if BLUR_LEFT_V_WEIGHT > 1e-6 else None
+
+            if blurred_h is not None and blurred_v is not None:
+                blurred = (blurred_h * BLUR_LEFT_H_WEIGHT + blurred_v * BLUR_LEFT_V_WEIGHT) / max(BLUR_LEFT_H_WEIGHT + BLUR_LEFT_V_WEIGHT, 1e-6)
+            elif blurred_h is not None:
+                blurred = blurred_h
+            elif blurred_v is not None:
+                blurred = blurred_v
+            else:
+                blurred = tensor_4d
+
+            tensor_4d = tensor_4d * (1.0 - alpha) + blurred * alpha
+
+        if abs(render_dilate_x) > 1e-5 or abs(render_dilate_y) > 1e-5:
+            tensor_4d = custom_dilate(tensor_4d, float(render_dilate_x), float(render_dilate_y), False, max_raw_value)
+        if render_blur_x > 0 or render_blur_y > 0:
+            tensor_4d = custom_blur(tensor_4d, float(render_blur_x), float(render_blur_y), False, max_raw_value)
+        
+        batch_depth_numpy_float = tensor_4d.squeeze(1).cpu().numpy()
+        del tensor_4d
+        if torch.cuda.is_available():
+            torch.cuda.empty_cache()
+
+    return batch_depth_numpy_float
+
+
+def compute_global_depth_stats(
+    depth_map_reader: VideoReader,
+    total_frames: int,
+    chunk_size: int = 100,
+) -> Tuple[float, float]:
+    """Compute the global min and max depth values from a depth video.
+
+    Reads the depth video in chunks to compute the overall min and max
+    pixel values. Assumes raw pixel values that need to be scaled
+    (e.g., from 0-255 or 0-1023 range).
+
+    Args:
+        depth_map_reader: Active VideoReader for the depth map
+        total_frames: Total number of frames in the depth video
+        chunk_size: Number of frames to process per chunk
+
+    Returns:
+        Tuple of (global_min, global_max) as float values
+    """
+    logger.info(
+        f"==> Starting global depth stats pre-pass for {total_frames} frames..."
+    )
+    global_min, global_max = np.inf, -np.inf
+
+    for i in range(0, total_frames, chunk_size):
+        current_indices = list(range(i, min(i + chunk_size, total_frames)))
+        if not current_indices:
+            break
+
+        chunk_numpy_raw = depth_map_reader.get_batch(current_indices).asnumpy()
+
+        # Handle RGB vs Grayscale depth maps
+        if chunk_numpy_raw.ndim == 4:
+            if chunk_numpy_raw.shape[-1] == 3:  # RGB
+                chunk_numpy = chunk_numpy_raw.mean(axis=-1)
+            else:  # Grayscale with channel dim
+                chunk_numpy = chunk_numpy_raw.squeeze(-1)
+        else:
+            chunk_numpy = chunk_numpy_raw
+
+        chunk_min = chunk_numpy.min()
+        chunk_max = chunk_numpy.max()
+
+        if chunk_min < global_min:
+            global_min = chunk_min
+        if chunk_max > global_max:
+            global_max = chunk_max
+
+    logger.info(
+        f"==> Global depth stats computed: "
+        f"min_raw={global_min:.3f}, max_raw={global_max:.3f}"
+    )
+    return float(global_min), float(global_max)
+
+
+def _infer_depth_bit_depth(depth_stream_info: Optional[dict]) -> int:
+    """Infer the bit depth of a depth stream from ffprobe info.
+
+    Attempts to determine the bit depth of a depth video stream based
+    on pixel format and profile information from ffprobe.
+
+    Args:
+        depth_stream_info: Dictionary containing stream info from ffprobe
+
+    Returns:
+        Inferred bit depth (8, 10, 12, 16, etc.)
+    """
+    if not depth_stream_info:
+        return 8
+    pix_fmt = str(depth_stream_info.get("pix_fmt", "")).lower()
+    profile = str(depth_stream_info.get("profile", "")).lower()
+
+    # Common patterns: yuv420p10le, yuv444p12le, gray16le, etc.
+    m = re.search(r"(?:p|gray)(\d+)", pix_fmt)
+    if m:
+        try:
+            return int(m.group(1))
+        except Exception:
+            pass
+    if "main10" in profile or "10" in profile:
+        return 10
+    return 8
+
+
+def _build_depth_vf(pix_fmt: str, out_w: int, out_h: int) -> str:
+    """Build FFmpeg video filter for depth map extraction.
+
+    Creates an FFmpeg filter string to extract the depth (luma) plane
+    from a video and convert it to gray16le format.
+
+    Args:
+        pix_fmt: Pixel format of the source video
+        out_w: Output width
+        out_h: Output height
+
+    Returns:
+        FFmpeg filter string for depth extraction
+    """
+    pix_fmt = (pix_fmt or "").lower()
+    # If it's already gray*, don't try to extract planes.
+    if pix_fmt.startswith("gray"):
+        return f"scale={out_w}:{out_h}:flags=bilinear,format=gray16le"
+    # Otherwise, extract luma plane (Y) and convert to gray16.
+    return f"extractplanes=y,scale={out_w}:{out_h}:flags=bilinear,format=gray16le"
+
+
+class _NumpyBatch:
+    """Minimal wrapper to match Decord's get_batch(...).asnumpy() API."""
+
+    def __init__(self, arr: np.ndarray):
+        """Initialize with a numpy array.
+
+        Args:
+            arr: Numpy array to wrap
+        """
+        self._arr = arr
+
+    def asnumpy(self) -> np.ndarray:
+        """Return the underlying numpy array.
+
+        Returns:
+            The wrapped numpy array
+        """
+        return self._arr
+
+
+class FFmpegDepthPipeReader:
+    """Sequential FFmpeg-backed depth reader preserving 10-bit+ values.
+
+    Implements a small subset of Decord's VideoReader API used by the
+    splatter. Optimized for sequential access (render path).
+
+    Supported operations:
+        - __len__()
+        - seek(idx)
+        - get_batch([idx0, idx1, ...]).asnumpy()
+
+    Note:
+        This reader is optimized for sequential access patterns
+        typical in the render path.
+    """
+
+    def __init__(
+        self,
+        path: str,
+        out_w: int,
+        out_h: int,
+        bit_depth: int,
+        num_frames: int,
+        pix_fmt: str = "",
+    ):
+        """Initialize the FFmpeg depth pipe reader.
+
+        Args:
+            path: Path to the depth video file
+            out_w: Output width for decoded frames
+            out_h: Output height for decoded frames
+            bit_depth: Bit depth of the depth values
+            num_frames: Number of frames to decode
+            pix_fmt: Pixel format of the source video
+        """
+        self.path = path
+        self.out_w = int(out_w)
+        self.out_h = int(out_h)
+        self.bit_depth = int(bit_depth) if bit_depth else 16
+        self._num_frames = int(num_frames) if num_frames is not None else 0
+        self._pix_fmt = pix_fmt or ""
+        self._proc: Optional[subprocess.Popen] = None
+        self._next_index = 0
+        self._frame_bytes = self.out_w * self.out_h * 2  # gray16le
+        self._msb_shift: Optional[int] = None
+        self._use_16_to_n_scale: bool = False
+        self._start_process()
+
+    def __len__(self) -> int:
+        """Return the number of frames.
+
+        Returns:
+            Total number of frames in the video
+        """
+        return self._num_frames
+
+    def _start_process(self):
+        """Start the FFmpeg process for depth decoding."""
+        self.close()
+        vf = _build_depth_vf(self._pix_fmt, self.out_w, self.out_h)
+        cmd = [
+            "ffmpeg",
+            "-hide_banner",
+            "-loglevel",
+            "error",
+            "-nostdin",
+            "-i",
+            self.path,
+            "-an",
+            "-sn",
+            "-dn",
+            "-vframes",
+            str(self._num_frames)
+            if self._num_frames and self._num_frames > 0
+            else "999999999",
+            "-vf",
+            vf,
+            "-f",
+            "rawvideo",
+            "pipe:1",
+        ]
+        self._proc = subprocess.Popen(
+            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
+        )
+        self._next_index = 0
+        self._msb_shift = None
+
+    def close(self):
+        """Close the FFmpeg process and cleanup resources."""
+        try:
+            if self._proc is not None:
+                if self._proc.stdout:
+                    try:
+                        self._proc.stdout.close()
+                    except Exception:
+                        pass
+                if self._proc.stderr:
+                    try:
+                        self._proc.stderr.close()
+                    except Exception:
+                        pass
+                try:
+                    self._proc.terminate()
+                except Exception:
+                    pass
+        finally:
+            self._proc = None
+
+    def __del__(self):
+        """Destructor to ensure cleanup."""
+        self.close()
+
+    def seek(self, idx: int):
+        """Seek to a specific frame index.
+
+        Best-effort seek. Efficient only if seeking forward sequentially.
+
+        Args:
+            idx: Target frame index
+        """
+        idx = int(idx)
+        if idx == self._next_index:
+            return
+        if idx < self._next_index:
+            # Restart and fast-forward by discarding frames
+            self._start_process()
+        # Discard until we reach idx
+        to_skip = idx - self._next_index
+        if to_skip <= 0:
+            self._next_index = idx
+            return
+        if self._proc is None or self._proc.stdout is None:
+            self._start_process()
+        discard_bytes = to_skip * self._frame_bytes
+        _ = self._proc.stdout.read(discard_bytes)
+        self._next_index = idx
+
+    def _read_exact(self, nbytes: int) -> bytes:
+        """Read exactly nbytes from the FFmpeg pipe.
+
+        Args:
+            nbytes: Number of bytes to read
+
+        Returns:
+            Bytes read from the pipe
+
+        Raises:
+            EOFError: If unexpected end of data
+        """
+        if self._proc is None or self._proc.stdout is None:
+            self._start_process()
+        buf = b""
+        while len(buf) < nbytes:
+            chunk = self._proc.stdout.read(nbytes - len(buf))
+            if not chunk:
+                break
+            buf += chunk
+        return buf
+
+    def _maybe_apply_shift(self, arr_u16: np.ndarray) -> np.ndarray:
+        """Normalize decoded gray16 values back into native bit-depth range.
+
+        FFmpeg's format conversion to gray16le may either:
+          1. Left-shift the original N-bit values into the MSBs, or
+          2. Scale values to the full 16-bit range.
+
+        This method detects the behavior and applies the appropriate
+        normalization.
+
+        Args:
+            arr_u16: Array of uint16 depth values
+
+        Returns:
+            Normalized depth array
+        """
+        if self._msb_shift is None:
+            expected_max = (
+                (1 << self.bit_depth) - 1 if 0 < self.bit_depth < 16 else None
+            )
+            if expected_max is None:
+                self._msb_shift = 0
+                self._use_16_to_n_scale = False
+            else:
+                # Sample to avoid full-frame scans on large batches
+                flat = arr_u16.reshape(-1)
+                step = max(1, flat.size // 50000)
+                sample = flat[::step]
+                max_val = int(sample.max(initial=0))
+
+                if max_val <= expected_max:
+                    self._msb_shift = 0
+                    self._use_16_to_n_scale = False
+                else:
+                    shift = 16 - self.bit_depth
+                    if shift <= 0:
+                        self._msb_shift = 0
+                        self._use_16_to_n_scale = False
+                    else:
+                        low_mask = (1 << shift) - 1
+                        # If low bits are zero, it's MSB-aligned
+                        low_bits_max = int((sample & low_mask).max(initial=0))
+                        if low_bits_max == 0:
+                            self._msb_shift = shift
+                            self._use_16_to_n_scale = False
+                        else:
+                            # Scaled to 16-bit; scale back down
+                            self._msb_shift = 0
+                            self._use_16_to_n_scale = True
+
+        expected_max = (1 << self.bit_depth) - 1 if 0 < self.bit_depth < 16 else None
+        if expected_max is None:
+            return arr_u16
+
+        if self._msb_shift and self._msb_shift > 0:
+            return (arr_u16 >> self._msb_shift).astype(np.uint16)
+
+        if self._use_16_to_n_scale:
+            # Map 0..65535 -> 0..expected_max with rounding
+            arr32 = arr_u16.astype(np.uint32)
+            return ((arr32 * expected_max + 32767) // 65535).astype(np.uint16)
+
+        return arr_u16
+
+    def get_batch(self, indices):
+        """Get a batch of frames by their indices.
+
+        Args:
+            indices: List of frame indices to retrieve
+
+        Returns:
+            _NumpyBatch containing the requested frames
+        """
+        indices = list(indices)
+        if not indices:
+            return _NumpyBatch(
+                np.zeros((0, self.out_h, self.out_w, 1), dtype=np.uint16)
+            )
+
+        # Render path calls are contiguous and increasing
+        first = int(indices[0])
+        if first != self._next_index:
+            self.seek(first)
+
+        n = len(indices)
+        expected = self._frame_bytes * n
+        buf = self._read_exact(expected)
+        if len(buf) != expected:
+            raise EOFError(
+                f"FFmpegDepthPipeReader: expected {expected} bytes, got {len(buf)}"
+            )
+
+        arr = np.frombuffer(buf, dtype=np.uint16).reshape(
+            n, self.out_h, self.out_w, 1
+        )
+        arr = self._maybe_apply_shift(arr)
+        self._next_index = first + n
+        return _NumpyBatch(arr.copy())
+
+
+class _ResizingDepthReader:
+    """Wrapper reader that resizes 8-bit depth frames to target resolution.
+
+    Uses OpenCV resizing to ensure parity between preview and render paths.
+    """
+
+    def __init__(self, inner_reader, out_w, out_h):
+        """Initialize with inner reader and target dimensions.
+
+        Args:
+            inner_reader: Inner VideoReader or similar
+            out_w: Target output width
+            out_h: Target output height
+        """
+        self._inner = inner_reader
+        self._out_w = int(out_w)
+        self._out_h = int(out_h)
+
+    def __len__(self) -> int:
+        """Return total number of frames.
+
+        Returns:
+            Frame count
+        """
+        return len(self._inner)
+
+    def seek(self, *args, **kwargs):
+        """Proxy seek to inner reader."""
+        return self._inner.seek(*args, **kwargs)
+
+    def get_batch(self, indices):
+        """Get and resize frames.
+
+        Args:
+            indices: List of frame indices
+
+        Returns:
+            _NumpyBatch containing resized frames
+        """
+        arr = self._inner.get_batch(indices).asnumpy()
+        # arr is typically (N,H,W,C) from Decord
+        in_h = int(arr.shape[1])
+        in_w = int(arr.shape[2])
+        if in_w == self._out_w and in_h == self._out_h:
+            return _NumpyBatch(arr)
+
+        interp = (
+            cv2.INTER_LINEAR
+            if (self._out_w > in_w or self._out_h > in_h)
+            else cv2.INTER_AREA
+        )
+
+        if arr.ndim == 4:
+            out = np.empty(
+                (arr.shape[0], self._out_h, self._out_w, arr.shape[3]),
+                dtype=arr.dtype,
+            )
+            for i in range(arr.shape[0]):
+                out[i] = cv2.resize(
+                    arr[i], (self._out_w, self._out_h), interpolation=interp
+                )
+        else:
+            out = np.empty(
+                (arr.shape[0], self._out_h, self._out_w), dtype=arr.dtype
+            )
+            for i in range(arr.shape[0]):
+                out[i] = cv2.resize(
+                    arr[i], (self._out_w, self._out_h), interpolation=interp
+                )
+
+        return _NumpyBatch(out)
+
+
+def load_pre_rendered_depth(
+    depth_map_path: str,
+    process_length: int,
+    target_height: int,
+    target_width: int,
+    match_resolution_to_target: bool,
+) -> Tuple[Any, int, int, int, Optional[dict]]:
+    """Initialize a reader for chunked depth map reading.
+
+    Preserves 10-bit+ depth maps by using an FFmpeg-backed reader when
+    needed. No normalization or autogain is applied here.
+
+    Args:
+        depth_map_path: Path to the depth map video file
+        process_length: Number of frames to process (-1 for all)
+        target_height: Target height for output frames
+        target_width: Target width for output frames
+        match_resolution_to_target: Whether to resize to target resolution
+
+    Returns:
+        Tuple of (depth_reader, total_depth_frames_to_process,
+                  actual_depth_height, actual_depth_width, depth_stream_info)
+
+    Raises:
+        NotImplementedError: If NPZ format is encountered
+        ValueError: If unsupported format is encountered
+    """
+    # Import here to avoid circular dependency
+    from dependency.stereocrafter_util import get_video_stream_info
+
+    logger.debug(f"==> Initializing depth reader from: {depth_map_path}")
+
+    depth_stream_info = get_video_stream_info(depth_map_path)
+    bit_depth = _infer_depth_bit_depth(depth_stream_info)
+    pix_fmt = str((depth_stream_info or {}).get("pix_fmt", ""))
+
+    logger.info(
+        f"==> Depth map stream: pix_fmt='{pix_fmt}', "
+        f"profile='{str((depth_stream_info or {}).get('profile', ''))}', "
+        f"inferred_bit_depth={bit_depth}"
+    )
+
+    if depth_map_path.lower().endswith((".mp4", ".avi", ".mov", ".mkv")):
+        # Determine total frames available
+        total_depth_frames_available = 0
+        try:
+            nb = None
+            if isinstance(depth_stream_info, dict):
+                nb = depth_stream_info.get("nb_frames")
+                if nb in (None, "", "N/A"):
+                    nb = depth_stream_info.get("num_frames") or depth_stream_info.get(
+                        "nb_read_frames"
+                    )
+            if nb not in (None, "", "N/A"):
+                total_depth_frames_available = int(float(nb))
+            else:
+                _tmp = VideoReader(depth_map_path, ctx=cpu(0))
+                total_depth_frames_available = len(_tmp)
+                del _tmp
+        except Exception as e:
+            logger.warning(
+                f"Could not determine depth frame count for '{depth_map_path}': {e}"
+            )
+
+        total_depth_frames_to_process = total_depth_frames_available
+        if process_length != -1 and process_length < total_depth_frames_available:
+            total_depth_frames_to_process = process_length
+
+        # Choose reader implementation
+        if bit_depth > 8:
+            depth_reader = FFmpegDepthPipeReader(
+                depth_map_path,
+                out_w=target_width,
+                out_h=target_height,
+                bit_depth=bit_depth,
+                num_frames=total_depth_frames_available,
+                pix_fmt=pix_fmt,
+            )
+        else:
+            # 8-bit: decode at native res with Decord
+            depth_reader = VideoReader(
+                depth_map_path, ctx=cpu(0)
+            )  # decode at native res
+
+            first_depth_frame_shape = depth_reader.get_batch([0]).asnumpy().shape
+            actual_depth_height, actual_depth_width = first_depth_frame_shape[1:3]
+
+            if match_resolution_to_target and (
+                actual_depth_width != target_width
+                or actual_depth_height != target_height
+            ):
+                depth_reader = _ResizingDepthReader(
+                    depth_reader, out_w=target_width, out_h=target_height
+                )
+                actual_depth_height, actual_depth_width = (
+                    int(target_height),
+                    int(target_width),
+                )
+
+        first_depth_frame_shape = depth_reader.get_batch([0]).asnumpy().shape
+        actual_depth_height, actual_depth_width = first_depth_frame_shape[1:3]
+
+        logger.debug(
+            f"==> Depth reader ready. Final depth resolution: "
+            f"{actual_depth_width}x{actual_depth_height}. "
+            f"Frames available: {total_depth_frames_available}. "
+            f"Frames to process: {total_depth_frames_to_process}. "
+            f"bit_depth={bit_depth}, pix_fmt='{pix_fmt}'."
+        )
+
+        return (
+            depth_reader,
+            total_depth_frames_to_process,
+            actual_depth_height,
+            actual_depth_width,
+            depth_stream_info,
+        )
+
+    elif depth_map_path.lower().endswith(".npz"):
+        logger.error(
+            "NPZ support is temporarily disabled with disk chunking refactor. "
+            "Please convert NPZ to MP4 depth video."
+        )
+        raise NotImplementedError(
+            "NPZ depth map loading is not yet supported with disk chunking."
+        )
+    else:
+        raise ValueError(
+            f"Unsupported depth map format: {os.path.basename(depth_map_path)}. "
+            "Only MP4 are supported with disk chunking."
+        )
diff --git a/core/splatting/forward_warp.py b/core/splatting/forward_warp.py
new file mode 100644
index 0000000..e47957c
--- /dev/null
+++ b/core/splatting/forward_warp.py
@@ -0,0 +1,109 @@
+"""Forward warp module for stereo video generation.
+
+Provides the ForwardWarpStereo PyTorch module for warping images
+based on disparity maps for 3D video generation.
+"""
+
+import logging
+from typing import Tuple, Union
+
+import torch
+import torch.nn as nn
+
+logger = logging.getLogger(__name__)
+
+try:
+    from Forward_Warp import forward_warp
+
+    logger.info("CUDA Forward Warp is available.")
+except Exception:
+    from dependency.forward_warp_pytorch import forward_warp
+
+    logger.info("Forward Warp Pytorch is active.")
+
+
+class ForwardWarpStereo(nn.Module):
+    """
+    PyTorch module for forward warping an image based on a disparity map.
+
+    This module performs forward warping where each pixel in the input
+    image is shifted according to the disparity map to create a warped
+    output suitable for stereoscopic 3D video generation.
+
+    Args:
+        eps: Small epsilon value for numerical stability (default: 1e-6)
+        occlu_map: Whether to return occlusion map alongside warped image
+                   (default: False)
+
+    Example:
+        >>> import torch
+        >>> from core.splatting.forward_warp import ForwardWarpStereo
+        >>> 
+        >>> # Create module
+        >>> fws = ForwardWarpStereo(eps=1e-6, occlu_map=False)
+        >>> 
+        >>> # Forward pass
+        >>> image = torch.randn(1, 3, 512, 1024)  # [B, C, H, W]
+        >>> disparity = torch.randn(1, 1, 512, 512)  # [B, 1, H, W]
+        >>> warped = fws(image, disparity)
+    """
+
+    def __init__(self, eps: float = 1e-6, occlu_map: bool = False):
+        """Initialize the forward warp stereo module.
+
+        Args:
+            eps: Small value to prevent division by zero
+            occlu_map: Whether to return occlusion map
+        """
+        super().__init__()
+        self.eps = eps
+        self.occlu_map = occlu_map
+        self.fw = forward_warp()
+
+    def forward(
+        self,
+        im: torch.Tensor,
+        disp: torch.Tensor,
+    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
+        """Perform forward warping of image based on disparity.
+
+        Args:
+            im: Input image tensor of shape [B, C, H, W]
+            disp: Disparity map tensor of shape [B, 1, H, W]
+                  Positive values shift pixels right (right eye view)
+
+        Returns:
+            If occlu_map is False:
+                Warped image tensor of shape [B, C, H, W]
+            If occlu_map is True:
+                Tuple of (warped_image, occlusion_map)
+                - warped_image: [B, C, H, W]
+                - occlusion_map: [B, 1, H, W] showing areas without source pixels
+        """
+        im = im.contiguous()
+        disp = disp.contiguous()
+
+        # Create weights map from disparity
+        weights_map = disp - disp.min()
+        weights_map = (1.414) ** weights_map
+
+        # Create flow field from disparity (negative for right eye shift)
+        flow = -disp.squeeze(1)
+        dummy_flow = torch.zeros_like(flow, requires_grad=False)
+        flow = torch.stack((flow, dummy_flow), dim=-1)
+
+        # Perform forward warp
+        res_accum = self.fw(im * weights_map, flow)
+        mask = self.fw(weights_map, flow)
+        mask.clamp_(min=self.eps)
+        res = res_accum / mask
+
+        if not self.occlu_map:
+            return res
+        else:
+            # Compute occlusion map
+            ones = torch.ones_like(disp, requires_grad=False)
+            occlu_map = self.fw(ones, flow)
+            occlu_map.clamp_(0.0, 1.0)
+            occlu_map = 1.0 - occlu_map
+            return res, occlu_map
diff --git a/core/splatting/fusion_export.py b/core/splatting/fusion_export.py
new file mode 100644
index 0000000..bacc0c3
--- /dev/null
+++ b/core/splatting/fusion_export.py
@@ -0,0 +1,541 @@
+"""Fusion export sidecar generation module.
+
+Handles parsing Fusion Export files, matching them to depth maps,
+and generating/saving FSSIDECAR files using carry-forward logic.
+"""
+
+import glob
+import json
+import logging
+import math
+import os
+from typing import Any, Dict, List, Optional, Tuple
+
+try:
+    from moviepy.editor import VideoFileClip
+except ImportError:
+    # Fallback/stub for systems without moviepy
+    class VideoFileClip:
+        """Stub for systems without moviepy.
+
+        Provides a minimal interface compatible with the moviepy VideoFileClip
+        for systems where moviepy is not installed.
+        """
+
+        def __init__(self, *args, **kwargs):
+            """Initialize the stub VideoFileClip.
+
+            Args:
+                *args: Variable arguments (ignored)
+                **kwargs: Keyword arguments (ignored)
+            """
+            pass
+
+        def close(self):
+            """Close the video file (no-op for stub)."""
+            pass
+
+        @property
+        def fps(self):
+            """Get frames per second (returns None for stub)."""
+            return None
+
+        @property
+        def duration(self):
+            """Get video duration in seconds (returns None for stub)."""
+            return None
+
+
+logger = logging.getLogger(__name__)
+
+
+class FusionSidecarGenerator:
+    """
+    Handles parsing Fusion Export files and generating sidecar files.
+
+    This class processes Fusion (.fsexport) files which contain
+    metadata markers with depth/stereo parameters. It matches these
+    markers to depth map videos and generates sidecar files with
+    carry-forward logic for parameter inheritance.
+
+    Args:
+        master_gui: Reference to main GUI for status updates
+        sidecar_manager: SidecarConfigManager instance for file operations
+
+    Example:
+        >>> from dependency.stereocrafter_util import SidecarConfigManager
+        >>> from core.splatting.fusion_export import FusionSidecarGenerator
+        >>>
+        >>> sidecar_manager = SidecarConfigManager()
+        >>> generator = FusionSidecarGenerator(main_gui, sidecar_manager)
+        >>> generator.generate_sidecars()  # Opens file dialogs
+    """
+
+    # Configuration mapping Fusion export keys to sidecar keys
+    FUSION_PARAMETER_CONFIG = {
+        "convergence": {
+            "label": "Convergence Plane",
+            "type": float,
+            "default": 0.5,
+            "fusion_key": "Convergence",
+            "sidecar_key": "convergence_plane",
+            "decimals": 3,
+        },
+        "max_disparity": {
+            "label": "Max Disparity",
+            "type": float,
+            "default": 35.0,
+            "fusion_key": "MaxDisparity",
+            "sidecar_key": "max_disparity",
+            "decimals": 1,
+        },
+        "gamma": {
+            "label": "Gamma Correction",
+            "type": float,
+            "default": 1.0,
+            "fusion_key": "FrontGamma",
+            "sidecar_key": "gamma",
+            "decimals": 2,
+        },
+        "frame_overlap": {
+            "label": "Frame Overlap",
+            "type": float,
+            "default": 3,
+            "fusion_key": "Overlap",
+            "sidecar_key": "frame_overlap",
+            "decimals": 0,
+        },
+        "input_bias": {
+            "label": "Input Bias",
+            "type": float,
+            "default": 0.0,
+            "fusion_key": "Bias",
+            "sidecar_key": "input_bias",
+            "decimals": 2,
+        },
+        "left_border": {
+            "label": "Left Border",
+            "type": float,
+            "default": 0.0,
+            "fusion_key": "LeftBorder",
+            "sidecar_key": "left_border",
+            "decimals": 3,
+        },
+        "right_border": {
+            "label": "Right Border",
+            "type": float,
+            "default": 0.0,
+            "fusion_key": "RightBorder",
+            "sidecar_key": "right_border",
+            "decimals": 3,
+        },
+        "manual_border": {
+            "label": "Border Mode",
+            "type": str,
+            "default": "Off",
+            "fusion_key": "BorderMode",
+            "sidecar_key": "border_mode",
+            "decimals": 0,
+        },
+        "auto_border_l": {
+            "label": "Auto Border L",
+            "type": float,
+            "default": 0.0,
+            "fusion_key": "AutoBorderL",
+            "sidecar_key": "auto_border_L",
+            "decimals": 3,
+        },
+        "auto_border_r": {
+            "label": "Auto Border R",
+            "type": float,
+            "default": 0.0,
+            "fusion_key": "AutoBorderR",
+            "sidecar_key": "auto_border_R",
+            "decimals": 3,
+        },
+    }
+
+    def __init__(self, master_gui, sidecar_manager):
+        """Initialize the Fusion sidecar generator.
+
+        Args:
+            master_gui: GUI object with status_label and other UI elements
+            sidecar_manager: SidecarConfigManager for file operations
+        """
+        self.master_gui = master_gui
+        self.sidecar_manager = sidecar_manager
+        self.logger = logging.getLogger(__name__)
+
+    def _get_video_frame_count(self, file_path: str) -> int:
+        """Safely get the frame count of a video file using moviepy.
+
+        Args:
+            file_path: Path to the video file
+
+        Returns:
+            Number of frames, or 0 if determination fails
+        """
+        try:
+            clip = VideoFileClip(file_path)
+            fps = clip.fps
+            duration = clip.duration
+            if fps is None or duration is None:
+                fps = 24
+                if duration is None:
+                    return 0
+
+            frames = math.ceil(duration * fps)
+            clip.close()
+            return frames
+        except Exception as e:
+            self.logger.warning(
+                f"Error getting frame count for {os.path.basename(file_path)}: {e}"
+            )
+            return 0
+
+    def _load_and_validate_fsexport(self, file_path: str) -> Optional[List[Dict]]:
+        """Load, parse, and validate marker data from a Fusion Export file.
+
+        Args:
+            file_path: Path to the .fsexport file
+
+        Returns:
+            List of marker dictionaries, or None if invalid
+        """
+        try:
+            with open(file_path, "r") as f:
+                export_data = json.load(f)
+        except json.JSONDecodeError as e:
+            self.logger.error(
+                f"Failed to parse JSON in {os.path.basename(file_path)}: {e}"
+            )
+            return None
+        except Exception as e:
+            self.logger.error(
+                f"Failed to read {os.path.basename(file_path)}: {e}"
+            )
+            return None
+
+        markers = export_data.get("markers", [])
+        if not markers:
+            self.logger.warning("No 'markers' found in the export file.")
+            return None
+
+        # Sort markers by frame number (critical for carry-forward logic)
+        markers.sort(key=lambda m: m["frame"])
+        self.logger.info(
+            f"Loaded {len(markers)} markers from {os.path.basename(file_path)}."
+        )
+        return markers
+
+    def _scan_target_videos(
+        self, folder: str
+    ) -> Optional[List[Dict[str, Any]]]:
+        """Scan a folder for video files and compute their frame counts.
+
+        Args:
+            folder: Path to folder containing depth map videos
+
+        Returns:
+            List of video data dictionaries, or None if no videos found
+        """
+        video_extensions = ("*.mp4", "*.avi", "*.mov", "*.mkv")
+        found_files_paths = []
+        for ext in video_extensions:
+            found_files_paths.extend(glob.glob(os.path.join(folder, ext)))
+        sorted_files_paths = sorted(found_files_paths)
+
+        if not sorted_files_paths:
+            self.logger.warning(f"No video depth map files found in: {folder}")
+            return None
+
+        target_video_data = []
+        cumulative_frames = 0
+
+        for full_path in sorted_files_paths:
+            total_frames = self._get_video_frame_count(full_path)
+
+            if total_frames == 0:
+                self.logger.warning(
+                    f"Skipping {os.path.basename(full_path)} due to zero frame count."
+                )
+                continue
+
+            target_video_data.append(
+                {
+                    "full_path": full_path,
+                    "basename": os.path.basename(full_path),
+                    "total_frames": total_frames,
+                    "timeline_start_frame": cumulative_frames,
+                    "timeline_end_frame": cumulative_frames + total_frames - 1,
+                }
+            )
+            cumulative_frames += total_frames
+
+        self.logger.info(
+            f"Scanned {len(target_video_data)} video files. "
+            f"Total timeline frames: {cumulative_frames}."
+        )
+        return target_video_data
+
+    def _apply_parameters(
+        self,
+        target_videos: List[Dict[str, Any]],
+        markers: List[Dict[str, Any]],
+    ) -> Tuple[Dict[str, Any], int]:
+        """Apply parameters from markers to videos using carry-forward logic.
+
+        Args:
+            target_videos: List of video data dictionaries
+            markers: List of marker dictionaries from Fusion export
+
+        Returns:
+            Tuple of (sidecar_data, applied_count)
+        """
+        applied_count = 0
+
+        # Initialize last known values with defaults
+        last_param_vals = {}
+        for key, config in self.FUSION_PARAMETER_CONFIG.items():
+            last_param_vals[key] = config["default"]
+
+        for file_data in target_videos:
+            file_start_frame = file_data["timeline_start_frame"]
+
+            # Find most relevant marker (latest <= file_start_frame)
+            relevant_marker = None
+            for marker in markers:
+                if marker["frame"] <= file_start_frame:
+                    relevant_marker = marker
+                else:
+                    break
+
+            current_param_vals = last_param_vals.copy()
+
+            if relevant_marker and relevant_marker.get("values"):
+                marker_values = relevant_marker["values"]
+                updated_from_marker = False
+
+                for key, config in self.FUSION_PARAMETER_CONFIG.items():
+                    fusion_key = config["fusion_key"]
+                    default_val = config["default"]
+
+                    if fusion_key in marker_values:
+                        val = marker_values.get(fusion_key, default_val)
+                        try:
+                            current_param_vals[key] = config["type"](val)
+                            updated_from_marker = True
+                        except (ValueError, TypeError):
+                            self.logger.warning(
+                                f"Marker value for '{fusion_key}' is invalid ({val}). "
+                                "Using previous/default value."
+                            )
+
+                if updated_from_marker:
+                    applied_count += 1
+
+            # Update for carry-forward
+            last_param_vals = current_param_vals.copy()
+
+        # Build sidecar data from final parameter values
+        sidecar_data = {}
+        for key, config in self.FUSION_PARAMETER_CONFIG.items():
+            value = last_param_vals[key]
+            if config["type"] is bool:
+                sidecar_data[config["sidecar_key"]] = bool(value)
+            elif config["type"] is str:
+                # String values (like border_mode) don't need rounding
+                sidecar_data[config["sidecar_key"]] = str(value)
+            else:
+                sidecar_data[config["sidecar_key"]] = round(
+                    float(value), config["decimals"]
+                )
+
+        return sidecar_data, applied_count
+
+    def generate_sidecars(self, filedialog=None, messagebox=None) -> None:
+        """Main entry point for Fusion Export to Sidecar generation workflow.
+
+        Opens file dialogs to select the export file and target folder,
+        then generates sidecar files for matching depth maps.
+
+        Args:
+            filedialog: tkinter filedialog module (optional, defaults to tkinter.filedialog)
+            messagebox: tkinter messagebox module (optional, defaults to tkinter.messagebox)
+        """
+        # Use default tkinter modules if not provided
+        if filedialog is None:
+            from tkinter import filedialog
+        if messagebox is None:
+            from tkinter import messagebox
+
+        # 1. Select Fusion Export File
+        export_file_path = filedialog.askopenfilename(
+            defaultextension=".fsexport",
+            filetypes=[
+                ("Fusion Export Files", "*.fsexport.txt;*.fsexport"),
+                ("All Files", "*.*"),
+            ],
+            title="Select Fusion Export (.fsexport) File",
+        )
+        if not export_file_path:
+            self.master_gui.status_label.config(
+                text="Fusion export selection cancelled."
+            )
+            return
+
+        markers = self._load_and_validate_fsexport(export_file_path)
+        if markers is None:
+            self.master_gui.status_label.config(text="Fusion export loading failed.")
+            return
+
+        # 2. Select Target Depth Map Folder
+        target_folder = filedialog.askdirectory(
+            title="Select Target Depth Map Folder"
+        )
+        if not target_folder:
+            self.master_gui.status_label.config(
+                text="Depth map folder selection cancelled."
+            )
+            return
+
+        target_videos = self._scan_target_videos(target_folder)
+        if target_videos is None or not target_videos:
+            self.master_gui.status_label.config(text="No valid depth map videos found.")
+            return
+
+        # 3. Apply Parameters and Generate Sidecars
+        sidecar_data, applied_count = self._apply_parameters(target_videos, markers)
+
+        for file_data in target_videos:
+            base_name_without_ext = os.path.splitext(file_data["full_path"])[0]
+            json_filename = base_name_without_ext + ".fssidecar"
+
+            if not self.sidecar_manager.save_sidecar_data(json_filename, sidecar_data):
+                self.logger.error(
+                    f"Failed to save sidecar for {file_data['basename']}."
+                )
+
+        # 4. Final Status
+        if applied_count == 0:
+            self.master_gui.status_label.config(
+                text="Finished: No parameters were applied from the export file."
+            )
+        else:
+            self.master_gui.status_label.config(
+                text=f"Finished: Applied markers to {applied_count} files, "
+                f"generated {len(target_videos)} FSSIDECARs."
+            )
+        messagebox.showinfo(
+            "Sidecar Generation Complete",
+            f"Successfully processed {os.path.basename(export_file_path)} "
+            f"and generated {len(target_videos)} FSSIDECAR files.",
+        )
+
+    def generate_custom_sidecars(self, filedialog=None, messagebox=None) -> None:
+        """Generate sidecars with custom names without requiring video files.
+
+        Opens dialogs to select export file and output path, then generates
+        indexed sidecar files for each marker.
+
+        Args:
+            filedialog: tkinter filedialog module (optional, defaults to tkinter.filedialog)
+            messagebox: tkinter messagebox module (optional, defaults to tkinter.messagebox)
+        """
+        # Use default tkinter modules if not provided
+        if filedialog is None:
+            from tkinter import filedialog
+        if messagebox is None:
+            from tkinter import messagebox
+        # 1. Select Fusion Export File
+        export_file_path = filedialog.askopenfilename(
+            defaultextension=".fsexport",
+            filetypes=[
+                ("Fusion Export Files", "*.fsexport.txt;*.fsexport"),
+                ("All Files", "*.*"),
+            ],
+            title="Select Fusion Export (.fsexport) File",
+        )
+        if not export_file_path:
+            self.master_gui.status_label.config(
+                text="Fusion export selection cancelled."
+            )
+            return
+
+        markers = self._load_and_validate_fsexport(export_file_path)
+        if markers is None:
+            self.master_gui.status_label.config(text="Fusion export loading failed.")
+            return
+
+        # 2. Select Output Sidecar Path
+        custom_save_path = filedialog.asksaveasfilename(
+            defaultextension=".fssidecar",
+            filetypes=[("Sidecar Files", "*.fssidecar")],
+            title="Save Sidecar As",
+            initialfile=os.path.splitext(os.path.basename(export_file_path))[0],
+        )
+        if not custom_save_path:
+            self.master_gui.status_label.config(text="Custom sidecar export cancelled.")
+            return
+
+        # 3. Process Markers
+        applied_count = 0
+        last_param_vals = {
+            key: config["default"]
+            for key, config in self.FUSION_PARAMETER_CONFIG.items()
+        }
+
+        for i, marker in enumerate(markers):
+            current_param_vals = last_param_vals.copy()
+            if marker.get("values"):
+                marker_values = marker["values"]
+                updated_from_marker = False
+                for key, config in self.FUSION_PARAMETER_CONFIG.items():
+                    fusion_key = config["fusion_key"]
+                    if fusion_key in marker_values:
+                        try:
+                            current_param_vals[key] = config["type"](
+                                marker_values[fusion_key]
+                            )
+                            updated_from_marker = True
+                        except (ValueError, TypeError):
+                            pass
+                if updated_from_marker:
+                    applied_count += 1
+
+            # Prepare sidecar data
+            sidecar_data = {}
+            for key, config in self.FUSION_PARAMETER_CONFIG.items():
+                value = current_param_vals[key]
+                if config["type"] is bool:
+                    sidecar_data[config["sidecar_key"]] = bool(value)
+                elif config["type"] is str:
+                    # String values (like border_mode) don't need rounding
+                    sidecar_data[config["sidecar_key"]] = str(value)
+                else:
+                    sidecar_data[config["sidecar_key"]] = round(
+                        float(value), config["decimals"]
+                    )
+
+            # Determine filename
+            if len(markers) == 1:
+                target_filename = custom_save_path
+            else:
+                base, ext = os.path.splitext(custom_save_path)
+                target_filename = f"{base}_{i + 1:04d}{ext}"
+
+            if not self.sidecar_manager.save_sidecar_data(
+                target_filename, sidecar_data
+            ):
+                self.logger.error(f"Failed to save custom sidecar: {target_filename}")
+
+            last_param_vals = current_param_vals.copy()
+
+        # 4. Final Status
+        self.master_gui.status_label.config(
+            text=f"Finished: Generated {len(markers)} custom FSSIDECARs."
+        )
+        messagebox.showinfo(
+            "Custom Export Complete",
+            f"Successfully generated {len(markers)} custom FSSIDECAR files.",
+        )
diff --git a/core/splatting/gui_widgets.py b/core/splatting/gui_widgets.py
new file mode 100644
index 0000000..7e93ad3
--- /dev/null
+++ b/core/splatting/gui_widgets.py
@@ -0,0 +1,490 @@
+"""GUI widget creation utilities for Splatting GUI.
+
+Provides helper functions for creating common widget patterns used
+in the Splatting GUI, including labeled entries, folder selection rows,
+checkbox groups, and sliders with tooltips.
+"""
+
+import logging
+import tkinter as tk
+from tkinter import ttk
+from typing import Any, Callable, Dict, List, Optional, Tuple, Union
+
+logger = logging.getLogger(__name__)
+
+
+class Tooltip:
+    """Tooltip helper class for tkinter widgets.
+
+    Creates a simple tooltip that appears when hovering over a widget.
+
+    Args:
+        widget: The widget to attach the tooltip to
+        text: The tooltip text content
+    """
+
+    def __init__(self, widget: tk.Widget, text: str):
+        """Initialize the tooltip.
+
+        Args:
+            widget: The widget to attach the tooltip to
+            text: The tooltip text content
+        """
+        self.widget = widget
+        self.text = text
+        self.tooltip_window: Optional[tk.Toplevel] = None
+        self.widget.bind("<Enter>", self._show)
+        self.widget.bind("<Leave>", self._hide)
+
+    def _show(self, event=None):
+        """Display the tooltip."""
+        x, y, _, _ = self.widget.bbox("insert")
+        x += self.widget.winfo_rootx() + 25
+        y += self.widget.winfo_rooty() + 25
+
+        self.tooltip_window = tk.Toplevel(self.widget)
+        self.tooltip_window.wm_overrideredirect(True)
+        self.tooltip_window.wm_geometry(f"+{x}+{y}")
+
+        label = tk.Label(
+            self.tooltip_window,
+            text=self.text,
+            justify=tk.LEFT,
+            background="#ffffe0",
+            relief=tk.SOLID,
+            borderwidth=1,
+            font=("Segoe UI", 9),
+        )
+        label.pack()
+
+    def _hide(self, event=None):
+        """Hide the tooltip."""
+        if self.tooltip_window:
+            self.tooltip_window.destroy()
+            self.tooltip_window = None
+
+
+def create_tooltip(widget: tk.Widget, text: str) -> Tooltip:
+    """Create and attach a tooltip to a widget.
+
+    Args:
+        widget: The widget to attach the tooltip to
+        text: The tooltip text content
+
+    Returns:
+        Tooltip instance
+    """
+    return Tooltip(widget, text)
+
+
+def create_labeled_entry(
+    parent: tk.Widget,
+    label_text: str,
+    textvariable: tk.Variable,
+    tooltip_text: Optional[str] = None,
+    entry_width: int = 40,
+    row: int = 0,
+    column: int = 0,
+    sticky: str = "ew",
+    padx: int = 5,
+    pady: int = 2,
+) -> Tuple[ttk.Label, ttk.Entry]:
+    """Create a label and entry pair.
+
+    Creates a standard label and entry widget pair, commonly used for
+    text input fields in the GUI.
+
+    Args:
+        parent: Parent widget
+        label_text: Text for the label
+        textvariable: Tkinter variable for the entry
+        tooltip_text: Optional tooltip text for both widgets
+        entry_width: Width of the entry widget
+        row: Grid row for label
+        column: Grid column for label (entry goes in column+1)
+        sticky: Sticky option for grid
+        padx: Horizontal padding
+        pady: Vertical padding
+
+    Returns:
+        Tuple of (label_widget, entry_widget)
+    """
+    label = ttk.Label(parent, text=label_text)
+    label.grid(row=row, column=column, sticky="e", padx=padx, pady=pady)
+
+    entry = ttk.Entry(parent, textvariable=textvariable, width=entry_width)
+    entry.grid(row=row, column=column + 1, sticky=sticky, padx=padx, pady=pady)
+
+    if tooltip_text:
+        create_tooltip(label, tooltip_text)
+        create_tooltip(entry, tooltip_text)
+
+    return label, entry
+
+
+def create_folder_selection_row(
+    parent: tk.Widget,
+    label_text: str,
+    textvariable: tk.StringVar,
+    browse_folder_callback: Callable,
+    select_file_callback: Optional[Callable] = None,
+    file_types: Optional[List[Tuple[str, str]]] = None,
+    tooltip_key: Optional[str] = None,
+    row: int = 0,
+    create_tooltips_func: Optional[Callable] = None,
+) -> Dict[str, tk.Widget]:
+    """Create a folder selection row with label, entry, and browse buttons.
+
+    Creates the standard pattern of: Label | Entry | Browse Folder | [Select File]
+    Used for input/output folder selection throughout the GUI.
+
+    Args:
+        parent: Parent widget (usually a LabelFrame)
+        label_text: Text for the label
+        textvariable: StringVar for the entry
+        browse_folder_callback: Callback for folder browse button
+        select_file_callback: Optional callback for file select button
+        file_types: Optional file types for file dialog [(description, pattern), ...]
+        tooltip_key: Key for tooltip lookup (if create_tooltips_func provided)
+        row: Grid row
+        create_tooltips_func: Optional function to create tooltips by key
+
+    Returns:
+        Dictionary of created widgets
+    """
+    widgets = {}
+
+    # Label
+    lbl = ttk.Label(parent, text=label_text)
+    lbl.grid(row=row, column=0, sticky="e", padx=5, pady=0)
+    widgets["label"] = lbl
+
+    # Entry
+    entry = ttk.Entry(parent, textvariable=textvariable)
+    entry.grid(row=row, column=1, padx=5, pady=0, sticky="ew")
+    widgets["entry"] = entry
+
+    # Browse Folder button
+    btn_folder = ttk.Button(parent, text="Browse Folder", command=browse_folder_callback)
+    btn_folder.grid(row=row, column=2, padx=2, pady=0)
+    widgets["btn_browse_folder"] = btn_folder
+
+    # Select File button (optional)
+    if select_file_callback:
+        btn_file = ttk.Button(parent, text="Select File", command=select_file_callback)
+        btn_file.grid(row=row, column=3, padx=2, pady=0)
+        widgets["btn_select_file"] = btn_file
+
+        if create_tooltips_func and tooltip_key:
+            create_tooltips_func(lbl, tooltip_key)
+            create_tooltips_func(entry, tooltip_key)
+            create_tooltips_func(btn_folder, f"{tooltip_key}_folder")
+            create_tooltips_func(btn_file, f"{tooltip_key}_file")
+    else:
+        if create_tooltips_func and tooltip_key:
+            create_tooltips_func(lbl, tooltip_key)
+            create_tooltips_func(entry, tooltip_key)
+            create_tooltips_func(btn_folder, tooltip_key)
+
+    return widgets
+
+
+def create_checkbox_group(
+    parent: tk.Widget,
+    checkboxes: List[Dict[str, Any]],
+    row: int = 0,
+    columns: int = 3,
+) -> List[ttk.Checkbutton]:
+    """Create a group of checkboxes in a grid layout.
+
+    Args:
+        parent: Parent widget
+        checkboxes: List of checkbox configs with keys:
+            - text: Checkbox label
+            - variable: BooleanVar
+            - command: Optional callback
+            - tooltip: Optional tooltip text
+            - width: Optional width
+        row: Starting grid row
+        columns: Number of columns in the grid
+
+    Returns:
+        List of created Checkbutton widgets
+    """
+    created = []
+
+    for i, config in enumerate(checkboxes):
+        col = i % columns
+        current_row = row + (i // columns)
+
+        cb = ttk.Checkbutton(
+            parent,
+            text=config.get("text", ""),
+            variable=config.get("variable"),
+            command=config.get("command"),
+            width=config.get("width", None),
+        )
+        cb.grid(row=current_row, column=col, sticky="w", padx=5, pady=2)
+
+        if config.get("tooltip"):
+            create_tooltip(cb, config["tooltip"])
+
+        created.append(cb)
+
+    return created
+
+
+def create_labeled_slider(
+    parent: tk.Widget,
+    label_text: str,
+    variable: tk.Variable,
+    from_value: float,
+    to_value: float,
+    resolution: float = 0.1,
+    orient: str = tk.HORIZONTAL,
+    length: int = 200,
+    tooltip_text: Optional[str] = None,
+    command: Optional[Callable] = None,
+    show_value: bool = True,
+    row: int = 0,
+    column: int = 0,
+) -> Tuple[ttk.Label, ttk.Scale, Optional[ttk.Label]]:
+    """Create a labeled slider with optional value display.
+
+    Args:
+        parent: Parent widget
+        label_text: Text for the label
+        variable: Tkinter variable (DoubleVar or IntVar)
+        from_value: Minimum value
+        to_value: Maximum value
+        resolution: Step size
+        orient: Orientation (HORIZONTAL or VERTICAL)
+        length: Length of the slider in pixels
+        tooltip_text: Optional tooltip text
+        command: Optional callback when value changes
+        show_value: Whether to show value label next to slider
+        row: Grid row
+        column: Grid column
+
+    Returns:
+        Tuple of (label_widget, slider_widget, value_label or None)
+    """
+    # Label
+    label = ttk.Label(parent, text=label_text)
+    label.grid(row=row, column=column, sticky="e", padx=5, pady=2)
+
+    # Slider frame (to hold slider + value)
+    slider_frame = ttk.Frame(parent)
+    slider_frame.grid(row=row, column=column + 1, sticky="ew", padx=5, pady=2)
+    slider_frame.grid_columnconfigure(0, weight=1)
+
+    # Slider
+    slider = ttk.Scale(
+        slider_frame,
+        from_=from_value,
+        to=to_value,
+        orient=orient,
+        length=length,
+        variable=variable,
+        command=command,
+    )
+    slider.pack(side=tk.LEFT, fill=tk.X, expand=True)
+
+    # Value label (optional)
+    value_label = None
+    if show_value:
+        value_label = ttk.Label(slider_frame, textvariable=variable, width=6)
+        value_label.pack(side=tk.LEFT, padx=(5, 0))
+
+    if tooltip_text:
+        create_tooltip(label, tooltip_text)
+        create_tooltip(slider, tooltip_text)
+        if value_label:
+            create_tooltip(value_label, tooltip_text)
+
+    return label, slider, value_label
+
+
+def create_button_group(
+    parent: tk.Widget,
+    buttons: List[Dict[str, Any]],
+    row: int = 0,
+    column: int = 0,
+    orientation: str = "horizontal",
+) -> List[ttk.Button]:
+    """Create a group of buttons.
+
+    Args:
+        parent: Parent widget
+        buttons: List of button configs with keys:
+            - text: Button label
+            - command: Callback function
+            - tooltip: Optional tooltip text
+            - state: Optional initial state ("normal", "disabled")
+            - width: Optional width
+        row: Grid row
+        column: Grid column
+        orientation: "horizontal" or "vertical"
+
+    Returns:
+        List of created Button widgets
+    """
+    created = []
+    frame = ttk.Frame(parent)
+    frame.grid(row=row, column=column, sticky="ew", padx=5, pady=2)
+
+    for i, config in enumerate(buttons):
+        btn = ttk.Button(
+            frame,
+            text=config.get("text", ""),
+            command=config.get("command"),
+            width=config.get("width", None),
+        )
+
+        if config.get("state"):
+            btn.config(state=config["state"])
+
+        if orientation == "horizontal":
+            btn.pack(side=tk.LEFT, padx=2)
+        else:
+            btn.pack(side=tk.TOP, pady=2)
+
+        if config.get("tooltip"):
+            create_tooltip(btn, config["tooltip"])
+
+        created.append(btn)
+
+    return created
+
+
+def create_section_frame(
+    parent: tk.Widget,
+    title: str,
+    row: int = 0,
+    column: int = 0,
+    columnspan: int = 1,
+    padx: int = 10,
+    pady: int = 2,
+) -> ttk.LabelFrame:
+    """Create a labeled frame section.
+
+    Args:
+        parent: Parent widget
+        title: Frame title text
+        row: Grid row
+        column: Grid column
+        columnspan: Number of columns to span
+        padx: Horizontal padding
+        pady: Vertical padding
+
+    Returns:
+        Created LabelFrame widget
+    """
+    frame = ttk.LabelFrame(parent, text=title)
+    frame.grid(
+        row=row, column=column, columnspan=columnspan, sticky="nsew", padx=padx, pady=pady
+    )
+    return frame
+
+
+def create_dropdown(
+    parent: tk.Widget,
+    label_text: str,
+    variable: tk.StringVar,
+    values: List[str],
+    command: Optional[Callable] = None,
+    tooltip_text: Optional[str] = None,
+    row: int = 0,
+    column: int = 0,
+    width: int = 20,
+) -> Tuple[ttk.Label, ttk.Combobox]:
+    """Create a labeled dropdown (Combobox).
+
+    Args:
+        parent: Parent widget
+        label_text: Text for the label
+        variable: StringVar for the dropdown
+        values: List of dropdown values
+        command: Optional callback when selection changes
+        tooltip_text: Optional tooltip text
+        row: Grid row
+        column: Grid column
+        width: Width of the combobox
+
+    Returns:
+        Tuple of (label_widget, combobox_widget)
+    """
+    label = ttk.Label(parent, text=label_text)
+    label.grid(row=row, column=column, sticky="e", padx=5, pady=2)
+
+    combobox = ttk.Combobox(
+        parent,
+        textvariable=variable,
+        values=values,
+        width=width,
+        state="readonly",
+    )
+    combobox.grid(row=row, column=column + 1, sticky="w", padx=5, pady=2)
+
+    if command:
+        combobox.bind("<<ComboboxSelected>>", lambda e: command())
+
+    if tooltip_text:
+        create_tooltip(label, tooltip_text)
+        create_tooltip(combobox, tooltip_text)
+
+    return label, combobox
+
+
+def configure_grid_weights(
+    widget: tk.Widget,
+    column_weights: Optional[List[int]] = None,
+    row_weights: Optional[List[int]] = None,
+) -> None:
+    """Configure grid column and row weights for responsive layout.
+
+    Args:
+        widget: The widget whose grid to configure
+        column_weights: List of weights for each column (None = 1 for all)
+        row_weights: List of weights for each row (None = no configuration)
+    """
+    if column_weights:
+        for i, weight in enumerate(column_weights):
+            widget.grid_columnconfigure(i, weight=weight)
+
+    if row_weights:
+        for i, weight in enumerate(row_weights):
+            widget.grid_rowconfigure(i, weight=weight)
+
+
+# Predefined tooltip texts for common widgets
+COMMON_TOOLTIPS = {
+    "input_source_clips": "Folder or file containing source video clips",
+    "input_source_clips_folder": "Browse for a folder containing source video clips",
+    "input_source_clips_file": "Select a single source video file",
+    "input_depth_maps": "Folder or file containing depth maps (*.mp4, *.npz)",
+    "input_depth_maps_folder": "Browse for a folder containing depth maps",
+    "input_depth_maps_file": "Select a single depth map file",
+    "output_splatted": "Output folder for splatted (right-eye) videos",
+    "enable_full_res": "Enable full resolution output processing",
+    "enable_low_res": "Enable low resolution output processing",
+    "full_res_batch_size": "Number of frames to process per batch at full resolution",
+    "low_res_batch_size": "Number of frames to process per batch at low resolution",
+    "max_disp": "Maximum disparity (3D depth intensity). Typical range: 5-40.",
+    "convergence": "Zero parallax plane position (0.0=foreground, 1.0=background)",
+    "depth_gamma": "Gamma correction for depth map (1.0=linear, <1.0=compress far, >1.0=emphasize far)",
+    "dual_output": "Output both left and right eye videos (for external 3D encoding)",
+    "multi_map": "Enable multiple depth map selection for comparison",
+}
+
+
+def get_common_tooltip(key: str) -> Optional[str]:
+    """Get predefined tooltip text for a common key.
+
+    Args:
+        key: Tooltip key
+
+    Returns:
+        Tooltip text or None if not found
+    """
+    return COMMON_TOOLTIPS.get(key)
diff --git a/core/splatting/main_gui.py b/core/splatting/main_gui.py
new file mode 100644
index 0000000..4f265fb
--- /dev/null
+++ b/core/splatting/main_gui.py
@@ -0,0 +1,1428 @@
+"""Main GUI module for StereoCrafter Splatting application.
+
+This module provides the main SplatterGUI class which serves as a coordinator
+delegating functionality to specialized modules in the core.splatting package.
+
+The GUI follows a modular architecture where:
+- ConfigManager handles configuration loading/saving
+- ThemeManager handles dark/light mode switching
+- BorderScanner handles automatic border detection
+- ConvergenceEstimatorWrapper handles auto-convergence
+- PreviewRenderer handles preview frame generation
+- BatchProcessor handles batch video processing
+- GUI widgets are created using helpers from gui_widgets
+
+Example:
+    from core.splatting.main_gui import SplatterGUI
+    app = SplatterGUI()
+    app.mainloop()
+"""
+
+# Standard library imports
+import gc
+import glob
+import json
+import logging
+import math
+import os
+import queue
+import re
+import shutil
+import subprocess
+import threading
+import time
+from typing import Any, Dict, List, Optional, Tuple
+
+# Third-party imports
+import cv2
+import numpy as np
+import torch
+import torch.nn.functional as F
+from decord import VideoReader, cpu
+from PIL import Image
+from ttkthemes import ThemedTk
+
+# tkinter imports
+import tkinter as tk
+from tkinter import filedialog, messagebox, ttk
+import tkinter.font as tkfont
+
+# Local/core imports - Stage 9: Updated to use modular architecture
+from core.common import ThemeManager
+from core.splatting import (
+    ConfigManager,
+    load_config,
+    save_config,
+    SPLATTER_DEFAULT_CONFIG,
+    BorderScanner,
+    ConvergenceEstimatorWrapper,
+    ForwardWarpStereo,
+    FusionSidecarGenerator,
+    PreviewRenderer,
+    BatchProcessor,
+    ProcessingTask,
+    ProcessingSettings,
+    BatchSetupResult,
+    compute_global_depth_stats,
+    load_pre_rendered_depth,
+    FFmpegDepthPipeReader,
+    DEPTH_VIS_TV10_BLACK_NORM,
+    DEPTH_VIS_TV10_WHITE_NORM,
+    # GUI Widget helpers
+    Tooltip,
+    create_tooltip,
+    create_labeled_entry,
+    create_folder_selection_row,
+    create_checkbox_group,
+    create_labeled_slider,
+    create_button_group,
+    create_section_frame,
+    create_dropdown,
+    configure_grid_weights,
+    get_common_tooltip,
+    COMMON_TOOLTIPS,
+)
+
+# Dependency imports
+from dependency.stereocrafter_util import (
+    logger,
+    get_video_stream_info,
+    draw_progress_bar,
+    check_cuda_availability,
+    release_cuda_memory,
+    CUDA_AVAILABLE,
+    set_util_logger_level,
+    start_ffmpeg_pipe_process,
+    custom_blur,
+    custom_dilate,
+    custom_dilate_left,
+    create_single_slider_with_label_updater,
+    create_dual_slider_layout,
+    SidecarConfigManager,
+    apply_dubois_anaglyph,
+    apply_optimized_anaglyph,
+)
+
+from dependency.video_previewer import VideoPreviewer
+
+try:
+    from moviepy.editor import VideoFileClip
+except ImportError:
+    # Fallback/stub for systems without moviepy
+    class VideoFileClip:
+        def __init__(self, *args, **kwargs):
+            logging.warning("moviepy.editor not found. Frame counting disabled.")
+
+        def close(self):
+            pass
+
+        @property
+        def fps(self):
+            return None
+
+        @property
+        def duration(self):
+            return None
+
+
+# Version information
+GUI_VERSION = "26-01-30.0"
+
+
+class SplatterGUI(ThemedTk):
+    """Main GUI class for StereoCrafter Splatting application.
+    
+    This class serves as a coordinator, delegating functionality to specialized
+    modules while maintaining GUI state and handling user interactions.
+    
+    The class initializes and coordinates:
+    - ConfigManager: Configuration loading/saving
+    - ThemeManager: Dark/light theme management
+    - BorderScanner: Automatic border detection
+    - ConvergenceEstimatorWrapper: Auto-convergence estimation
+    - PreviewRenderer: Preview frame generation
+    - BatchProcessor: Batch video processing
+    
+    Attributes:
+        UI_PROCESS_COL_MIN: Minimum width for process settings column
+        UI_DEPTH_COL_MIN: Minimum width for depth adjustment column
+        APP_CONFIG_DEFAULTS: Default configuration values
+        SIDECAR_KEY_MAP: Mapping between sidecar and internal config keys
+    """
+
+    # --- UI MINIMUM WIDTHS ---
+    UI_PROCESS_COL_MIN = 330
+    UI_DEPTH_COL_MIN = 520
+
+    # --- GLOBAL CONFIGURATION DICTIONARY ---
+    APP_CONFIG_DEFAULTS = {
+        # File Extensions
+        "SIDECAR_EXT": ".fssidecar",
+        "OUTPUT_SIDECAR_EXT": ".spsidecar",
+        "DEFAULT_CONFIG_FILENAME": "config_splat.splatcfg",
+        # GUI/Processing Defaults (Used for reset/fallback)
+        "MAX_DISP": "30.0",
+        "CONV_POINT": "0.5",
+        "PROC_LENGTH": "-1",
+        "BATCH_SIZE_FULL": "10",
+        "BATCH_SIZE_LOW": "15",
+        "CRF_OUTPUT": "23",
+        # Depth Processing Defaults
+        "DEPTH_GAMMA": "1.0",
+        "DEPTH_DILATE_SIZE_X": "3",
+        "DEPTH_DILATE_SIZE_Y": "3",
+        "DEPTH_BLUR_SIZE_X": "5",
+        "DEPTH_BLUR_SIZE_Y": "5",
+        "DEPTH_DILATE_LEFT": "0",
+        "DEPTH_BLUR_LEFT": "0",
+        "DEPTH_BLUR_LEFT_MIX": "0.5",
+        "BORDER_WIDTH": "0.0",
+        "BORDER_BIAS": "0.0",
+        "BORDER_LEFT": "0.0",
+        "BORDER_RIGHT": "0.0",
+        "BORDER_MODE": "Off",
+        "AUTO_BORDER_L": "0.0",
+        "AUTO_BORDER_R": "0.0",
+    }
+
+    # Maps Sidecar JSON Key to the internal variable key
+    SIDECAR_KEY_MAP = {
+        "convergence_plane": "CONV_POINT",
+        "max_disparity": "MAX_DISP",
+        "gamma": "DEPTH_GAMMA",
+        "depth_dilate_size_x": "DEPTH_DILATE_SIZE_X",
+        "depth_dilate_size_y": "DEPTH_DILATE_SIZE_Y",
+        "depth_blur_size_x": "DEPTH_BLUR_SIZE_X",
+        "depth_blur_size_y": "DEPTH_BLUR_SIZE_Y",
+        "depth_dilate_left": "DEPTH_DILATE_LEFT",
+        "depth_blur_left": "DEPTH_BLUR_LEFT",
+        "depth_blur_left_mix": "DEPTH_BLUR_LEFT_MIX",
+        "frame_overlap": "FRAME_OVERLAP",
+        "input_bias": "INPUT_BIAS",
+        "selected_depth_map": "SELECTED_DEPTH_MAP",
+        "left_border": "BORDER_LEFT",
+        "right_border": "BORDER_RIGHT",
+        "border_mode": "BORDER_MODE",
+        "auto_border_L": "AUTO_BORDER_L",
+        "auto_border_R": "AUTO_BORDER_R",
+    }
+
+    MOVE_TO_FINISHED_ENABLED = True
+
+    def __init__(self):
+        """Initialize the SplatterGUI application.
+        
+        Sets up the main window, initializes coordinator modules,
+        creates tkinter variables, builds the UI, and starts the
+        main event loop components.
+        """
+        super().__init__(theme="default")
+        self.title(f"Stereocrafter Splatting (Batch) {GUI_VERSION}")
+
+        # --- Stage 9: Initialize coordinator modules ---
+        self.config_manager = ConfigManager(
+            defaults=SPLATTER_DEFAULT_CONFIG,
+            config_filename="config_splat.splatcfg"
+        )
+        self.theme_manager = None  # Initialized after dark_mode_var is created
+        self.border_scanner = None  # Initialized after widgets are created
+        self.convergence_estimator = ConvergenceEstimatorWrapper()
+        self.preview_renderer = PreviewRenderer(cuda_available=CUDA_AVAILABLE)
+        self.batch_processor = None  # Initialized when processing starts
+        
+        # Load configuration
+        self.app_config = self.config_manager.load()
+        self.help_texts = {}
+        self._load_help_texts()
+        
+        # Sidecar manager (from dependency)
+        self.sidecar_manager = SidecarConfigManager()
+        
+        # Caches and state
+        self._dp_total_est_cache = {}
+        self._dp_total_true_cache = {}
+        self._dp_total_true_active_sig = None
+        self._dp_total_true_active_val = None
+        self._auto_pass_csv_cache = None
+        self._auto_pass_csv_path = None
+        self._auto_conv_cache = {"Average": None, "Peak": None}
+        self._auto_conv_cached_path = None
+        self._is_auto_conv_running = False
+        self._preview_debounce_timer = None
+        self.slider_label_updaters = []
+        self.set_convergence_value_programmatically = None
+        self._clip_norm_cache: Dict[str, Tuple[float, float]] = {}
+        self._gn_warning_shown = False
+
+        # Startup flag
+        self._is_startup = True
+        self.debug_mode_var = tk.BooleanVar(
+            value=self.app_config.get("debug_mode_enabled", False)
+        )
+        self._debug_logging_enabled = False
+        
+        # Window geometry
+        self.window_x = self.app_config.get("window_x", None)
+        self.window_y = self.app_config.get("window_y", None)
+        self.window_width = self.app_config.get("window_width", 620)
+        self.window_height = self.app_config.get("window_height", 750)
+
+        # --- Initialize all tkinter variables ---
+        self._init_variables()
+        
+        # --- Stage 9: Initialize theme manager after dark_mode_var exists ---
+        self.theme_manager = ThemeManager(
+            dark_mode_var=self.dark_mode_var,
+            config=self.app_config
+        )
+
+        # Processing control
+        self.stop_event = threading.Event()
+        self.progress_queue = queue.Queue()
+        self.processing_thread = None
+
+        # Create UI
+        self._create_widgets()
+        self._setup_keyboard_shortcuts()
+        self.style = ttk.Style()
+
+        # Apply theme and geometry
+        self.update_idletasks()
+        self._apply_theme(is_startup=True)
+        self._set_saved_geometry()
+        self._is_startup = False
+        self._configure_logging()
+
+        # Initialize widget states
+        self.after(10, self.toggle_processing_settings_fields)
+        self.after(10, self._toggle_sidecar_update_button_state)
+
+        # Multi-map initialization
+        if self.multi_map_var.get():
+            self.after(15, self._on_multi_map_toggle)
+
+        # Apply preview overlays
+        self.after(20, self._apply_preview_overlay_toggles)
+
+        # Start queue checker
+        self.after(100, self.check_queue)
+
+        # Bind closing protocol
+        self.protocol("WM_DELETE_WINDOW", self.exit_app)
+
+        # Slider widgets list
+        self.slider_widgets = []
+        
+        # --- Stage 9: Initialize border scanner after widgets are created ---
+        self.border_scanner = BorderScanner(gui_context=self)
+
+    def _init_variables(self):
+        """Initialize all tkinter variables from configuration.
+        
+        Creates StringVar, BooleanVar, and other tkinter variables
+        with values loaded from the configuration or defaults.
+        """
+        defaults = self.APP_CONFIG_DEFAULTS
+
+        # Theme and display
+        self.dark_mode_var = tk.BooleanVar(
+            value=self.app_config.get("dark_mode_enabled", False)
+        )
+
+        # Input/Output paths
+        self.input_source_clips_var = tk.StringVar(
+            value=self.app_config.get("input_source_clips", "./input_source_clips")
+        )
+        self.input_depth_maps_var = tk.StringVar(
+            value=self.app_config.get("input_depth_maps", "./input_depth_maps")
+        )
+        self.output_splatted_var = tk.StringVar(
+            value=self.app_config.get("output_splatted", "./output_splatted")
+        )
+
+        # Multi-map mode
+        self.multi_map_var = tk.BooleanVar(
+            value=bool(self.app_config.get("multi_map_enabled", False))
+        )
+        self.selected_depth_map_var = tk.StringVar(value="")
+        self.depth_map_subfolders = []
+        self.depth_map_radio_buttons = []
+        self.depth_map_radio_dict = {}
+        self._current_video_sidecar_map = None
+        self._suppress_sidecar_map_update = False
+        self._last_loaded_source_video = None
+        
+        # Trace depth map folder changes
+        self.input_depth_maps_var.trace_add(
+            "write", lambda *args: self._on_depth_map_folder_changed()
+        )
+
+        # Processing settings
+        self.max_disp_var = tk.StringVar(
+            value=self.app_config.get("max_disp", defaults["MAX_DISP"])
+        )
+        self.process_length_var = tk.StringVar(
+            value=self.app_config.get("process_length", defaults["PROC_LENGTH"])
+        )
+        self.process_from_var = tk.StringVar(value="")
+        self.process_to_var = tk.StringVar(value="")
+        self.batch_size_var = tk.StringVar(
+            value=self.app_config.get("batch_size", defaults["BATCH_SIZE_FULL"])
+        )
+
+        # Output settings
+        self.dual_output_var = tk.BooleanVar(
+            value=self.app_config.get("dual_output", False)
+        )
+        self.enable_global_norm_var = tk.BooleanVar(
+            value=self.app_config.get("enable_global_norm", False)
+        )
+        self.enable_full_res_var = tk.BooleanVar(
+            value=self.app_config.get("enable_full_resolution", True)
+        )
+        self.enable_low_res_var = tk.BooleanVar(
+            value=self.app_config.get("enable_low_resolution", True)
+        )
+        self.pre_res_width_var = tk.StringVar(
+            value=self.app_config.get("pre_res_width", "1024")
+        )
+        self.pre_res_height_var = tk.StringVar(
+            value=self.app_config.get("pre_res_height", "512")
+        )
+        self.low_res_batch_size_var = tk.StringVar(
+            value=self.app_config.get("low_res_batch_size", defaults["BATCH_SIZE_LOW"])
+        )
+        self.zero_disparity_anchor_var = tk.StringVar(
+            value=self.app_config.get("convergence_point", defaults["CONV_POINT"])
+        )
+        self.output_crf_var = tk.StringVar(
+            value=self.app_config.get("output_crf", defaults["CRF_OUTPUT"])
+        )
+        
+        # Separate CRF values for Full vs Low output
+        _legacy_crf = self.app_config.get("output_crf", defaults["CRF_OUTPUT"])
+        self.output_crf_full_var = tk.StringVar(
+            value=self.app_config.get("output_crf_full", _legacy_crf)
+        )
+        self.output_crf_low_var = tk.StringVar(
+            value=self.app_config.get("output_crf_low", _legacy_crf)
+        )
+        
+        # Output color metadata tags
+        self.color_tags_mode_var = tk.StringVar(
+            value=self.app_config.get("color_tags_mode", "Auto")
+        )
+
+        # Dev Tools
+        self.skip_lowres_preproc_var = tk.BooleanVar(
+            value=bool(self.app_config.get("skip_lowres_preproc", False))
+        )
+        self.track_dp_total_true_on_render_var = tk.BooleanVar(value=False)
+
+        self.move_to_finished_var = tk.BooleanVar(
+            value=self.app_config.get("move_to_finished", True)
+        )
+
+        # Preview overlays
+        self.crosshair_enabled_var = tk.BooleanVar(
+            value=bool(self.app_config.get("crosshair_enabled", False))
+        )
+        self.crosshair_white_var = tk.BooleanVar(
+            value=bool(self.app_config.get("crosshair_white", False))
+        )
+        self.crosshair_multi_var = tk.BooleanVar(
+            value=bool(self.app_config.get("crosshair_multi", False))
+        )
+        self.depth_pop_enabled_var = tk.BooleanVar(
+            value=bool(self.app_config.get("depth_pop_enabled", False))
+        )
+
+        self.auto_convergence_mode_var = tk.StringVar(
+            value=self.app_config.get("auto_convergence_mode", "Off")
+        )
+
+        # Depth pre-processing
+        self.depth_gamma_var = tk.StringVar(
+            value=self.app_config.get("depth_gamma", defaults["DEPTH_GAMMA"])
+        )
+        
+        # Handle backward compatibility for dilate size
+        _ddx = self.app_config.get(
+            "depth_dilate_size_x", defaults["DEPTH_DILATE_SIZE_X"]
+        )
+        _ddy = self.app_config.get(
+            "depth_dilate_size_y", defaults["DEPTH_DILATE_SIZE_Y"]
+        )
+        try:
+            _ddx_f = float(_ddx)
+            if 30.0 < _ddx_f <= 40.0:
+                _ddx = -(_ddx_f - 30.0)
+        except Exception:
+            pass
+        try:
+            _ddy_f = float(_ddy)
+            if 30.0 < _ddy_f <= 40.0:
+                _ddy = -(_ddy_f - 30.0)
+        except Exception:
+            pass
+        
+        self.depth_dilate_size_x_var = tk.StringVar(value=str(_ddx))
+        self.depth_dilate_size_y_var = tk.StringVar(value=str(_ddy))
+        self.depth_blur_size_x_var = tk.StringVar(
+            value=self.app_config.get("depth_blur_size_x", defaults["DEPTH_BLUR_SIZE_X"])
+        )
+        self.depth_blur_size_y_var = tk.StringVar(
+            value=self.app_config.get("depth_blur_size_y", defaults["DEPTH_BLUR_SIZE_Y"])
+        )
+        self.depth_dilate_left_var = tk.StringVar(
+            value=self.app_config.get("depth_dilate_left", defaults["DEPTH_DILATE_LEFT"])
+        )
+        self.depth_blur_left_var = tk.StringVar(
+            value=self.app_config.get("depth_blur_left", defaults["DEPTH_BLUR_LEFT"])
+        )
+        self.depth_blur_left_mix_var = tk.StringVar(
+            value=self.app_config.get("depth_blur_left_mix", defaults["DEPTH_BLUR_LEFT_MIX"])
+        )
+
+        # Sidecar control toggles
+        self.enable_sidecar_gamma_var = tk.BooleanVar(
+            value=self.app_config.get("enable_sidecar_gamma", True)
+        )
+        self.enable_sidecar_blur_dilate_var = tk.BooleanVar(
+            value=self.app_config.get("enable_sidecar_blur_dilate", True)
+        )
+        self.update_slider_from_sidecar_var = tk.BooleanVar(
+            value=self.app_config.get("update_slider_from_sidecar", True)
+        )
+        self.auto_save_sidecar_var = tk.BooleanVar(
+            value=self.app_config.get("auto_save_sidecar", False)
+        )
+
+        # Border settings
+        self.border_width_var = tk.StringVar(
+            value=self.app_config.get("border_width", defaults["BORDER_WIDTH"])
+        )
+        self.border_bias_var = tk.StringVar(
+            value=self.app_config.get("border_bias", defaults["BORDER_BIAS"])
+        )
+        self.border_mode_var = tk.StringVar(
+            value=self.app_config.get("border_mode", defaults["BORDER_MODE"])
+        )
+        self.auto_border_L_var = tk.StringVar(
+            value=self.app_config.get("auto_border_L", defaults["AUTO_BORDER_L"])
+        )
+        self.auto_border_R_var = tk.StringVar(
+            value=self.app_config.get("auto_border_R", defaults["AUTO_BORDER_R"])
+        )
+
+        # Add traces for automatic border calculation
+        self.zero_disparity_anchor_var.trace_add(
+            "write", self._on_convergence_or_disparity_changed
+        )
+        self.max_disp_var.trace_add("write", self._on_convergence_or_disparity_changed)
+        self.border_mode_var.trace_add("write", self._on_border_mode_change)
+
+        # Previewer settings
+        self.preview_source_var = tk.StringVar(
+            value=self.app_config.get("preview_source", "Splat Result")
+        )
+        self.preview_size_var = tk.StringVar(
+            value=self.app_config.get("preview_size", "75%")
+        )
+
+        # Current processing information display
+        self.processing_filename_var = tk.StringVar(value="N/A")
+        self.processing_resolution_var = tk.StringVar(value="N/A")
+        self.processing_frames_var = tk.StringVar(value="N/A")
+        self.processing_disparity_var = tk.StringVar(value="N/A")
+        self.processing_convergence_var = tk.StringVar(value="N/A")
+        self.processing_task_name_var = tk.StringVar(value="N/A")
+        self.processing_gamma_var = tk.StringVar(value="N/A")
+        self.processing_map_var = tk.StringVar(value="N/A")
+
+        # Widget management
+        self.widgets_to_disable = []
+
+    def _create_widgets(self):
+        """Create all GUI widgets and layout.
+        
+        This method builds the complete user interface including:
+        - Menu bar
+        - Input/Output folder selection
+        - Processing settings
+        - Depth adjustment controls
+        - Border settings
+        - Preview panel
+        - Control buttons
+        - Status bar
+        
+        Uses helper functions from core.splatting.gui_widgets where applicable.
+        """
+        # --- Menu Bar ---
+        self._create_menu_bar()
+        
+        # --- Main Container Frame ---
+        self.main_container = ttk.Frame(self)
+        self.main_container.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
+
+        # --- Input/Output Folders Frame ---
+        self._create_folder_selection_frame()
+
+        # --- Process Resolution and Settings Frame ---
+        self._create_processing_settings_frame()
+
+        # --- Depth Adjustment Frame ---
+        self._create_depth_adjustment_frame()
+
+        # --- Preview Frame ---
+        self._create_preview_frame()
+
+        # --- Control Buttons Frame ---
+        self._create_control_buttons_frame()
+
+        # --- Current Processing Information Frame ---
+        self._create_processing_info_frame()
+
+        # --- Status Bar ---
+        self._create_status_bar()
+
+    def _create_menu_bar(self):
+        """Create the menu bar with File, Settings, Tools, and Help menus."""
+        self.menubar = tk.Menu(self)
+        self.config(menu=self.menubar)
+
+        # File Menu
+        self.file_menu = tk.Menu(self.menubar, tearoff=0)
+        self.menubar.add_cascade(label="File", menu=self.file_menu)
+        self.file_menu.add_command(
+            label="Load Settings", command=self.load_settings
+        )
+        self.file_menu.add_command(
+            label="Save Settings", command=self.save_settings
+        )
+        self.file_menu.add_separator()
+        self.file_menu.add_command(
+            label="Exit", command=self.exit_app
+        )
+
+        # Settings Menu
+        self.settings_menu = tk.Menu(self.menubar, tearoff=0)
+        self.menubar.add_cascade(label="Settings", menu=self.settings_menu)
+        self.settings_menu.add_checkbutton(
+            label="Dark Mode",
+            variable=self.dark_mode_var,
+            command=self._apply_theme
+        )
+        self.settings_menu.add_separator()
+        self.settings_menu.add_command(
+            label="Reset to Defaults", command=self.reset_to_defaults
+        )
+
+        # Tools Menu
+        self.tools_menu = tk.Menu(self.menubar, tearoff=0)
+        self.menubar.add_cascade(label="Tools", menu=self.tools_menu)
+        self.tools_menu.add_command(
+            label="Fusion Export to Sidecar",
+            command=self._on_fusion_export_menu
+        )
+        self.tools_menu.add_command(
+            label="Custom Fusion Sidecar Export",
+            command=self._on_custom_fusion_export_menu
+        )
+
+        # Help Menu
+        self.help_menu = tk.Menu(self.menubar, tearoff=0)
+        self.menubar.add_cascade(label="Help", menu=self.help_menu)
+        self.help_menu.add_command(
+            label="About", command=self._show_about
+        )
+
+    def _create_folder_selection_frame(self):
+        """Create the Input/Output folder selection section."""
+        folder_frame = ttk.LabelFrame(
+            self.main_container, text="Input / Output Folders", padding=5
+        )
+        folder_frame.pack(fill=tk.X, pady=(0, 5))
+
+        # Source clips folder
+        ttk.Label(folder_frame, text="Source Clips:").grid(row=0, column=0, sticky=tk.W)
+        ttk.Entry(folder_frame, textvariable=self.input_source_clips_var).grid(
+            row=0, column=1, sticky=tk.EW, padx=5
+        )
+        ttk.Button(
+            folder_frame, text="Browse...",
+            command=lambda: self._browse_folder(self.input_source_clips_var)
+        ).grid(row=0, column=2)
+
+        # Depth maps folder
+        ttk.Label(folder_frame, text="Depth Maps:").grid(row=1, column=0, sticky=tk.W, pady=2)
+        ttk.Entry(folder_frame, textvariable=self.input_depth_maps_var).grid(
+            row=1, column=1, sticky=tk.EW, padx=5
+        )
+        ttk.Button(
+            folder_frame, text="Browse...",
+            command=lambda: self._browse_folder(self.input_depth_maps_var)
+        ).grid(row=1, column=2)
+
+        # Output folder
+        ttk.Label(folder_frame, text="Output:").grid(row=2, column=0, sticky=tk.W)
+        ttk.Entry(folder_frame, textvariable=self.output_splatted_var).grid(
+            row=2, column=1, sticky=tk.EW, padx=5
+        )
+        ttk.Button(
+            folder_frame, text="Browse...",
+            command=lambda: self._browse_folder(self.output_splatted_var)
+        ).grid(row=2, column=2)
+
+        # Multi-map checkbox
+        ttk.Checkbutton(
+            folder_frame, text="Multi-Map Mode",
+            variable=self.multi_map_var,
+            command=self._on_multi_map_toggle
+        ).grid(row=3, column=0, columnspan=3, sticky=tk.W, pady=(5, 0))
+
+        folder_frame.columnconfigure(1, weight=1)
+
+    def _create_processing_settings_frame(self):
+        """Create the processing settings section."""
+        process_frame = ttk.LabelFrame(
+            self.main_container, text="Processing Settings", padding=5
+        )
+        process_frame.pack(fill=tk.X, pady=(0, 5))
+
+        # Resolution checkboxes
+        res_frame = ttk.Frame(process_frame)
+        res_frame.grid(row=0, column=0, columnspan=2, sticky=tk.W)
+        
+        ttk.Checkbutton(
+            res_frame, text="Full Resolution",
+            variable=self.enable_full_res_var,
+            command=self.toggle_processing_settings_fields
+        ).pack(side=tk.LEFT, padx=(0, 10))
+        
+        ttk.Checkbutton(
+            res_frame, text="Low Resolution",
+            variable=self.enable_low_res_var,
+            command=self.toggle_processing_settings_fields
+        ).pack(side=tk.LEFT)
+
+        # Max Disparity
+        ttk.Label(process_frame, text="Max Disparity:").grid(
+            row=1, column=0, sticky=tk.W, pady=2
+        )
+        ttk.Entry(process_frame, textvariable=self.max_disp_var, width=10).grid(
+            row=1, column=1, sticky=tk.W, padx=5
+        )
+
+        # Process Length
+        ttk.Label(process_frame, text="Process Length:").grid(
+            row=2, column=0, sticky=tk.W, pady=2
+        )
+        ttk.Entry(process_frame, textvariable=self.process_length_var, width=10).grid(
+            row=2, column=1, sticky=tk.W, padx=5
+        )
+
+        # Batch Size
+        ttk.Label(process_frame, text="Batch Size:").grid(
+            row=3, column=0, sticky=tk.W, pady=2
+        )
+        ttk.Entry(process_frame, textvariable=self.batch_size_var, width=10).grid(
+            row=3, column=1, sticky=tk.W, padx=5
+        )
+
+        process_frame.columnconfigure(1, weight=1)
+
+    def _create_depth_adjustment_frame(self):
+        """Create the depth adjustment controls section."""
+        depth_frame = ttk.LabelFrame(
+            self.main_container, text="Depth Adjustment", padding=5
+        )
+        depth_frame.pack(fill=tk.X, pady=(0, 5))
+
+        # Convergence (Zero Disparity Anchor)
+        ttk.Label(depth_frame, text="Convergence:").grid(row=0, column=0, sticky=tk.W)
+        conv_scale = ttk.Scale(
+            depth_frame, from_=0.0, to=1.0,
+            variable=self.zero_disparity_anchor_var,
+            orient=tk.HORIZONTAL
+        )
+        conv_scale.grid(row=0, column=1, sticky=tk.EW, padx=5)
+        ttk.Entry(depth_frame, textvariable=self.zero_disparity_anchor_var, width=8).grid(
+            row=0, column=2
+        )
+
+        # Gamma
+        ttk.Label(depth_frame, text="Gamma:").grid(row=1, column=0, sticky=tk.W, pady=2)
+        gamma_scale = ttk.Scale(
+            depth_frame, from_=0.1, to=3.0,
+            variable=self.depth_gamma_var,
+            orient=tk.HORIZONTAL
+        )
+        gamma_scale.grid(row=1, column=1, sticky=tk.EW, padx=5)
+        ttk.Entry(depth_frame, textvariable=self.depth_gamma_var, width=8).grid(
+            row=1, column=2
+        )
+
+        depth_frame.columnconfigure(1, weight=1)
+
+    def _create_preview_frame(self):
+        """Create the video preview section."""
+        preview_frame = ttk.LabelFrame(
+            self.main_container, text="Preview", padding=5
+        )
+        preview_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 5))
+
+        # Preview source dropdown
+        ttk.Label(preview_frame, text="Source:").grid(row=0, column=0, sticky=tk.W)
+        preview_combo = ttk.Combobox(
+            preview_frame,
+            textvariable=self.preview_source_var,
+            values=["Splat Result", "Depth Map", "Anaglyph 3D"],
+            state="readonly"
+        )
+        preview_combo.grid(row=0, column=1, sticky=tk.W, padx=5)
+        preview_combo.bind("<<ComboboxSelected>>", lambda e: self._on_preview_source_changed())
+
+        # Create previewer
+        self.previewer = VideoPreviewer(
+            preview_frame,
+            processing_callback=self._preview_processing_callback,
+            find_sources_callback=self._find_preview_sources,
+            get_params_callback=self._get_current_settings_dict,
+            preview_size_var=self.preview_size_var,
+        )
+        self.previewer.grid(row=1, column=0, columnspan=2, sticky=tk.NSEW, pady=5)
+
+        preview_frame.rowconfigure(1, weight=1)
+        preview_frame.columnconfigure(1, weight=1)
+
+    def _create_control_buttons_frame(self):
+        """Create the control buttons section."""
+        control_frame = ttk.Frame(self.main_container)
+        control_frame.pack(fill=tk.X, pady=(0, 5))
+
+        ttk.Button(
+            control_frame, text="Start Processing",
+            command=self.start_processing
+        ).pack(side=tk.LEFT, padx=(0, 5))
+
+        ttk.Button(
+            control_frame, text="Stop",
+            command=self.stop_processing
+        ).pack(side=tk.LEFT, padx=5)
+
+        ttk.Button(
+            control_frame, text="Preview",
+            command=self._update_preview
+        ).pack(side=tk.RIGHT)
+
+    def _create_processing_info_frame(self):
+        """Create the current processing information display section."""
+        self.info_frame = ttk.LabelFrame(
+            self.main_container, text="Current Processing Information", padding=5
+        )
+        self.info_frame.pack(fill=tk.X, pady=(0, 5))
+
+        # Create info labels
+        info_items = [
+            ("Filename:", self.processing_filename_var),
+            ("Resolution:", self.processing_resolution_var),
+            ("Frames:", self.processing_frames_var),
+            ("Disparity:", self.processing_disparity_var),
+            ("Convergence:", self.processing_convergence_var),
+            ("Task:", self.processing_task_name_var),
+        ]
+
+        self.info_labels = []
+        for i, (label_text, var) in enumerate(info_items):
+            ttk.Label(self.info_frame, text=label_text).grid(
+                row=i, column=0, sticky=tk.W
+            )
+            label = ttk.Label(self.info_frame, textvariable=var)
+            label.grid(row=i, column=1, sticky=tk.W, padx=5)
+            self.info_labels.append(label)
+
+        self.info_frame.columnconfigure(1, weight=1)
+
+    def _create_status_bar(self):
+        """Create the status bar at the bottom."""
+        self.status_label = ttk.Label(
+            self, text="Ready", relief=tk.SUNKEN, anchor=tk.W
+        )
+        self.status_label.pack(side=tk.BOTTOM, fill=tk.X)
+
+    def _setup_keyboard_shortcuts(self):
+        """Set up keyboard shortcuts for the application."""
+        self.bind("<Control-o>", lambda e: self.load_settings())
+        self.bind("<Control-s>", lambda e: self.save_settings())
+        self.bind("<F5>", lambda e: self.start_processing())
+        self.bind("<Escape>", lambda e: self.stop_processing())
+
+    def _apply_theme(self, is_startup: bool = False):
+        """Apply the selected theme (dark or light) to the GUI.
+        
+        Stage 9: Delegates to ThemeManager for theme application.
+        
+        Args:
+            is_startup: Whether this is being called during initial startup
+        """
+        # Use ThemeManager to apply theme
+        self.theme_manager.apply_theme_to_style(self.style, self)
+        self.theme_manager.apply_theme_to_menus(
+            [self.file_menu, self.settings_menu, self.tools_menu, self.help_menu],
+            self.menubar
+        )
+        self.theme_manager.apply_theme_to_labels(self.info_labels)
+        
+        # Apply to previewer canvas
+        if hasattr(self, "previewer") and hasattr(self.previewer, "preview_canvas"):
+            self.theme_manager.apply_theme_to_canvas(self.previewer.preview_canvas)
+
+        # Update idletasks for accurate sizing
+        self.update_idletasks()
+
+    def _on_fusion_export_menu(self):
+        """Handle Fusion Export to Sidecar menu command."""
+        generator = FusionSidecarGenerator(self, self.sidecar_manager)
+        generator.generate_sidecars(filedialog, messagebox)
+
+    def _on_custom_fusion_export_menu(self):
+        """Handle Custom Fusion Sidecar Export menu command."""
+        generator = FusionSidecarGenerator(self, self.sidecar_manager)
+        generator.generate_custom_sidecars(filedialog, messagebox)
+
+    def _load_config(self):
+        """Load configuration from file (backward compatibility)."""
+        # Stage 9: Uses ConfigManager
+        self.app_config = self.config_manager.load()
+
+    def _load_help_texts(self):
+        """Load help texts from JSON file."""
+        help_file = "help_texts.json"
+        if os.path.exists(help_file):
+            try:
+                with open(help_file, "r") as f:
+                    self.help_texts = json.load(f)
+            except Exception as e:
+                logger.warning(f"Failed to load help texts: {e}")
+
+    def _set_saved_geometry(self):
+        """Set the window geometry from saved configuration."""
+        if self.window_x is not None and self.window_y is not None:
+            self.geometry(f"{self.window_width}x{self.window_height}+{self.window_x}+{self.window_y}")
+        else:
+            self.geometry(f"{self.window_width}x{self.window_height}")
+
+    def _configure_logging(self):
+        """Configure logging based on debug mode."""
+        if self.debug_mode_var.get():
+            set_util_logger_level(logging.DEBUG)
+            self._debug_logging_enabled = True
+        else:
+            set_util_logger_level(logging.INFO)
+            self._debug_logging_enabled = False
+
+    def _browse_folder(self, var):
+        """Open a folder dialog and update a StringVar."""
+        current_path = var.get()
+        if os.path.isdir(current_path):
+            initial_dir = current_path
+        elif os.path.exists(current_path):
+            initial_dir = os.path.dirname(current_path)
+        else:
+            initial_dir = None
+
+        folder = filedialog.askdirectory(initialdir=initial_dir)
+        if folder:
+            var.set(folder)
+
+    def _browse_file(self, var, filetypes_list):
+        """Open a file dialog and update a StringVar."""
+        current_path = var.get()
+        if os.path.exists(current_path):
+            initial_dir = (
+                os.path.dirname(current_path)
+                if os.path.isfile(current_path)
+                else current_path
+            )
+        else:
+            initial_dir = None
+
+        file_path = filedialog.askopenfilename(
+            initialdir=initial_dir, filetypes=filetypes_list
+        )
+        if file_path:
+            var.set(file_path)
+
+    def _safe_float(self, var, default=0.0):
+        """Safely convert StringVar/BooleanVar to float."""
+        try:
+            val = var.get()
+            if isinstance(val, bool):
+                return float(val)
+            return float(val)
+        except (ValueError, TypeError, tk.TclError):
+            return default
+
+    # --- Placeholder methods for preview functionality ---
+    def _find_preview_sources(self):
+        """Find preview sources for the previewer."""
+        source_path = self.input_source_clips_var.get()
+        depth_path = self.input_depth_maps_var.get()
+        multi_map = self.multi_map_var.get()
+        
+        return self.preview_renderer.find_preview_sources(
+            source_path, depth_path, multi_map
+        )
+
+    def _preview_processing_callback(self, source_frame, depth_frame, settings, mode):
+        """Process frames for preview."""
+        return self.preview_renderer.render_preview_frame(
+            source_frame, depth_frame, settings, mode
+        )
+
+    def _on_preview_source_changed(self):
+        """Handle preview source change."""
+        self._update_preview()
+
+    def _update_preview(self):
+        """Update the preview display."""
+        if hasattr(self, "previewer"):
+            self.previewer.show_current_frame()
+
+    # --- Placeholder methods for border/convergence functionality ---
+    def _on_convergence_or_disparity_changed(self, *args):
+        """Handle changes to convergence or disparity values."""
+        if self.border_mode_var.get() == "Auto Basic":
+            self._calculate_auto_borders()
+
+    def _on_border_mode_change(self, *args):
+        """Handle border mode changes."""
+        mode = self.border_mode_var.get()
+        if mode == "Auto Basic":
+            self._calculate_auto_borders()
+        elif mode == "Auto Adv.":
+            self._scan_borders_for_current_clip()
+
+    def _calculate_auto_borders(self):
+        """Calculate automatic borders in Auto Basic mode."""
+        try:
+            conv = float(self.zero_disparity_anchor_var.get())
+            max_disp = float(self.max_disp_var.get())
+            gamma = float(self.depth_gamma_var.get())
+            
+            # Use default values for depth edges in basic mode
+            left_border, right_border = self.border_scanner.calculate_basic_border(
+                conv, max_disp, gamma
+            )
+            
+            self.auto_border_L_var.set(str(left_border))
+            self.auto_border_R_var.set(str(right_border))
+            self._sync_sliders_to_auto_borders()
+        except Exception as e:
+            logger.error(f"Auto border calculation failed: {e}")
+
+    def _scan_borders_for_current_clip(self):
+        """Scan borders for the currently loaded clip."""
+        # Get current depth path (would need to be determined from current source)
+        depth_path = self._get_current_depth_path()
+        if not depth_path:
+            return
+        
+        try:
+            conv = float(self.zero_disparity_anchor_var.get())
+            max_disp = float(self.max_disp_var.get())
+            gamma = float(self.depth_gamma_var.get())
+            
+            result = self.border_scanner.scan_current_clip(
+                depth_path=depth_path,
+                conv=conv,
+                max_disp=max_disp,
+                gamma=gamma,
+                stop_event=self.stop_event,
+                status_callback=lambda msg: self.status_label.config(text=msg)
+            )
+            
+            if result:
+                left_border, right_border = result
+                self.auto_border_L_var.set(str(left_border))
+                self.auto_border_R_var.set(str(right_border))
+                self._sync_sliders_to_auto_borders()
+        except Exception as e:
+            logger.error(f"Border scan failed: {e}")
+
+    def _get_current_depth_path(self):
+        """Get the depth path for the currently selected source."""
+        # This would need to be implemented based on how sources are tracked
+        # For now, return the depth folder path
+        return self.input_depth_maps_var.get()
+
+    def _sync_sliders_to_auto_borders(self):
+        """Synchronize border sliders to auto-calculated values."""
+        try:
+            left = float(self.auto_border_L_var.get())
+            right = float(self.auto_border_R_var.get())
+            
+            # Convert to width/bias representation
+            width = (left + right) / 2.0
+            bias = (right - left) / 2.0
+            
+            self.border_width_var.set(str(width))
+            self.border_bias_var.set(str(bias))
+        except Exception as e:
+            logger.error(f"Failed to sync border sliders: {e}")
+
+    # --- Placeholder methods for multi-map functionality ---
+    def _on_multi_map_toggle(self):
+        """Handle Multi-Map mode toggle."""
+        if self.multi_map_var.get():
+            self._scan_depth_map_folders()
+        else:
+            self._clear_depth_map_radio_buttons()
+
+    def _on_depth_map_folder_changed(self):
+        """Handle depth map folder path changes."""
+        if self.multi_map_var.get():
+            self._scan_depth_map_folders()
+
+    def _scan_depth_map_folders(self):
+        """Scan for depth map subfolders in Multi-Map mode."""
+        depth_path = self.input_depth_maps_var.get()
+        if not os.path.isdir(depth_path):
+            return
+        
+        # Find subfolders
+        self.depth_map_subfolders = []
+        for entry in os.listdir(depth_path):
+            full_path = os.path.join(depth_path, entry)
+            if os.path.isdir(full_path) and entry.lower() != "sidecars":
+                self.depth_map_subfolders.append(entry)
+        
+        self._update_depth_map_radio_buttons()
+
+    def _update_depth_map_radio_buttons(self):
+        """Update the depth map radio button UI."""
+        # This would create radio buttons for each subfolder
+        pass
+
+    def _clear_depth_map_radio_buttons(self):
+        """Clear the depth map radio buttons."""
+        self.depth_map_subfolders = []
+        self.depth_map_radio_buttons = []
+        self.depth_map_radio_dict = {}
+
+    # --- Placeholder methods for overlay functionality ---
+    def _apply_preview_overlay_toggles(self):
+        """Apply preview overlay settings (crosshair, depth pop)."""
+        # This would apply the overlay settings to the preview
+        pass
+
+    def toggle_processing_settings_fields(self):
+        """Toggle processing settings fields based on resolution selections."""
+        # This would enable/disable fields based on which resolutions are selected
+        pass
+
+    def _toggle_sidecar_update_button_state(self):
+        """Toggle the sidecar update button state."""
+        # This would update the UI state for sidecar controls
+        pass
+
+    def _get_current_settings_dict(self):
+        """Get current processing settings as a dictionary.
+        
+        Returns a dictionary of all current GUI settings for use by
+        the VideoPreviewer and other components.
+        
+        Returns:
+            Dictionary containing current processing parameters
+        """
+        return {
+            "max_disp": float(self.max_disp_var.get()),
+            "convergence_point": float(self.zero_disparity_anchor_var.get()),
+            "depth_gamma": float(self.depth_gamma_var.get()),
+            "depth_dilate_size_x": float(self.depth_dilate_size_x_var.get()),
+            "depth_dilate_size_y": float(self.depth_dilate_size_y_var.get()),
+            "depth_blur_size_x": float(self.depth_blur_size_x_var.get()),
+            "depth_blur_size_y": float(self.depth_blur_size_y_var.get()),
+            "depth_dilate_left": float(self.depth_dilate_left_var.get()),
+            "depth_blur_left": float(self.depth_blur_left_var.get()),
+            "border_mode": self.border_mode_var.get(),
+            "border_width": float(self.border_width_var.get()),
+            "border_bias": float(self.border_bias_var.get()),
+            "auto_border_L": float(self.auto_border_L_var.get()),
+            "auto_border_R": float(self.auto_border_R_var.get()),
+            "enable_global_norm": self.enable_global_norm_var.get(),
+            "dual_output": self.dual_output_var.get(),
+            "process_length": int(self.process_length_var.get()),
+        }
+
+    # --- Processing methods ---
+    def start_processing(self):
+        """Start the batch processing workflow.
+        
+        Stage 9: Uses BatchProcessor for the processing workflow.
+        """
+        if self.processing_thread and self.processing_thread.is_alive():
+            messagebox.showwarning("Processing", "Processing is already running.")
+            return
+
+        # Validate settings
+        validation_result = self._validate_processing_settings()
+        if not validation_result.is_valid:
+            messagebox.showerror("Validation Error", validation_result.error_message)
+            return
+
+        # Initialize batch processor
+        self.batch_processor = BatchProcessor(
+            progress_queue=self.progress_queue,
+            stop_event=self.stop_event
+        )
+
+        # Reset stop event
+        self.stop_event.clear()
+
+        # Start processing in a thread
+        self.processing_thread = threading.Thread(
+            target=self._processing_worker,
+            args=(validation_result.tasks, validation_result.settings)
+        )
+        self.processing_thread.start()
+
+        self.status_label.config(text="Processing started...")
+
+    def _validate_processing_settings(self):
+        """Validate processing settings and prepare tasks.
+        
+        Returns:
+            BatchSetupResult with validation status and prepared tasks
+        """
+        # This would validate all settings and prepare the task list
+        # For now, return a placeholder result
+        return BatchSetupResult(
+            is_valid=True,
+            tasks=[],
+            settings=ProcessingSettings(
+                max_disp=float(self.max_disp_var.get()),
+                convergence=float(self.zero_disparity_anchor_var.get()),
+                gamma=float(self.depth_gamma_var.get()),
+            ),
+            error_message=""
+        )
+
+    def _processing_worker(self, tasks, settings):
+        """Worker thread for batch processing.
+        
+        Args:
+            tasks: List of ProcessingTask objects
+            settings: ProcessingSettings object
+        """
+        try:
+            output_dir = self.output_splatted_var.get()
+            self.batch_processor.run_batch_process(
+                tasks=tasks,
+                settings=settings,
+                output_dir=output_dir,
+                progress_callback=self._on_progress_update,
+                completion_callback=self._on_processing_complete
+            )
+        except Exception as e:
+            logger.error(f"Processing failed: {e}", exc_info=True)
+            self.progress_queue.put(("error", str(e)))
+
+    def _on_progress_update(self, progress_info):
+        """Handle progress updates from the batch processor."""
+        self.progress_queue.put(("progress", progress_info))
+
+    def _on_processing_complete(self, success, message):
+        """Handle processing completion."""
+        self.progress_queue.put(("complete", success, message))
+
+    def stop_processing(self):
+        """Stop the current processing operation."""
+        if self.processing_thread and self.processing_thread.is_alive():
+            self.stop_event.set()
+            self.status_label.config(text="Stopping...")
+
+    def check_queue(self):
+        """Check the progress queue for updates."""
+        try:
+            while True:
+                msg = self.progress_queue.get_nowait()
+                msg_type = msg[0]
+                
+                if msg_type == "progress":
+                    # Update progress display
+                    progress_info = msg[1]
+                    self.status_label.config(text=f"Processing: {progress_info}")
+                
+                elif msg_type == "error":
+                    messagebox.showerror("Error", msg[1])
+                    self.status_label.config(text="Error occurred")
+                
+                elif msg_type == "complete":
+                    success = msg[1]
+                    message = msg[2]
+                    if success:
+                        self.status_label.config(text=f"Complete: {message}")
+                    else:
+                        self.status_label.config(text=f"Failed: {message}")
+                
+                self.progress_queue.task_done()
+        except queue.Empty:
+            pass
+        
+        # Schedule next check
+        self.after(100, self.check_queue)
+
+    # --- Settings management ---
+    def load_settings(self):
+        """Load settings from a user-selected file."""
+        filename = filedialog.askopenfilename(
+            defaultextension=".splatcfg",
+            filetypes=[("Splat Config", "*.splatcfg"), ("JSON Files", "*.json"), ("All Files", "*.*")]
+        )
+        if not filename:
+            return
+
+        loaded = self.config_manager.load_settings_from_file(filename)
+        if loaded:
+            # Sync to tkinter variables
+            for key, value in loaded.items():
+                var_name = f"{key}_var"
+                if hasattr(self, var_name):
+                    var = getattr(self, var_name)
+                    try:
+                        var.set(value)
+                    except Exception as e:
+                        logger.warning(f"Failed to set {key}: {e}")
+            
+            self.status_label.config(text=f"Settings loaded from {os.path.basename(filename)}")
+
+    def save_settings(self):
+        """Save settings to a user-selected file."""
+        filename = filedialog.asksaveasfilename(
+            defaultextension=".splatcfg",
+            filetypes=[("Splat Config", "*.splatcfg"), ("JSON Files", "*.json"), ("All Files", "*.*")]
+        )
+        if not filename:
+            return
+
+        # Get current config from tkinter variables
+        current_config = self._get_current_config_from_vars()
+        
+        if self.config_manager.save_settings_to_file(current_config, filename):
+            self.status_label.config(text=f"Settings saved to {os.path.basename(filename)}")
+
+    def _get_current_config_from_vars(self):
+        """Extract current configuration from tkinter variables."""
+        config = {}
+        
+        # Map of tkinter variable names to config keys
+        var_map = {
+            "input_source_clips_var": "input_source_clips",
+            "input_depth_maps_var": "input_depth_maps",
+            "output_splatted_var": "output_splatted",
+            "multi_map_var": "multi_map_enabled",
+            "max_disp_var": "max_disp",
+            "process_length_var": "process_length",
+            "batch_size_var": "batch_size",
+            "dual_output_var": "dual_output",
+            "enable_global_norm_var": "enable_global_norm",
+            "enable_full_res_var": "enable_full_resolution",
+            "enable_low_res_var": "enable_low_resolution",
+            "pre_res_width_var": "pre_res_width",
+            "pre_res_height_var": "pre_res_height",
+            "low_res_batch_size_var": "low_res_batch_size",
+            "zero_disparity_anchor_var": "convergence_point",
+            "output_crf_var": "output_crf",
+            "output_crf_full_var": "output_crf_full",
+            "output_crf_low_var": "output_crf_low",
+            "color_tags_mode_var": "color_tags_mode",
+            "dark_mode_var": "dark_mode_enabled",
+            "move_to_finished_var": "move_to_finished",
+            "crosshair_enabled_var": "crosshair_enabled",
+            "crosshair_white_var": "crosshair_white",
+            "crosshair_multi_var": "crosshair_multi",
+            "depth_pop_enabled_var": "depth_pop_enabled",
+            "auto_convergence_mode_var": "auto_convergence_mode",
+            "depth_gamma_var": "depth_gamma",
+            "depth_dilate_size_x_var": "depth_dilate_size_x",
+            "depth_dilate_size_y_var": "depth_dilate_size_y",
+            "depth_blur_size_x_var": "depth_blur_size_x",
+            "depth_blur_size_y_var": "depth_blur_size_y",
+            "depth_dilate_left_var": "depth_dilate_left",
+            "depth_blur_left_var": "depth_blur_left",
+            "depth_blur_left_mix_var": "depth_blur_left_mix",
+            "enable_sidecar_gamma_var": "enable_sidecar_gamma",
+            "enable_sidecar_blur_dilate_var": "enable_sidecar_blur_dilate",
+            "update_slider_from_sidecar_var": "update_slider_from_sidecar",
+            "auto_save_sidecar_var": "auto_save_sidecar",
+            "border_width_var": "border_width",
+            "border_bias_var": "border_bias",
+            "border_mode_var": "border_mode",
+            "auto_border_L_var": "auto_border_L",
+            "auto_border_R_var": "auto_border_R",
+            "preview_source_var": "preview_source",
+            "preview_size_var": "preview_size",
+            "debug_mode_var": "debug_mode_enabled",
+        }
+        
+        for var_name, config_key in var_map.items():
+            if hasattr(self, var_name):
+                var = getattr(self, var_name)
+                try:
+                    config[config_key] = var.get()
+                except Exception:
+                    pass
+        
+        # Add window geometry
+        config["window_width"] = self.winfo_width()
+        config["window_height"] = self.winfo_height()
+        config["window_x"] = self.winfo_x()
+        config["window_y"] = self.winfo_y()
+        
+        return config
+
+    def reset_to_defaults(self):
+        """Reset all settings to their default values."""
+        defaults = self.APP_CONFIG_DEFAULTS
+        
+        # Reset variables to defaults
+        self.max_disp_var.set(defaults["MAX_DISP"])
+        self.zero_disparity_anchor_var.set(defaults["CONV_POINT"])
+        self.process_length_var.set(defaults["PROC_LENGTH"])
+        self.batch_size_var.set(defaults["BATCH_SIZE_FULL"])
+        self.depth_gamma_var.set(defaults["DEPTH_GAMMA"])
+        self.border_mode_var.set(defaults["BORDER_MODE"])
+        self.border_width_var.set(defaults["BORDER_WIDTH"])
+        self.border_bias_var.set(defaults["BORDER_BIAS"])
+        
+        self.status_label.config(text="Settings reset to defaults")
+
+    # --- Utility methods ---
+    def _show_about(self):
+        """Show the About dialog."""
+        messagebox.showinfo(
+            "About",
+            f"StereoCrafter Splatting GUI\nVersion: {GUI_VERSION}\n\n"
+            "A diffusion-based 2D-to-3D video conversion tool."
+        )
+
+    def exit_app(self):
+        """Exit the application, saving configuration."""
+        # Save configuration
+        current_config = self._get_current_config_from_vars()
+        self.config_manager.config = current_config
+        self.config_manager.save()
+        
+        # Stop any running processing
+        if self.processing_thread and self.processing_thread.is_alive():
+            self.stop_event.set()
+            self.processing_thread.join(timeout=2.0)
+        
+        # Destroy the window
+        self.destroy()
+
+
+# Make the class available at module level
+__all__ = ['SplatterGUI']
diff --git a/core/splatting/preview_rendering.py b/core/splatting/preview_rendering.py
new file mode 100644
index 0000000..7924a0b
--- /dev/null
+++ b/core/splatting/preview_rendering.py
@@ -0,0 +1,628 @@
+"""Preview rendering module for various display modes.
+
+Handles preview frame generation for various display modes including:
+- Splat Result (Full/Low resolution)
+- Occlusion Mask
+- Depth Map (raw and colorized)
+- Anaglyph 3D (Dubois and Optimized)
+- Wigglegram
+
+This module provides the rendering logic extracted from the main GUI
+to enable preview generation for the splatting workflow.
+"""
+
+import logging
+import os
+from typing import Dict, List, Optional, Tuple
+
+import cv2
+import numpy as np
+import torch
+import torch.nn.functional as F
+from PIL import Image
+
+from dependency.stereocrafter_util import (
+    apply_dubois_anaglyph,
+    apply_optimized_anaglyph,
+    release_cuda_memory,
+)
+
+from .forward_warp import ForwardWarpStereo
+
+logger = logging.getLogger(__name__)
+
+# Constants
+DEPTH_VIS_APPLY_TV_RANGE_EXPANSION_10BIT = True
+
+
+class PreviewRenderer:
+    """Handles preview frame generation for various display modes.
+
+    Supports multiple preview modes including:
+    - Splat Result (Full and Low resolution variants)
+    - Occlusion Mask
+    - Depth Map (raw grayscale and colorized)
+    - Anaglyph 3D (standard, Dubois, Optimized)
+    - Wigglegram
+    - Original (Left Eye)
+    """
+
+    # Supported preview modes
+    MODES = {
+        "splat": ["Splat Result", "Splat Result(Low)"],
+        "occlusion": ["Occlusion Mask", "Occlusion Mask(Low)"],
+        "depth_raw": ["Depth Map"],
+        "depth_color": ["Depth Map (Color)"],
+        "original": ["Original (Left Eye)"],
+        "anaglyph": ["Anaglyph 3D"],
+        "anaglyph_dubois": ["Dubois Anaglyph"],
+        "anaglyph_optimized": ["Optimized Anaglyph"],
+        "wigglegram": ["Wigglegram"],
+    }
+
+    def __init__(self, cuda_available: bool = True):
+        """Initialize preview renderer.
+
+        Args:
+            cuda_available: Whether CUDA is available for GPU processing
+        """
+        self.logger = logging.getLogger(__name__)
+        self.cuda_available = cuda_available
+
+    def find_preview_sources(
+        self,
+        source_path: str,
+        depth_path: str,
+        multi_map: bool = False,
+    ) -> List[Dict[str, str]]:
+        """Find matching source video and depth map pairs for preview.
+
+        Scans for matching source video and depth map pairs, handling both
+        single-file mode and folder/batch modes. Supports Multi-Map mode
+        where depth maps are organized in subfolders.
+
+        Args:
+            source_path: Path to source video file or folder
+            depth_path: Path to depth map file or folder
+            multi_map: Whether Multi-Map mode is enabled
+
+        Returns:
+            List of dictionaries with 'source_video' and 'depth_map' keys
+        """
+        import glob
+
+        if not source_path or not depth_path:
+            self.logger.warning("Preview Scan Failed: Source or depth path is empty.")
+            return []
+
+        # Single-file mode
+        is_source_file = os.path.isfile(source_path)
+        is_depth_file = os.path.isfile(depth_path)
+
+        if is_source_file and is_depth_file:
+            self.logger.debug(
+                f"Preview Scan: Single file mode. Source: {source_path}, Depth: {depth_path}"
+            )
+            return [{"source_video": source_path, "depth_map": depth_path}]
+
+        # Folder/batch mode
+        if not os.path.isdir(source_path) or not os.path.isdir(depth_path):
+            self.logger.error(
+                "Preview Scan Failed: Inputs must be two files or two valid directories."
+            )
+            return []
+
+        video_extensions = ("*.mp4", "*.avi", "*.mov", "*.mkv")
+        source_videos = []
+        for ext in video_extensions:
+            source_videos.extend(glob.glob(os.path.join(source_path, ext)))
+
+        if not source_videos:
+            self.logger.warning(f"No source videos found in folder: {source_path}")
+            return []
+
+        video_source_list = []
+
+        if multi_map:
+            # Multi-Map mode: search all map subfolders
+            depth_candidate_folders = []
+            try:
+                for entry in os.listdir(depth_path):
+                    full_sub = os.path.join(depth_path, entry)
+                    if os.path.isdir(full_sub) and entry.lower() != "sidecars":
+                        depth_candidate_folders.append(full_sub)
+            except FileNotFoundError:
+                self.logger.error(
+                    f"Preview Scan Failed: Depth folder not found: {depth_path}"
+                )
+                return []
+
+            for video_path in sorted(source_videos):
+                base_name = os.path.splitext(os.path.basename(video_path))[0]
+                matched = False
+
+                for dpath in depth_candidate_folders:
+                    mp4 = os.path.join(dpath, f"{base_name}_depth.mp4")
+                    npz = os.path.join(dpath, f"{base_name}_depth.npz")
+
+                    if os.path.exists(mp4):
+                        video_source_list.append(
+                            {"source_video": video_path, "depth_map": mp4}
+                        )
+                        matched = True
+                        break
+                    elif os.path.exists(npz):
+                        video_source_list.append(
+                            {"source_video": video_path, "depth_map": npz}
+                        )
+                        matched = True
+                        break
+
+                if not matched:
+                    self.logger.debug(
+                        f"Preview Scan: No depth map found for '{base_name}'."
+                    )
+        else:
+            # Normal mode: single depth folder
+            for video_path in sorted(source_videos):
+                base_name = os.path.splitext(os.path.basename(video_path))[0]
+
+                candidates = [
+                    os.path.join(depth_path, f"{base_name}_depth.mp4"),
+                    os.path.join(depth_path, f"{base_name}_depth.npz"),
+                    os.path.join(depth_path, f"{base_name}.mp4"),
+                    os.path.join(depth_path, f"{base_name}.npz"),
+                ]
+
+                matching_depth_path = None
+                for dp in candidates:
+                    if os.path.exists(dp):
+                        matching_depth_path = dp
+                        break
+
+                if matching_depth_path:
+                    video_source_list.append(
+                        {
+                            "source_video": video_path,
+                            "depth_map": matching_depth_path,
+                        }
+                    )
+
+        if not video_source_list:
+            self.logger.warning("Preview Scan: No matching source/depth pairs found.")
+        else:
+            self.logger.info(
+                f"Preview Scan: Found {len(video_source_list)} matching pairs."
+            )
+
+        return video_source_list
+
+    def render_preview_frame(
+        self,
+        source_frame: torch.Tensor,
+        depth_frame: torch.Tensor,
+        settings: Dict,
+        preview_mode: str,
+        wigglegram_callback=None,
+    ) -> Optional[Image.Image]:
+        """Render a preview frame based on the selected mode.
+
+        Performs splatting and renders the result in the requested preview mode.
+        Supports various output modes including anaglyph 3D, depth visualization,
+        and wigglegram animation.
+
+        Args:
+            source_frame: Source video frame tensor [1, 3, H, W]
+            depth_frame: Depth map frame tensor [1, 1, H, W] or [1, C, H, W]
+            settings: Dictionary of processing parameters
+            preview_mode: Preview mode string (e.g., 'Splat Result', 'Anaglyph 3D')
+            wigglegram_callback: Optional callback for wigglegram animation
+
+        Returns:
+            Rendered PIL Image, or None for Wigglegram mode (handled by callback)
+        """
+        if not self.cuda_available:
+            self.logger.error("Preview processing requires a CUDA-enabled GPU.")
+            return None
+
+        self.logger.debug("--- Starting Preview Render ---")
+
+        # Check inputs
+        if source_frame is None or depth_frame is None:
+            self.logger.error("Preview failed: Missing source or depth frame.")
+            return None
+
+        # Get settings
+        if not settings:
+            self.logger.error("Preview failed: No settings provided.")
+            return None
+
+        # Determine if low-res preview
+        is_low_res = preview_mode in ["Splat Result(Low)", "Occlusion Mask(Low)"]
+
+        # Get original dimensions
+        W_orig = source_frame.shape[3]
+        H_orig = source_frame.shape[2]
+
+        # Setup target resolution
+        W_target, H_target = W_orig, H_orig
+
+        # Handle low-res preview sizing
+        if is_low_res:
+            W_target, H_target = self._calculate_low_res_dimensions(
+                W_orig, H_orig, settings.get("target_width", 0)
+            )
+
+            try:
+                source_resized = F.interpolate(
+                    source_frame.cuda(),
+                    size=(H_target, W_target),
+                    mode="bilinear",
+                    align_corners=False,
+                )
+            except Exception as e:
+                self.logger.error(
+                    f"Low-Res preview resize failed: {e}. Falling back to original."
+                )
+                W_target, H_target = W_orig, H_orig
+                source_resized = source_frame.cuda()
+        else:
+            source_resized = source_frame.cuda()
+
+        # Process depth frame
+        depth_processed = self._process_depth_for_preview(
+            depth_frame, W_orig, H_orig, settings, is_low_res, W_target, H_target
+        )
+
+        if depth_processed is None:
+            return None
+
+        # Perform splatting
+        stereo_projector = ForwardWarpStereo(occlu_map=True).cuda()
+
+        # Resize depth to target resolution
+        disp_map = (
+            torch.from_numpy(depth_processed)
+            .unsqueeze(0)
+            .unsqueeze(0)
+            .float()
+            .cuda()
+        )
+
+        if H_target != disp_map.shape[2] or W_target != disp_map.shape[3]:
+            disp_map = F.interpolate(
+                disp_map,
+                size=(H_target, W_target),
+                mode="bilinear",
+                align_corners=False,
+            )
+
+        # Calculate disparity
+        convergence = float(settings.get("convergence_point", 0.5))
+        max_disp = float(settings.get("max_disp", 20.0))
+        tv_comp = float(settings.get("tv_disp_compensation", 1.0))
+
+        disp_map = (disp_map - convergence) * 2.0
+        actual_max_disp_pixels = (max_disp / 20.0 / 100.0) * W_target * tv_comp
+        disp_map = disp_map * actual_max_disp_pixels
+
+        # Perform forward warp
+        with torch.no_grad():
+            right_eye_raw, occlusion_mask = stereo_projector(source_resized, disp_map)
+            right_eye = right_eye_raw
+
+        # Apply borders for anaglyph and wigglegram
+        left_pct = settings.get("left_border_pct", 0.0)
+        right_pct = settings.get("right_border_pct", 0.0)
+
+        if preview_mode in [
+            "Anaglyph 3D",
+            "Dubois Anaglyph",
+            "Optimized Anaglyph",
+            "Wigglegram",
+        ]:
+            l_px = int(round(left_pct * W_target / 100.0))
+            r_px = int(round(right_pct * W_target / 100.0))
+
+            if l_px > 0:
+                source_resized[:, :, :, :l_px] = 0.0
+            if r_px > 0:
+                right_eye[:, :, :, -r_px:] = 0.0
+
+        # Render based on mode
+        final_tensor = self._render_by_mode(
+            source_resized,
+            right_eye,
+            depth_processed,
+            occlusion_mask,
+            preview_mode,
+        )
+
+        # Handle wigglegram special case
+        if preview_mode == "Wigglegram" and wigglegram_callback:
+            wigglegram_callback(source_resized.cpu(), right_eye.cpu())
+            del stereo_projector, disp_map, right_eye_raw, occlusion_mask
+            release_cuda_memory()
+            return None
+
+        # Convert to PIL Image
+        if final_tensor is not None:
+            pil_img = Image.fromarray(
+                (final_tensor.squeeze(0).permute(1, 2, 0).numpy() * 255).astype(np.uint8)
+            )
+        else:
+            pil_img = None
+
+        # Cleanup
+        del stereo_projector, disp_map, right_eye_raw, occlusion_mask
+        release_cuda_memory()
+
+        self.logger.debug("--- Finished Preview Render ---")
+        return pil_img
+
+    def _calculate_low_res_dimensions(
+        self, W_orig: int, H_orig: int, target_width: int
+    ) -> Tuple[int, int]:
+        """Calculate aspect-ratio-correct low-res dimensions.
+
+        Args:
+            W_orig: Original width
+            H_orig: Original height
+            target_width: Requested target width
+
+        Returns:
+            Tuple of (W_target, H_target) divisible by 2
+        """
+        if target_width <= 0:
+            return W_orig, H_orig
+
+        aspect_ratio = W_orig / H_orig
+        H_calculated = int(round(target_width / aspect_ratio))
+
+        # Ensure divisible by 2
+        W_target = target_width if target_width % 2 == 0 else target_width + 1
+        H_target = H_calculated if H_calculated % 2 == 0 else H_calculated + 1
+
+        if W_target <= 0 or H_target <= 0:
+            return W_orig, H_orig
+
+        return W_target, H_target
+
+    def _process_depth_for_preview(
+        self,
+        depth_frame: torch.Tensor,
+        W_orig: int,
+        H_orig: int,
+        settings: Dict,
+        is_low_res: bool,
+        W_target: int,
+        H_target: int,
+    ) -> Optional[np.ndarray]:
+        """Process depth frame for preview rendering.
+
+        Handles depth preprocessing including normalization, gamma correction,
+        and optional resizing for low-res previews.
+
+        Args:
+            depth_frame: Raw depth frame tensor
+            W_orig: Original width
+            H_orig: Original height
+            settings: Processing settings dictionary
+            is_low_res: Whether this is a low-res preview
+            W_target: Target width
+            H_target: Target height
+
+        Returns:
+            Processed depth as numpy array [H, W], or None on error
+        """
+        # Convert to numpy
+        depth_numpy = depth_frame.squeeze(0).permute(1, 2, 0).cpu().numpy()
+
+        # Ensure 3D
+        if depth_numpy.ndim == 2:
+            depth_numpy = depth_numpy[:, :, None]
+
+        # Resize to original resolution if needed
+        if depth_numpy.shape[0] != H_orig or depth_numpy.shape[1] != W_orig:
+            try:
+                interp = (
+                    cv2.INTER_LINEAR
+                    if (W_orig > depth_numpy.shape[1] or H_orig > depth_numpy.shape[0])
+                    else cv2.INTER_AREA
+                )
+                depth_numpy = cv2.resize(depth_numpy, (W_orig, H_orig), interpolation=interp)
+                if depth_numpy.ndim == 2:
+                    depth_numpy = depth_numpy[:, :, None]
+            except Exception as e:
+                self.logger.error(f"Depth resize failed: {e}")
+
+        # Get max raw value for scaling
+        max_raw = depth_numpy.max()
+        if max_raw < 1.0:
+            max_raw = 1.0
+
+        # Determine scaling factor
+        if max_raw <= 256.0 and max_raw > 1.0:
+            scale = 255.0
+        elif max_raw > 256.0 and max_raw <= 1024.0:
+            scale = max_raw
+        elif max_raw > 1024.0:
+            scale = 65535.0
+        else:
+            scale = 1.0
+
+        # Normalize to 0-1
+        depth_normalized = depth_numpy.squeeze() / scale
+        depth_normalized = np.clip(depth_normalized, 0, 1)
+
+        # Apply gamma
+        gamma = float(settings.get("depth_gamma", 1.0))
+        if round(gamma, 2) != 1.0:
+            depth_normalized = 1.0 - np.power(
+                1.0 - depth_normalized, gamma
+            )
+            depth_normalized = np.clip(depth_normalized, 0, 1)
+
+        # Low-res: resize processed depth
+        if is_low_res and (depth_normalized.shape[0] != H_target or depth_normalized.shape[1] != W_target):
+            try:
+                interp = (
+                    cv2.INTER_AREA
+                    if (W_target < depth_normalized.shape[1] and H_target < depth_normalized.shape[0])
+                    else cv2.INTER_LINEAR
+                )
+                depth_normalized = cv2.resize(depth_normalized, (W_target, H_target), interpolation=interp)
+            except Exception as e:
+                self.logger.error(f"Low-res depth resize failed: {e}")
+
+        return depth_normalized
+
+    def _render_by_mode(
+        self,
+        left_eye: torch.Tensor,
+        right_eye: torch.Tensor,
+        depth: np.ndarray,
+        occlusion: torch.Tensor,
+        mode: str,
+    ) -> Optional[torch.Tensor]:
+        """Render output based on preview mode.
+
+        Args:
+            left_eye: Left eye tensor [1, 3, H, W]
+            right_eye: Right eye tensor [1, 3, H, W]
+            depth: Normalized depth array [H, W]
+            occlusion: Occlusion mask tensor [1, 1, H, W]
+            mode: Preview mode string
+
+        Returns:
+            Final output tensor [1, 3, H, W], or None
+        """
+        if mode in ["Splat Result", "Splat Result(Low)"]:
+            return right_eye.cpu()
+
+        elif mode in ["Occlusion Mask", "Occlusion Mask(Low)"]:
+            return occlusion.repeat(1, 3, 1, 1).cpu()
+
+        elif mode == "Depth Map":
+            return self._render_depth_raw(depth)
+
+        elif mode == "Depth Map (Color)":
+            return self._render_depth_color(depth)
+
+        elif mode == "Original (Left Eye)":
+            return left_eye.cpu()
+
+        elif mode == "Anaglyph 3D":
+            return self._render_anaglyph_simple(left_eye, right_eye)
+
+        elif mode == "Dubois Anaglyph":
+            return self._render_anaglyph_dubois(left_eye, right_eye)
+
+        elif mode == "Optimized Anaglyph":
+            return self._render_anaglyph_optimized(left_eye, right_eye)
+
+        else:
+            # Default to splat result
+            return right_eye.cpu()
+
+    def _render_depth_raw(self, depth: np.ndarray) -> torch.Tensor:
+        """Render raw grayscale depth map.
+
+        Args:
+            depth: Normalized depth array [H, W]
+
+        Returns:
+            RGB tensor [1, 3, H, W]
+        """
+        depth_uint8 = (np.clip(depth, 0, 1) * 255).astype(np.uint8)
+        depth_3ch = np.stack([depth_uint8] * 3, axis=-1)
+        return (
+            torch.from_numpy(depth_3ch).permute(2, 0, 1).unsqueeze(0).float() / 255.0
+        )
+
+    def _render_depth_color(self, depth: np.ndarray) -> torch.Tensor:
+        """Render colorized depth map.
+
+        Args:
+            depth: Normalized depth array [H, W]
+
+        Returns:
+            RGB tensor [1, 3, H, W]
+        """
+        depth_uint8 = (np.clip(depth, 0, 1) * 255).astype(np.uint8)
+        vis_color = cv2.applyColorMap(depth_uint8, cv2.COLORMAP_VIRIDIS)
+        vis_rgb = cv2.cvtColor(vis_color, cv2.COLOR_BGR2RGB)
+        return (
+            torch.from_numpy(vis_rgb).permute(2, 0, 1).unsqueeze(0).float() / 255.0
+        )
+
+    def _render_anaglyph_simple(
+        self, left: torch.Tensor, right: torch.Tensor
+    ) -> torch.Tensor:
+        """Render simple red-cyan anaglyph.
+
+        Args:
+            left: Left eye tensor [1, 3, H, W]
+            right: Right eye tensor [1, 3, H, W]
+
+        Returns:
+            Anaglyph tensor [1, 3, H, W]
+        """
+        left_np = (left.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(
+            np.uint8
+        )
+        right_np = (right.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(
+            np.uint8
+        )
+
+        left_gray = cv2.cvtColor(left_np, cv2.COLOR_RGB2GRAY)
+        anaglyph = right_np.copy()
+        anaglyph[:, :, 0] = left_gray
+
+        return torch.from_numpy(anaglyph).permute(2, 0, 1).float().unsqueeze(0) / 255.0
+
+    def _render_anaglyph_dubois(
+        self, left: torch.Tensor, right: torch.Tensor
+    ) -> torch.Tensor:
+        """Render Dubois anaglyph.
+
+        Args:
+            left: Left eye tensor [1, 3, H, W]
+            right: Right eye tensor [1, 3, H, W]
+
+        Returns:
+            Anaglyph tensor [1, 3, H, W]
+        """
+        left_np = (left.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(
+            np.uint8
+        )
+        right_np = (right.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(
+            np.uint8
+        )
+
+        anaglyph = apply_dubois_anaglyph(left_np, right_np)
+
+        return torch.from_numpy(anaglyph).permute(2, 0, 1).float().unsqueeze(0) / 255.0
+
+    def _render_anaglyph_optimized(
+        self, left: torch.Tensor, right: torch.Tensor
+    ) -> torch.Tensor:
+        """Render Optimized anaglyph.
+
+        Args:
+            left: Left eye tensor [1, 3, H, W]
+            right: Right eye tensor [1, 3, H, W]
+
+        Returns:
+            Anaglyph tensor [1, 3, H, W]
+        """
+        left_np = (left.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(
+            np.uint8
+        )
+        right_np = (right.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(
+            np.uint8
+        )
+
+        anaglyph = apply_optimized_anaglyph(left_np, right_np)
+
+        return torch.from_numpy(anaglyph).permute(2, 0, 1).float().unsqueeze(0) / 255.0
diff --git a/core/splatting/render_processor.py b/core/splatting/render_processor.py
new file mode 100644
index 0000000..78dac20
--- /dev/null
+++ b/core/splatting/render_processor.py
@@ -0,0 +1,497 @@
+"""Main splatting render processor.
+
+Implements the core video splatting algorithm, handling the processing loop,
+GPU computation (forward warping), and FFmpeg encoding.
+"""
+
+import math
+import os
+import time
+import logging
+import queue
+import threading
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+import cv2
+import numpy as np
+import torch
+import torch.nn.functional as F
+from decord import VideoReader
+
+from dependency.stereocrafter_util import (
+    start_ffmpeg_pipe_process,
+    release_cuda_memory,
+    draw_progress_bar,
+)
+from .forward_warp import ForwardWarpStereo
+from .depth_processing import process_depth_batch
+
+logger = logging.getLogger(__name__)
+
+
+class RenderProcessor:
+    """Handles the core splatting render loop for a single video task."""
+
+    def __init__(
+        self,
+        stop_event: threading.Event,
+        progress_queue: queue.Queue,
+    ):
+        """Initialize render processor.
+
+        Args:
+            stop_event: Event to signal stop/cancellation
+            progress_queue: Queue for sending progress updates to GUI
+        """
+        self.stop_event = stop_event
+        self.progress_queue = progress_queue
+        self._color_encode_flags = {}
+
+    def render_video(
+        self,
+        input_video_reader: VideoReader,
+        depth_map_reader: VideoReader,
+        total_frames_to_process: int,
+        processed_fps: float,
+        output_video_path_base: str,
+        target_output_height: int,
+        target_output_width: int,
+        max_disp: float,
+        batch_size: int,
+        dual_output: bool,
+        zero_disparity_anchor_val: float,
+        video_stream_info: Optional[dict],
+        input_bias: float,
+        assume_raw_input: bool,
+        global_depth_min: float,
+        global_depth_max: float,
+        depth_stream_info: Optional[dict],
+        user_output_crf: Optional[int] = None,
+        is_low_res_task: bool = False,
+        depth_gamma: float = 1.0,
+        depth_dilate_size_x: float = 0.0,
+        depth_dilate_size_y: float = 0.0,
+        depth_blur_size_x: float = 0.0,
+        depth_blur_size_y: float = 0.0,
+        depth_dilate_left: float = 0.0,
+        depth_blur_left: float = 0.0,
+        depth_blur_left_mix: float = 0.5,
+        skip_lowres_preproc: bool = False,
+        color_tags_mode: str = "Auto",
+        is_test_mode: bool = False,
+        test_target_frame_idx: Optional[int] = None,
+    ) -> bool:
+        """Core splatting render loop.
+
+        Args:
+            input_video_reader: Reader for source video
+            depth_map_reader: Reader for depth map
+            total_frames_to_process: Number of frames to process
+            processed_fps: Output video FPS
+            output_video_path_base: Base path for output video
+            target_output_height: Target height
+            target_output_width: Target width
+            max_disp: Max disparity percentage
+            batch_size: Frames per batch
+            dual_output: Whether to output left/right eyes
+            zero_disparity_anchor_val: Convergence anchor (0-1)
+            video_stream_info: Metadata for source video
+            input_bias: Depth input bias
+            assume_raw_input: Whether to skip normalization
+            global_depth_min: Global min depth used for normalization
+            global_max_depth: Global max depth used for normalization
+            depth_stream_info: Metadata for depth map
+            user_output_crf: FFmpeg CRF value
+            is_low_res_task: Whether this is a low-res pass
+            depth_gamma: Gamma correction for depth
+            depth_dilate_size_x: X dilation for depth
+            depth_dilate_size_y: Y dilation for depth
+            depth_blur_size_x: X blur for depth
+            depth_blur_size_y: Y blur for depth
+            depth_dilate_left: Left-eye dilation
+            depth_blur_left: Left-eye blur
+            depth_blur_left_mix: Mix factor for left-eye blur
+            skip_lowres_preproc: Whether to skip preprocessing for low-res
+            color_tags_mode: FFmpeg color tagging mode
+            is_test_mode: Whether in diagnostic test mode
+            test_target_frame_idx: Specific frame for diagnostic test
+
+        Returns:
+            True if completed successfully, False otherwise
+        """
+        logger.debug("==> Initializing ForwardWarpStereo module")
+        stereo_projector = ForwardWarpStereo(occlu_map=True).cuda()
+
+        height, width = target_output_height, target_output_width
+        os.makedirs(os.path.dirname(output_video_path_base), exist_ok=True)
+
+        # Determine output grid dimensions and final path
+        grid_height, grid_width = (height, width * 2) if dual_output else (height * 2, width * 2)
+        suffix = "_splatted2" if dual_output else "_splatted4"
+        res_suffix = f"_{width}"
+        final_output_video_path = f"{os.path.splitext(output_video_path_base)[0]}{res_suffix}{suffix}.mp4"
+
+        task_name = "LowRes" if is_low_res_task else "HiRes"
+        self._log_color_metadata(video_stream_info, task_name)
+
+        ffmpeg_process = None
+        if not is_test_mode:
+            encode_stream_info = self._get_encode_stream_info(video_stream_info, color_tags_mode)
+            ffmpeg_process = start_ffmpeg_pipe_process(
+                content_width=grid_width,
+                content_height=grid_height,
+                final_output_mp4_path=final_output_video_path,
+                fps=processed_fps,
+                video_stream_info=encode_stream_info,
+                user_output_crf=user_output_crf,
+                output_format_str="splatted_grid",
+                debug_label=task_name,
+            )
+            if ffmpeg_process is None:
+                logger.error("Failed to start FFmpeg pipe. Aborting splatting task.")
+                return False
+            
+            self._compare_encoding_flags(ffmpeg_process, task_name)
+
+        max_expected_raw_value = self._get_max_expected_raw_depth(depth_stream_info)
+        logger.debug(f"[DEPTH] Max expected raw value: {max_expected_raw_value}, assume_raw_input: {assume_raw_input}, global_depth_min: {global_depth_min:.2f}, global_depth_max: {global_depth_max:.2f}")
+        
+        frame_count = 0
+        encoding_successful = True
+
+        try:
+            frame_index_iter = (
+                [test_target_frame_idx]
+                if test_target_frame_idx is not None
+                else range(0, total_frames_to_process, batch_size)
+            )
+
+            for i in frame_index_iter:
+                if self.stop_event.is_set() or (ffmpeg_process is not None and ffmpeg_process.poll() is not None):
+                    break
+
+                batch_indices = list(range(i, min(i + batch_size, total_frames_to_process)))
+                if not batch_indices:
+                    break
+
+                # 1. Fetch frames
+                batch_video_numpy = input_video_reader.get_batch(batch_indices).asnumpy()
+                batch_depth_numpy_raw = depth_map_reader.get_batch(batch_indices).asnumpy()
+
+                # 2. Normalize and apply gamma (BEFORE dilation/blur)
+                # Convert to grayscale if needed
+                if batch_depth_numpy_raw.ndim == 4 and batch_depth_numpy_raw.shape[-1] == 3:
+                    batch_depth_gray = batch_depth_numpy_raw.mean(axis=-1)
+                elif batch_depth_numpy_raw.ndim == 4 and batch_depth_numpy_raw.shape[-1] == 1:
+                    batch_depth_gray = batch_depth_numpy_raw.squeeze(-1)
+                else:
+                    batch_depth_gray = batch_depth_numpy_raw
+                
+                batch_depth_float = batch_depth_gray.astype(np.float32)
+                
+                # Debug: Log raw depth range
+                logger.debug(f"[DEPTH] Raw depth range: min={batch_depth_float.min():.2f}, max={batch_depth_float.max():.2f}, shape={batch_depth_float.shape}")
+                
+                # Normalize to 0-1 range (matches original depthSplatting logic)
+                if assume_raw_input:
+                    # Raw input mode:
+                    # If global_depth_max is > 1.0 (e.g. 255 or 1023 passed from content scan), use it.
+                    # Otherwise fallback to max_expected_raw_value from metadata.
+                    if global_depth_max > 1.0:
+                         batch_depth_normalized = batch_depth_float / global_depth_max
+                    else:
+                         batch_depth_normalized = batch_depth_float / max(max_expected_raw_value, 1.0)
+                else:
+                    # Global normalization mode
+                    depth_range = global_depth_max - global_depth_min
+                    if depth_range > 1e-5:
+                        batch_depth_normalized = (batch_depth_float - global_depth_min) / depth_range
+                    else:
+                        # Collapsed range - fill with convergence anchor value
+                        batch_depth_normalized = np.full_like(
+                            batch_depth_float,
+                            fill_value=zero_disparity_anchor_val,
+                            dtype=np.float32,
+                        )
+                        logger.warning(
+                            f"Normalization collapsed to zero range ({global_depth_min:.4f} - {global_depth_max:.4f})."
+                        )
+                
+                batch_depth_normalized = np.clip(batch_depth_normalized, 0.0, 1.0)
+                logger.debug(f"[DEPTH] After normalization: min={batch_depth_normalized.min():.4f}, max={batch_depth_normalized.max():.4f}")
+                
+                # Apply gamma correction (inverted gamma formula)
+                if round(float(depth_gamma), 2) != 1.0:
+                    batch_depth_normalized = 1.0 - np.power(1.0 - batch_depth_normalized, depth_gamma)
+                    batch_depth_normalized = np.clip(batch_depth_normalized, 0.0, 1.0)
+                    logger.debug(f"[DEPTH] After gamma {depth_gamma}: min={batch_depth_normalized.min():.4f}, max={batch_depth_normalized.max():.4f}")
+                
+                # Convert back to "raw" format for process_depth_batch (which expects raw-like values)
+                # Scale back to max_raw_value range so dilation/blur work correctly
+                batch_depth_for_processing = batch_depth_normalized * max_expected_raw_value
+                
+                # Add channel dimension for process_depth_batch
+                if batch_depth_for_processing.ndim == 3:
+                    batch_depth_for_processing = batch_depth_for_processing[..., None]
+
+                # 3. Process depth batch (dilation/blur)
+                batch_depth_processed = process_depth_batch(
+                    batch_depth_numpy_raw=batch_depth_for_processing,
+                    depth_gamma=1.0,  # Already applied above
+                    depth_dilate_size_x=depth_dilate_size_x,
+                    depth_dilate_size_y=depth_dilate_size_y,
+                    depth_blur_size_x=depth_blur_size_x,
+                    depth_blur_size_y=depth_blur_size_y,
+                    max_raw_value=max_expected_raw_value,
+                    depth_dilate_left=depth_dilate_left,
+                    depth_blur_left=depth_blur_left,
+                    depth_blur_left_mix=depth_blur_left_mix,
+                    skip_preprocessing=skip_lowres_preproc and is_low_res_task,
+                )
+                
+                # Normalize back to 0-1 after processing
+                batch_depth_numpy_float = batch_depth_processed / max(max_expected_raw_value, 1.0)
+                batch_depth_numpy_float = np.clip(batch_depth_numpy_float, 0.0, 1.0)
+
+                # 4. GPU Splatting
+                batch_processed_frames = self._process_gpu_splatting(
+                    stereo_projector=stereo_projector,
+                    batch_video_numpy=batch_video_numpy,
+                    batch_depth_numpy_float=batch_depth_numpy_float,
+                    target_width=width,
+                    target_height=height,
+                    max_disp=max_disp,
+                    zero_disparity_anchor_val=zero_disparity_anchor_val,
+                    input_bias=input_bias,
+                )
+
+                # 5. Handle results (diag tests or FFmpeg write)
+                if is_test_mode and test_target_frame_idx is not None:
+                    self._handle_diagnostic_capture(batch_processed_frames, dual_output)
+                elif ffmpeg_process:
+                    self._write_to_ffmpeg(ffmpeg_process, batch_processed_frames, dual_output)
+
+                frame_count += len(batch_indices)
+                self.progress_queue.put(("processed", frame_count))
+                if not is_test_mode:
+                    draw_progress_bar(frame_count, total_frames_to_process, suffix=f"{task_name} Batch {i//batch_size}")
+                
+                # Cleanup batch
+                del batch_video_numpy, batch_depth_numpy_raw, batch_depth_numpy_float, batch_processed_frames
+                release_cuda_memory()
+
+        except Exception as e:
+            logger.error(f"Render error: {e}", exc_info=True)
+            encoding_successful = False
+        finally:
+            if ffmpeg_process:
+                try:
+                    ffmpeg_process.stdin.close()
+                    ffmpeg_process.wait(timeout=30)
+                except Exception as e:
+                    logger.warning(f"Error closing FFmpeg: {e}")
+            
+            del stereo_projector
+            release_cuda_memory()
+
+        return encoding_successful
+
+    def _log_color_metadata(self, info: Optional[dict], task_name: str):
+        if not info: return
+        try:
+            logger.debug(
+                f"[COLOR_META][{task_name}] input ffprobe: "
+                f"pix_fmt={info.get('pix_fmt')}, range={info.get('color_range')}, "
+                f"primaries={info.get('color_primaries')}, trc={info.get('transfer_characteristics')}, "
+                f"matrix={info.get('color_space')}"
+            )
+        except Exception: pass
+
+    def _get_encode_stream_info(self, source_info: Optional[dict], mode: str) -> dict:
+        info = dict(source_info) if source_info else {}
+        defaults = {
+            "color_primaries": "bt709",
+            "transfer_characteristics": "bt709",
+            "color_space": "bt709",
+            "color_range": "tv",
+        }
+        
+        if mode == "Auto":
+            for k, v in defaults.items(): info.setdefault(k, v)
+        elif mode == "BT.709 L":
+            info.update(defaults)
+        elif mode == "BT.709 F":
+            info.update(defaults)
+            info["color_range"] = "pc"
+        elif mode == "BT.2020 PQ":
+            info.update({"color_primaries": "bt2020", "transfer_characteristics": "smpte2084", "color_space": "bt2020nc", "color_range": "tv"})
+        elif mode == "BT.2020 HLG":
+            info.update({"color_primaries": "bt2020", "transfer_characteristics": "arib-std-b67", "color_space": "bt2020nc", "color_range": "tv"})
+        else:
+            for k, v in defaults.items(): info.setdefault(k, v)
+        return info
+
+    def _compare_encoding_flags(self, process: Any, task_name: str):
+        try:
+            flags = getattr(process, "sc_encode_flags", None)
+            if not flags: return
+            subset_keys = ["enc_codec", "enc_pix_fmt", "enc_profile", "enc_color_primaries", "enc_color_trc", "enc_colorspace", "quality_mode", "quality_value"]
+            subset = {k: flags.get(k) for k in subset_keys}
+            self._color_encode_flags[task_name] = subset
+            
+            other_name = "HiRes" if task_name == "LowRes" else "LowRes"
+            if other_name in self._color_encode_flags:
+                other = self._color_encode_flags[other_name]
+                diffs = {k: (other.get(k), subset.get(k)) for k in subset_keys if other.get(k) != subset.get(k)}
+                if diffs:
+                    logger.warning(f"[COLOR_META] Encoding flags differ ({other_name} vs {task_name}): {diffs}")
+                else:
+                    logger.debug(f"[COLOR_META] Encoding flags match between {other_name} and {task_name}.")
+        except Exception: pass
+
+    def _get_max_expected_raw_depth(self, info: Optional[dict]) -> float:
+        pix_fmt = info.get("pix_fmt") if info else None
+        profile = info.get("profile") if info else None
+        logger.debug(f"[DEPTH] Detecting bit depth from depth_stream_info: pix_fmt={pix_fmt}, profile={profile}, full_info={info}")
+        if pix_fmt:
+            if "10" in pix_fmt or "gray10" in pix_fmt or "12" in pix_fmt or (profile and "main10" in profile):
+                logger.debug(f"[DEPTH] Detected 10-bit depth (pix_fmt={pix_fmt})")
+                return 1023.0
+            if "8" in pix_fmt or pix_fmt in ["yuv420p", "yuv422p", "yuv444p"]:
+                logger.debug(f"[DEPTH] Detected 8-bit depth (pix_fmt={pix_fmt})")
+                return 255.0
+            if "float" in pix_fmt:
+                logger.debug(f"[DEPTH] Detected float depth (pix_fmt={pix_fmt})")
+                return 1.0
+        logger.warning(f"[DEPTH] Could not detect bit depth, defaulting to 1.0 (pix_fmt={pix_fmt})")
+        return 1.0
+
+    def _process_gpu_splatting(
+        self,
+        stereo_projector: ForwardWarpStereo,
+        batch_video_numpy: np.ndarray,
+        batch_depth_numpy_float: np.ndarray,
+        target_width: int,
+        target_height: int,
+        max_disp: float,
+        zero_disparity_anchor_val: float,
+        input_bias: float,
+    ) -> List[np.ndarray]:
+        """Process GPU splatting on normalized depth maps.
+        
+        Args:
+            batch_depth_numpy_float: Pre-normalized depth in range [0, 1]
+        """
+        # CRITICAL: Ensure depth matches video resolution before GPU processing
+        # batch_video_numpy: [B, H, W, 3]
+        # batch_depth_numpy_float: [B, H', W'] - already normalized to [0, 1]
+        
+        video_h, video_w = batch_video_numpy.shape[1], batch_video_numpy.shape[2]
+        
+        # Handle depth shape - ensure it's [B, H, W]
+        if batch_depth_numpy_float.ndim == 4:
+            if batch_depth_numpy_float.shape[-1] == 1:
+                batch_depth_numpy_float = batch_depth_numpy_float.squeeze(-1)
+            elif batch_depth_numpy_float.shape[-1] == 3:
+                batch_depth_numpy_float = batch_depth_numpy_float[..., 0]
+        
+        depth_h, depth_w = batch_depth_numpy_float.shape[1], batch_depth_numpy_float.shape[2]
+        
+        # Resize depth if dimensions don't match
+        if depth_h != video_h or depth_w != video_w:
+            logger.debug(f"Resizing depth from {depth_w}x{depth_h} to match video {video_w}x{video_h}")
+
+            
+            interp = cv2.INTER_AREA if (video_w < depth_w and video_h < depth_h) else cv2.INTER_LINEAR
+            resized_depth = np.empty((batch_depth_numpy_float.shape[0], video_h, video_w), dtype=batch_depth_numpy_float.dtype)
+            
+            for idx in range(batch_depth_numpy_float.shape[0]):
+                resized_depth[idx] = cv2.resize(
+                    batch_depth_numpy_float[idx],
+                    (video_w, video_h),
+                    interpolation=interp
+                )
+            batch_depth_numpy_float = resized_depth
+        
+        # Move to GPU
+        source_tensor = torch.from_numpy(batch_video_numpy).permute(0, 3, 1, 2).float().cuda() / 255.0
+        depth_tensor = torch.from_numpy(batch_depth_numpy_float).unsqueeze(1).float().cuda()
+
+        # Depth is already normalized to [0, 1], just clip and apply bias
+        depth_tensor = torch.clip(depth_tensor, 0.0, 1.0)
+        
+        if input_bias != 0:
+            depth_tensor = torch.clip(depth_tensor + input_bias, 0.0, 1.0)
+
+        # Disparity calculation
+        disp_map = (depth_tensor - zero_disparity_anchor_val) * 2.0
+        actual_max_disp_pixels = (max_disp / 20.0 / 100.0) * target_width
+        disp_map = disp_map * actual_max_disp_pixels
+
+        # Forward warp
+        with torch.no_grad():
+            right_eye_raw, occlusion_mask = stereo_projector(source_tensor, disp_map)
+        
+        # CPU conversion
+        left_cpu = source_tensor.cpu().numpy()
+        right_cpu = right_eye_raw.cpu().numpy()
+        occl_cpu = occlusion_mask.cpu().numpy()
+        depth_cpu = depth_tensor.cpu().numpy()
+
+        results = []
+        for j in range(len(batch_video_numpy)):
+            results.append({
+                "left": (np.clip(left_cpu[j].transpose(1, 2, 0), 0, 1) * 255).astype(np.uint8),
+                "right": (np.clip(right_cpu[j].transpose(1, 2, 0), 0, 1) * 255).astype(np.uint8),
+                "occlusion": (np.clip(occl_cpu[j].transpose(1, 2, 0), 0, 1) * 255).astype(np.uint8),
+                "depth": (np.clip(depth_cpu[j].transpose(1, 2, 0), 0, 1) * 255).astype(np.uint8),
+            })
+        return results
+
+
+    def _handle_diagnostic_capture(self, batch_results: List[dict], dual_output: bool):
+        # In test mode, we usually only have one frame
+        res = batch_results[0]
+        # This is a bit tricky since we don't have direct access to GUI previewer here.
+        # We'll put it in the queue for the GUI to handle.
+        grid = self._construct_grid(res, dual_output)
+        self.progress_queue.put(("diagnostic_capture", grid))
+
+    def _write_to_ffmpeg(self, process: Any, batch_results: List[dict], dual_output: bool):
+        for res in batch_results:
+            grid = self._construct_grid(res, dual_output)
+            # Convert to 16-bit and BGR for FFmpeg
+            grid_uint16 = (np.clip(grid, 0.0, 1.0) * 65535.0).astype(np.uint16)
+            grid_bgr = cv2.cvtColor(grid_uint16, cv2.COLOR_RGB2BGR)
+            process.stdin.write(grid_bgr.tobytes())
+
+    def _construct_grid(self, res: dict, dual_output: bool) -> np.ndarray:
+        """Construct output grid for encoding.
+        
+        dual_output=True: [occlusion_mask | right_eye] (2-panel)
+        dual_output=False: [left_eye | depth_vis]
+                           [occlusion_mask | right_eye] (4-panel)
+        
+        Returns float32 array in range [0, 1]
+        """
+        # Convert uint8 back to float for grid assembly
+        left = res["left"].astype(np.float32) / 255.0
+        right = res["right"].astype(np.float32) / 255.0
+        occlusion = res["occlusion"].astype(np.float32) / 255.0
+        depth = res["depth"].astype(np.float32) / 255.0
+        
+        # Ensure all are 3-channel
+        if occlusion.ndim == 2 or (occlusion.ndim == 3 and occlusion.shape[-1] == 1):
+            occlusion = np.stack([occlusion.squeeze()] * 3, axis=-1)
+        if depth.ndim == 2 or (depth.ndim == 3 and depth.shape[-1] == 1):
+            depth = np.stack([depth.squeeze()] * 3, axis=-1)
+        
+        if dual_output:
+            # 2-panel: occlusion on left, warped right eye on right
+            return np.concatenate([occlusion, right], axis=1)
+        else:
+            # 4-panel: top row (left, depth), bottom row (occlusion, right)
+            top_row = np.concatenate([left, depth], axis=1)
+            bot_row = np.concatenate([occlusion, right], axis=1)
+            return np.concatenate([top_row, bot_row], axis=0)
+
diff --git a/dependency/stereocrafter_util.py b/dependency/stereocrafter_util.py
index c8b898a..d5ef1d0 100644
--- a/dependency/stereocrafter_util.py
+++ b/dependency/stereocrafter_util.py
@@ -14,6 +14,7 @@ import subprocess
 import cv2
 import gc
 import time
+import re
 
 VERSION = "26-01-30.0"
 
@@ -965,7 +966,7 @@ def draw_progress_bar(current, total, bar_length=50, prefix="Progress:", suffix=
     # Format the suffix for completion
     actual_suffix = suffix
     if current == total:
-        actual_suffix = "Complete"
+        actual_suffix = "Complete      "
 
     print(f"\r{prefix} |{bar}| {percent:.1f}% {actual_suffix}", end="", flush=True)
 
@@ -1311,7 +1312,14 @@ def get_video_stream_info(video_path: str) -> Optional[dict]:
             encoding="utf-8",
             timeout=500,
         )
-        data = json.loads(result.stdout)
+        try:
+            data = json.loads(result.stdout)
+        except json.JSONDecodeError as e:
+            # logger.error(f"Failed to parse ffprobe output for {video_path}: {e}")
+            # Hackish fix for ffprobe version 4.4.2-0ubuntu0.22.04.1 returning bad side_data_list with missing "," that causes this error.
+            # "Expecting ',' delimiter: line 13 column 40 (char 229)"
+            repaired = re.sub(r'("type"\s*:\s*"[^"]+")(\s+)("side_data_list"\s*:)', r'\1,\2\3', result.stdout)
+            data = json.loads(repaired)
 
         stream_info = {}
         if "streams" in data and len(data["streams"]) > 0:
@@ -1375,7 +1383,7 @@ def get_video_stream_info(video_path: str) -> Optional[dict]:
         return None
     except json.JSONDecodeError as e:
         logger.error(f"Failed to parse ffprobe output for {video_path}: {e}")
-        logger.debug(f"Raw ffprobe stdout: {result.stdout}")
+        logger.error(f"Raw ffprobe stdout: {result.stdout}")
         return None
     except Exception as e:
         logger.error(
@@ -1761,7 +1769,7 @@ def start_ffmpeg_pipe_process(
     }
 
     if debug_label:
-        logger.info(
+        logger.debug(
             f"[COLOR_META][{debug_label}] src(pix_fmt={src_pix_fmt}, range={src_range}, primaries={src_prim}, trc={src_trc}, matrix={src_matrix}) "
             f"-> enc(codec={output_codec}, pix_fmt={output_pix_fmt}, profile={output_profile}, primaries={color_primaries}, trc={transfer_characteristics}, matrix={color_space}, {quality_mode}={quality_value})"
         )
diff --git a/splatting_gui.legacy.py b/splatting_gui.legacy.py
new file mode 100644
index 0000000..230554f
--- /dev/null
+++ b/splatting_gui.legacy.py
@@ -0,0 +1,10350 @@
+import gc
+import os
+import re
+import csv
+import cv2
+import glob
+import shutil
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torchvision.io import write_video
+from decord import VideoReader, cpu
+import tkinter as tk
+from tkinter import filedialog, ttk, messagebox
+import tkinter.font as tkfont
+from ttkthemes import ThemedTk
+import json
+import threading
+import queue
+import subprocess
+import time
+import logging
+from typing import Optional, Tuple, Any
+from PIL import Image
+import math
+
+# --- Depth Map Visualization Levels ---
+# These affect ONLY depth-map visualization (Preview 'Depth Map' and Map Test images),
+# not the depth values used for splatting.
+DEPTH_VIS_APPLY_TV_RANGE_EXPANSION_10BIT = True
+DEPTH_VIS_TV10_BLACK_NORM = 64.0 / 1023.0
+DEPTH_VIS_TV10_WHITE_NORM = 940.0 / 1023.0
+
+try:
+    from moviepy.editor import VideoFileClip
+except ImportError:
+    # Fallback/stub for systems without moviepy
+    class VideoFileClip:
+        def __init__(self, *args, **kwargs):
+            logging.warning("moviepy.editor not found. Frame counting disabled.")
+
+        def close(self):
+            pass
+
+        @property
+        def fps(self):
+            return None
+
+        @property
+        def duration(self):
+            return None
+
+
+# Import custom modules
+CUDA_AVAILABLE = False  # start state, will check automaticly later
+
+# --- MODIFIED IMPORT ---
+from dependency.stereocrafter_util import (
+    Tooltip,
+    logger,
+    get_video_stream_info,
+    draw_progress_bar,
+    check_cuda_availability,
+    release_cuda_memory,
+    CUDA_AVAILABLE,
+    set_util_logger_level,
+    start_ffmpeg_pipe_process,
+    custom_blur,
+    custom_dilate,
+    custom_dilate_left,
+    create_single_slider_with_label_updater,
+    create_dual_slider_layout,
+    SidecarConfigManager,
+    apply_dubois_anaglyph,
+    apply_optimized_anaglyph,
+)
+
+try:
+    from Forward_Warp import forward_warp
+
+    logger.info("CUDA Forward Warp is available.")
+except:
+    from dependency.forward_warp_pytorch import forward_warp
+
+    logger.info("Forward Warp Pytorch is active.")
+from dependency.video_previewer import VideoPreviewer
+
+GUI_VERSION = "26-01-30.0"
+
+
+class FusionSidecarGenerator:
+    """Handles parsing Fusion Export files, matching them to depth maps,
+    and generating/saving FSSIDECAR files using carry-forward logic."""
+
+    FUSION_PARAMETER_CONFIG = {
+        # Key: {Label, Type, Default, FusionKey(fsexport), SidecarKey(fssidecar), Decimals}
+        "convergence": {
+            "label": "Convergence Plane",
+            "type": float,
+            "default": 0.5,
+            "fusion_key": "Convergence",
+            "sidecar_key": "convergence_plane",
+            "decimals": 3,
+        },
+        "max_disparity": {
+            "label": "Max Disparity",
+            "type": float,
+            "default": 35.0,
+            "fusion_key": "MaxDisparity",
+            "sidecar_key": "max_disparity",
+            "decimals": 1,
+        },
+        "gamma": {
+            "label": "Gamma Correction",
+            "type": float,
+            "default": 1.0,
+            "fusion_key": "FrontGamma",
+            "sidecar_key": "gamma",
+            "decimals": 2,
+        },
+        # These keys exist in the sidecar manager but are usually set in the source tool
+        # We include them here for completeness if Fusion ever exported them
+        "frame_overlap": {
+            "label": "Frame Overlap",
+            "type": float,
+            "default": 3,
+            "fusion_key": "Overlap",
+            "sidecar_key": "frame_overlap",
+            "decimals": 0,
+        },
+        "input_bias": {
+            "label": "Input Bias",
+            "type": float,
+            "default": 0.0,
+            "fusion_key": "Bias",
+            "sidecar_key": "input_bias",
+            "decimals": 2,
+        },
+        "left_border": {
+            "label": "Left Border",
+            "type": float,
+            "default": 0.0,
+            "fusion_key": "LeftBorder",
+            "sidecar_key": "left_border",
+            "decimals": 3,
+        },
+        "right_border": {
+            "label": "Right Border",
+            "type": float,
+            "default": 0.0,
+            "fusion_key": "RightBorder",
+            "sidecar_key": "right_border",
+            "decimals": 3,
+        },
+        "manual_border": {
+            "label": "Border Mode",
+            "type": str,
+            "default": "Off",
+            "fusion_key": "BorderMode",
+            "sidecar_key": "border_mode",
+            "decimals": 0,
+        },
+        "auto_border_l": {
+            "label": "Auto Border L",
+            "type": float,
+            "default": 0.0,
+            "fusion_key": "AutoBorderL",
+            "sidecar_key": "auto_border_L",
+            "decimals": 3,
+        },
+        "auto_border_r": {
+            "label": "Auto Border R",
+            "type": float,
+            "default": 0.0,
+            "fusion_key": "AutoBorderR",
+            "sidecar_key": "auto_border_R",
+            "decimals": 3,
+        },
+    }
+
+    def __init__(self, master_gui, sidecar_manager):
+        self.master_gui = master_gui
+        self.sidecar_manager = sidecar_manager
+        self.logger = logging.getLogger(__name__)
+
+    def _get_video_frame_count(self, file_path):
+        """Safely gets the frame count of a video file using moviepy."""
+        try:
+            clip = VideoFileClip(file_path)
+            fps = clip.fps
+            duration = clip.duration
+            if fps is None or duration is None:
+                # If moviepy failed to get reliable info, fall back
+                fps = 24
+                if duration is None:
+                    return 0
+
+            frames = math.ceil(duration * fps)
+            clip.close()
+            return frames
+        except Exception as e:
+            self.logger.warning(
+                f"Error getting frame count for {os.path.basename(file_path)}: {e}"
+            )
+            return 0
+
+    def _load_and_validate_fsexport(self, file_path):
+        """Loads, parses, and validates marker data from a Fusion Export file."""
+        try:
+            with open(file_path, "r") as f:
+                export_data = json.load(f)
+        except json.JSONDecodeError as e:
+            messagebox.showerror(
+                "File Error",
+                f"Failed to parse JSON in {os.path.basename(file_path)}: {e}",
+            )
+            return None
+        except Exception as e:
+            messagebox.showerror(
+                "File Error", f"Failed to read {os.path.basename(file_path)}: {e}"
+            )
+            return None
+
+        markers = export_data.get("markers", [])
+        if not markers:
+            messagebox.showwarning(
+                "Data Warning", "No 'markers' found in the export file."
+            )
+            return None
+
+        # Sort markers by frame number (critical for carry-forward logic)
+        markers.sort(key=lambda m: m["frame"])
+        self.logger.info(
+            f"Loaded {len(markers)} markers from {os.path.basename(file_path)}."
+        )
+        return markers
+
+    def _scan_target_videos(self, folder):
+        """Scans the target folder for video files and computes their frame counts."""
+        video_extensions = ("*.mp4", "*.avi", "*.mov", "*.mkv")
+        found_files_paths = []
+        for ext in video_extensions:
+            found_files_paths.extend(glob.glob(os.path.join(folder, ext)))
+        sorted_files_paths = sorted(found_files_paths)
+
+        if not sorted_files_paths:
+            messagebox.showwarning(
+                "No Files", f"No video depth map files found in: {folder}"
+            )
+            return None
+
+        target_video_data = []
+        cumulative_frames = 0
+
+        for full_path in sorted_files_paths:
+            total_frames = self._get_video_frame_count(full_path)
+
+            if total_frames == 0:
+                self.logger.warning(
+                    f"Skipping {os.path.basename(full_path)} due to zero frame count."
+                )
+                continue
+
+            target_video_data.append(
+                {
+                    "full_path": full_path,
+                    "basename": os.path.basename(full_path),
+                    "total_frames": total_frames,
+                    "timeline_start_frame": cumulative_frames,
+                    "timeline_end_frame": cumulative_frames + total_frames - 1,
+                }
+            )
+            cumulative_frames += total_frames
+
+        self.logger.info(
+            f"Scanned {len(target_video_data)} video files. Total timeline frames: {cumulative_frames}."
+        )
+        return target_video_data
+
+    def generate_sidecars(self):
+        """Main entry point for the Fusion Export to Sidecar generation workflow."""
+
+        # 1. Select Fusion Export File
+        export_file_path = filedialog.askopenfilename(
+            defaultextension=".fsexport",
+            filetypes=[
+                ("Fusion Export Files", "*.fsexport.txt;*.fsexport"),
+                ("All Files", "*.*"),
+            ],
+            title="Select Fusion Export (.fsexport) File",
+        )
+        if not export_file_path:
+            self.master_gui.status_label.config(
+                text="Fusion export selection cancelled."
+            )
+            return
+
+        markers = self._load_and_validate_fsexport(export_file_path)
+        if markers is None:
+            self.master_gui.status_label.config(text="Fusion export loading failed.")
+            return
+
+        # 2. Select Target Depth Map Folder
+        target_folder = filedialog.askdirectory(title="Select Target Depth Map Folder")
+        if not target_folder:
+            self.master_gui.status_label.config(
+                text="Depth map folder selection cancelled."
+            )
+            return
+
+        target_videos = self._scan_target_videos(target_folder)
+        if target_videos is None or not target_videos:
+            self.master_gui.status_label.config(text="No valid depth map videos found.")
+            return
+
+        # 3. Apply Parameters (Carry-Forward Logic)
+        applied_count = 0
+
+        # Initialize last known values with the config defaults
+        last_param_vals = {}
+        for key, config in self.FUSION_PARAMETER_CONFIG.items():
+            last_param_vals[key] = config["default"]
+
+        for file_data in target_videos:
+            file_start_frame = file_data["timeline_start_frame"]
+
+            # Find the most relevant marker (latest marker frame <= file_start_frame)
+            relevant_marker = None
+            for marker in markers:
+                if marker["frame"] <= file_start_frame:
+                    relevant_marker = marker
+                else:
+                    break
+
+            current_param_vals = last_param_vals.copy()
+
+            if relevant_marker and relevant_marker.get("values"):
+                marker_values = relevant_marker["values"]
+                updated_from_marker = False
+
+                for key, config in self.FUSION_PARAMETER_CONFIG.items():
+                    fusion_key = config["fusion_key"]
+                    default_val = config["default"]
+
+                    if fusion_key in marker_values:
+                        # Attempt to cast the value from the marker to the expected type
+                        val = marker_values.get(fusion_key, default_val)
+                        try:
+                            current_param_vals[key] = config["type"](val)
+                            updated_from_marker = True
+                        except (ValueError, TypeError):
+                            self.logger.warning(
+                                f"Marker value for '{fusion_key}' is invalid ({val}). Using previous/default value."
+                            )
+
+                if updated_from_marker:
+                    applied_count += 1
+
+            # 4. Save Sidecar JSON
+            sidecar_data = {}
+            for key, config in self.FUSION_PARAMETER_CONFIG.items():
+                value = current_param_vals[key]
+                if config["type"] is bool:
+                    sidecar_data[config["sidecar_key"]] = bool(value)
+                else:
+                    # Round to configured decimals for clean sidecar output
+                    sidecar_data[config["sidecar_key"]] = round(
+                        float(value), config["decimals"]
+                    )
+
+            base_name_without_ext = os.path.splitext(file_data["full_path"])[0]
+            json_filename = (
+                base_name_without_ext + ".fssidecar"
+            )  # Target sidecar extension
+
+            if not self.sidecar_manager.save_sidecar_data(json_filename, sidecar_data):
+                self.logger.error(
+                    f"Failed to save sidecar for {file_data['basename']}."
+                )
+
+            # Update last values for carry-forward to the next file
+            last_param_vals = current_param_vals.copy()
+
+        # 5. Final Status
+        if applied_count == 0:
+            self.master_gui.status_label.config(
+                text="Finished: No parameters were applied from the export file."
+            )
+        else:
+            self.master_gui.status_label.config(
+                text=f"Finished: Applied markers to {applied_count} files, generated {len(target_videos)} FSSIDECARs."
+            )
+        messagebox.showinfo(
+            "Sidecar Generation Complete",
+            f"Successfully processed {os.path.basename(export_file_path)} and generated {len(target_videos)} FSSIDECAR files.",
+        )
+
+    def generate_custom_sidecars(self):
+        """Generates sidecars with a custom name without requiring existing video files."""
+
+        # 1. Select Fusion Export File
+        export_file_path = filedialog.askopenfilename(
+            defaultextension=".fsexport",
+            filetypes=[
+                ("Fusion Export Files", "*.fsexport.txt;*.fsexport"),
+                ("All Files", "*.*"),
+            ],
+            title="Select Fusion Export (.fsexport) File",
+        )
+        if not export_file_path:
+            self.master_gui.status_label.config(
+                text="Fusion export selection cancelled."
+            )
+            return
+
+        markers = self._load_and_validate_fsexport(export_file_path)
+        if markers is None:
+            self.master_gui.status_label.config(text="Fusion export loading failed.")
+            return
+
+        # 2. Select Output Sidecar Path and Name
+        custom_save_path = filedialog.asksaveasfilename(
+            defaultextension=".fssidecar",
+            filetypes=[("Sidecar Files", "*.fssidecar")],
+            title="Save Sidecar As",
+            initialfile=os.path.splitext(os.path.basename(export_file_path))[0],
+        )
+        if not custom_save_path:
+            self.master_gui.status_label.config(text="Custom sidecar export cancelled.")
+            return
+
+        # 3. Process markers
+        applied_count = 0
+        last_param_vals = {
+            key: config["default"]
+            for key, config in self.FUSION_PARAMETER_CONFIG.items()
+        }
+
+        for i, marker in enumerate(markers):
+            current_param_vals = last_param_vals.copy()
+            if marker.get("values"):
+                marker_values = marker["values"]
+                updated_from_marker = False
+                for key, config in self.FUSION_PARAMETER_CONFIG.items():
+                    fusion_key = config["fusion_key"]
+                    if fusion_key in marker_values:
+                        try:
+                            current_param_vals[key] = config["type"](
+                                marker_values[fusion_key]
+                            )
+                            updated_from_marker = True
+                        except (ValueError, TypeError):
+                            pass
+                if updated_from_marker:
+                    applied_count += 1
+
+            # Prepare sidecar data
+            sidecar_data = {}
+            for key, config in self.FUSION_PARAMETER_CONFIG.items():
+                value = current_param_vals[key]
+                if config["type"] is bool:
+                    sidecar_data[config["sidecar_key"]] = bool(value)
+                else:
+                    sidecar_data[config["sidecar_key"]] = round(
+                        float(value), config["decimals"]
+                    )
+
+            # Determine filename
+            if len(markers) == 1:
+                target_filename = custom_save_path
+            else:
+                # Append index if multiple markers (5-digit zero-padded)
+                base, ext = os.path.splitext(custom_save_path)
+                target_filename = f"{base}_{i + 1:04d}{ext}"
+
+            if not self.sidecar_manager.save_sidecar_data(
+                target_filename, sidecar_data
+            ):
+                self.logger.error(f"Failed to save custom sidecar: {target_filename}")
+
+            last_param_vals = current_param_vals.copy()
+
+        # 4. Final Status
+        self.master_gui.status_label.config(
+            text=f"Finished: Generated {len(markers)} custom FSSIDECARs."
+        )
+        messagebox.showinfo(
+            "Custom Export Complete",
+            f"Successfully generated {len(markers)} custom FSSIDECAR files.",
+        )
+
+
+class ForwardWarpStereo(nn.Module):
+    """
+    PyTorch module for forward warping an image based on a disparity map.
+    """
+
+    def __init__(self, eps=1e-6, occlu_map=False):
+        super(ForwardWarpStereo, self).__init__()
+        self.eps = eps
+        self.occlu_map = occlu_map
+        self.fw = forward_warp()
+
+    def forward(self, im, disp):
+        im = im.contiguous()
+        disp = disp.contiguous()
+        weights_map = disp - disp.min()
+        weights_map = (1.414) ** weights_map
+        flow = -disp.squeeze(1)
+        dummy_flow = torch.zeros_like(flow, requires_grad=False)
+        flow = torch.stack((flow, dummy_flow), dim=-1)
+        res_accum = self.fw(im * weights_map, flow)
+        mask = self.fw(weights_map, flow)
+        mask.clamp_(min=self.eps)
+        res = res_accum / mask
+        if not self.occlu_map:
+            return res
+        else:
+            ones = torch.ones_like(disp, requires_grad=False)
+            occlu_map = self.fw(ones, flow)
+            occlu_map.clamp_(0.0, 1.0)
+            occlu_map = 1.0 - occlu_map
+            return res, occlu_map
+
+
+class SplatterGUI(ThemedTk):
+    # --- UI MINIMUM WIDTHS (tweak these numbers) ---
+    # These are used as grid column *minimums* for the left (settings) and middle (sliders) columns.
+    # Tkinter's grid doesn't support true max-width caps, so the previous max-width clamp code was removed.
+    UI_PROCESS_COL_MIN = 330
+    UI_DEPTH_COL_MIN = 520
+
+    # --- GLOBAL CONFIGURATION DICTIONARY ---
+    APP_CONFIG_DEFAULTS = {
+        # File Extensions
+        "SIDECAR_EXT": ".fssidecar",
+        "OUTPUT_SIDECAR_EXT": ".spsidecar",
+        "DEFAULT_CONFIG_FILENAME": "config_splat.splatcfg",
+        # GUI/Processing Defaults (Used for reset/fallback)
+        "MAX_DISP": "30.0",
+        "CONV_POINT": "0.5",
+        "PROC_LENGTH": "-1",
+        "BATCH_SIZE_FULL": "10",
+        "BATCH_SIZE_LOW": "15",
+        "CRF_OUTPUT": "23",
+        # Depth Processing Defaults
+        "DEPTH_GAMMA": "1.0",
+        "DEPTH_DILATE_SIZE_X": "3",
+        "DEPTH_DILATE_SIZE_Y": "3",
+        "DEPTH_BLUR_SIZE_X": "5",
+        "DEPTH_BLUR_SIZE_Y": "5",
+        "DEPTH_DILATE_LEFT": "0",
+        "DEPTH_BLUR_LEFT": "0",
+        "DEPTH_BLUR_LEFT_MIX": "0.5",
+        "BORDER_WIDTH": "0.0",
+        "BORDER_BIAS": "0.0",
+        "BORDER_LEFT": "0.0",
+        "BORDER_RIGHT": "0.0",
+        "BORDER_MODE": "Off",
+        "AUTO_BORDER_L": "0.0",
+        "AUTO_BORDER_R": "0.0",
+    }
+    # ---------------------------------------
+    # Maps Sidecar JSON Key to the internal variable key (used in APP_CONFIG_DEFAULTS)
+    SIDECAR_KEY_MAP = {
+        "convergence_plane": "CONV_POINT",
+        "max_disparity": "MAX_DISP",
+        "gamma": "DEPTH_GAMMA",
+        "depth_dilate_size_x": "DEPTH_DILATE_SIZE_X",
+        "depth_dilate_size_y": "DEPTH_DILATE_SIZE_Y",
+        "depth_blur_size_x": "DEPTH_BLUR_SIZE_X",
+        "depth_blur_size_y": "DEPTH_BLUR_SIZE_Y",
+        "depth_dilate_left": "DEPTH_DILATE_LEFT",
+        "depth_blur_left": "DEPTH_BLUR_LEFT",
+        "depth_blur_left_mix": "DEPTH_BLUR_LEFT_MIX",
+        "frame_overlap": "FRAME_OVERLAP",
+        "input_bias": "INPUT_BIAS",
+        "selected_depth_map": "SELECTED_DEPTH_MAP",
+        "left_border": "BORDER_LEFT",
+        "right_border": "BORDER_RIGHT",
+        "border_mode": "BORDER_MODE",
+        "auto_border_L": "AUTO_BORDER_L",
+        "auto_border_R": "AUTO_BORDER_R",
+    }
+    MOVE_TO_FINISHED_ENABLED = True
+    # ---------------------------------------
+
+    def __init__(self):
+        super().__init__(theme="default")
+        self.title(f"Stereocrafter Splatting (Batch) {GUI_VERSION}")
+
+        self.app_config = {}
+        self.help_texts = {}
+        self.sidecar_manager = SidecarConfigManager()
+        # Cache: estimated per-clip max Total(D+P) keyed by signature
+        self._dp_total_est_cache = {}
+
+        # Cache: measured (render-time) per-clip max Total(D+P) keyed by signature
+        self._dp_total_true_cache = {}
+        self._dp_total_true_active_sig = None
+        self._dp_total_true_active_val = None
+
+        # Cache: AUTO-PASS CSV rows (optional) keyed by depth_map basename
+        self._auto_pass_csv_cache = None
+        self._auto_pass_csv_path = None
+
+
+        # --- NEW CACHE AND STATE ---
+        self._auto_conv_cache = {"Average": None, "Peak": None}
+        self._auto_conv_cached_path = None
+        self._is_auto_conv_running = False
+        self._preview_debounce_timer = None
+        self.slider_label_updaters = []
+        self.set_convergence_value_programmatically = None
+        self._clip_norm_cache: Dict[str, Tuple[float, float]] = {}
+        self._gn_warning_shown: bool = False
+
+        self._load_config()
+        self._load_help_texts()
+
+        self._is_startup = True  # NEW: for theme/geometry handling
+        self.debug_mode_var = tk.BooleanVar(
+            value=self.app_config.get("debug_mode_enabled", False)
+        )
+        self._debug_logging_enabled = False  # start in INFO mode
+        # NEW: Window size and position variables
+        self.window_x = self.app_config.get("window_x", None)
+        self.window_y = self.app_config.get("window_y", None)
+        self.window_width = self.app_config.get("window_width", 620)
+        self.window_height = self.app_config.get("window_height", 750)
+
+        # --- Variables with defaults ---
+        defaults = self.APP_CONFIG_DEFAULTS  # Convenience variable
+
+        self.dark_mode_var = tk.BooleanVar(
+            value=self.app_config.get("dark_mode_enabled", False)
+        )
+        self.input_source_clips_var = tk.StringVar(
+            value=self.app_config.get("input_source_clips", "./input_source_clips")
+        )
+        self.input_depth_maps_var = tk.StringVar(
+            value=self.app_config.get("input_depth_maps", "./input_depth_maps")
+        )
+        self.multi_map_var = tk.BooleanVar(
+            value=bool(self.app_config.get("multi_map_enabled", False))
+        )
+        self.selected_depth_map_var = tk.StringVar(value="")
+        self.depth_map_subfolders = []  # List of valid subfolders
+        self.depth_map_radio_buttons = []  # keep list for UI management
+        self.depth_map_radio_dict = {}  # NEW: map text->widget
+        self._current_video_sidecar_map = None  # Track sidecar's selected map
+        self._suppress_sidecar_map_update = (
+            False  # Prevent overwriting manual selections
+        )
+        self._last_loaded_source_video = (
+            None  # Track source video for NEW video detection
+        )
+        self.input_depth_maps_var.trace_add(
+            "write", lambda *args: self._on_depth_map_folder_changed()
+        )
+        self.output_splatted_var = tk.StringVar(
+            value=self.app_config.get("output_splatted", "./output_splatted")
+        )
+
+        self.max_disp_var = tk.StringVar(
+            value=self.app_config.get("max_disp", defaults["MAX_DISP"])
+        )
+        self.process_length_var = tk.StringVar(
+            value=self.app_config.get("process_length", defaults["PROC_LENGTH"])
+        )
+        self.process_from_var = tk.StringVar(value="")
+        self.process_to_var = tk.StringVar(value="")
+        self.batch_size_var = tk.StringVar(
+            value=self.app_config.get("batch_size", defaults["BATCH_SIZE_FULL"])
+        )
+
+        self.dual_output_var = tk.BooleanVar(
+            value=self.app_config.get("dual_output", False)
+        )
+        self.enable_global_norm_var = tk.BooleanVar(
+            value=self.app_config.get("enable_global_norm", False)
+        )
+        self.enable_full_res_var = tk.BooleanVar(
+            value=self.app_config.get("enable_full_resolution", True)
+        )
+        self.enable_low_res_var = tk.BooleanVar(
+            value=self.app_config.get("enable_low_resolution", True)
+        )
+        self.pre_res_width_var = tk.StringVar(
+            value=self.app_config.get("pre_res_width", "1024")
+        )
+        self.pre_res_height_var = tk.StringVar(
+            value=self.app_config.get("pre_res_height", "512")
+        )
+        self.low_res_batch_size_var = tk.StringVar(
+            value=self.app_config.get("low_res_batch_size", defaults["BATCH_SIZE_LOW"])
+        )
+        self.zero_disparity_anchor_var = tk.StringVar(
+            value=self.app_config.get("convergence_point", defaults["CONV_POINT"])
+        )
+        self.output_crf_var = tk.StringVar(
+            value=self.app_config.get("output_crf", defaults["CRF_OUTPUT"])
+        )
+        # Separate CRF values for Full vs Low output (fallback to legacy "output_crf")
+        _legacy_crf = self.app_config.get("output_crf", defaults["CRF_OUTPUT"])
+        self.output_crf_full_var = tk.StringVar(
+            value=self.app_config.get("output_crf_full", _legacy_crf)
+        )
+        self.output_crf_low_var = tk.StringVar(
+            value=self.app_config.get("output_crf_low", _legacy_crf)
+        )
+        # Output color metadata tags (metadata-only; does not affect splat math)
+        self.color_tags_mode_var = tk.StringVar(
+            value=self.app_config.get("color_tags_mode", "Auto")
+        )
+
+        # Dev Tools (not saved to config)
+        self.skip_lowres_preproc_var = tk.BooleanVar(
+            value=bool(self.app_config.get("skip_lowres_preproc", False))
+        )
+
+        # Dev Tools: Render-time exact Max Total(D+P) tracking (toggle for perf testing; not saved)
+        self.track_dp_total_true_on_render_var = tk.BooleanVar(value=False)
+
+        self.move_to_finished_var = tk.BooleanVar(
+            value=self.app_config.get("move_to_finished", True)
+        )
+
+        # Crosshair overlay for convergence checking (preview-only; not exported)
+        self.crosshair_enabled_var = tk.BooleanVar(
+            value=bool(self.app_config.get("crosshair_enabled", False))
+        )
+        self.crosshair_white_var = tk.BooleanVar(
+            value=bool(self.app_config.get("crosshair_white", False))
+        )
+        self.crosshair_multi_var = tk.BooleanVar(
+            value=bool(self.app_config.get("crosshair_multi", False))
+        )
+        self.depth_pop_enabled_var = tk.BooleanVar(
+            value=bool(self.app_config.get("depth_pop_enabled", False))
+        )
+
+        self.auto_convergence_mode_var = tk.StringVar(
+            value=self.app_config.get("auto_convergence_mode", "Off")
+        )
+
+        # --- Depth Pre-processing Variables ---
+        self.depth_gamma_var = tk.StringVar(
+            value=self.app_config.get("depth_gamma", defaults["DEPTH_GAMMA"])
+        )
+        _ddx = self.app_config.get(
+            "depth_dilate_size_x", defaults["DEPTH_DILATE_SIZE_X"]
+        )
+        _ddy = self.app_config.get(
+            "depth_dilate_size_y", defaults["DEPTH_DILATE_SIZE_Y"]
+        )
+        # Backward compatibility: older configs used 30..40 to represent erosion (-0..-10)
+        try:
+            _ddx_f = float(_ddx)
+            if 30.0 < _ddx_f <= 40.0:
+                _ddx = -(_ddx_f - 30.0)
+        except Exception:
+            pass
+        try:
+            _ddy_f = float(_ddy)
+            if 30.0 < _ddy_f <= 40.0:
+                _ddy = -(_ddy_f - 30.0)
+        except Exception:
+            pass
+        self.depth_dilate_size_x_var = tk.StringVar(value=str(_ddx))
+        self.depth_dilate_size_y_var = tk.StringVar(value=str(_ddy))
+        self.depth_blur_size_x_var = tk.StringVar(
+            value=self.app_config.get(
+                "depth_blur_size_x", defaults["DEPTH_BLUR_SIZE_X"]
+            )
+        )
+        self.depth_blur_size_y_var = tk.StringVar(
+            value=self.app_config.get(
+                "depth_blur_size_y", defaults["DEPTH_BLUR_SIZE_Y"]
+            )
+        )
+        self.depth_dilate_left_var = tk.StringVar(
+            value=self.app_config.get(
+                "depth_dilate_left", defaults["DEPTH_DILATE_LEFT"]
+            )
+        )
+        self.depth_blur_left_var = tk.StringVar(
+            value=self.app_config.get("depth_blur_left", defaults["DEPTH_BLUR_LEFT"])
+        )
+        self.depth_blur_left_mix_var = tk.StringVar(
+            value=self.app_config.get(
+                "depth_blur_left_mix", defaults["DEPTH_BLUR_LEFT_MIX"]
+            )
+        )
+        # --- NEW: Sidecar Control Toggle Variables ---
+        self.enable_sidecar_gamma_var = tk.BooleanVar(
+            value=self.app_config.get("enable_sidecar_gamma", True)
+        )
+        self.enable_sidecar_blur_dilate_var = tk.BooleanVar(
+            value=self.app_config.get("enable_sidecar_blur_dilate", True)
+        )
+        self.update_slider_from_sidecar_var = tk.BooleanVar(
+            value=self.app_config.get("update_slider_from_sidecar", True)
+        )
+        self.auto_save_sidecar_var = tk.BooleanVar(
+            value=self.app_config.get("auto_save_sidecar", False)
+        )
+        self.border_width_var = tk.StringVar(
+            value=self.app_config.get("border_width", defaults["BORDER_WIDTH"])
+        )
+        self.border_bias_var = tk.StringVar(
+            value=self.app_config.get("border_bias", defaults["BORDER_BIAS"])
+        )
+        self.border_mode_var = tk.StringVar(
+            value=self.app_config.get("border_mode", defaults["BORDER_MODE"])
+        )
+        self.auto_border_L_var = tk.StringVar(
+            value=self.app_config.get("auto_border_L", defaults["AUTO_BORDER_L"])
+        )
+        self.auto_border_R_var = tk.StringVar(
+            value=self.app_config.get("auto_border_R", defaults["AUTO_BORDER_R"])
+        )
+        # Add traces for automatic border calculation (Auto Basic mode)
+        self.zero_disparity_anchor_var.trace_add(
+            "write", self._on_convergence_or_disparity_changed
+        )
+        self.max_disp_var.trace_add("write", self._on_convergence_or_disparity_changed)
+        self.border_mode_var.trace_add("write", self._on_border_mode_change)
+
+        # --- NEW: Previewer Variables ---
+        self.preview_source_var = tk.StringVar(
+            value=self.app_config.get("preview_source", "Splat Result")
+        )
+        self.preview_size_var = tk.StringVar(
+            value=self.app_config.get("preview_size", "75%")
+        )
+
+        # --- Variables for "Current Processing Information" display ---
+        self.processing_filename_var = tk.StringVar(value="N/A")
+        self.processing_resolution_var = tk.StringVar(value="N/A")
+        self.processing_frames_var = tk.StringVar(value="N/A")
+        self.processing_disparity_var = tk.StringVar(value="N/A")
+        self.processing_convergence_var = tk.StringVar(value="N/A")
+        self.processing_task_name_var = tk.StringVar(value="N/A")
+        self.processing_gamma_var = tk.StringVar(value="N/A")
+        self.processing_map_var = tk.StringVar(value="N/A")
+
+        self.slider_label_updaters = []
+
+        self.widgets_to_disable = []
+
+        # --- Processing control variables ---
+        self.stop_event = threading.Event()
+        self.progress_queue = queue.Queue()
+        self.processing_thread = None
+
+        self._create_widgets()
+        self._setup_keyboard_shortcuts()
+        self.style = ttk.Style()
+
+        self.update_idletasks()  # Ensure widgets are rendered for correct reqheight
+        self._apply_theme(is_startup=True)  # Pass is_startup=True here
+        self._set_saved_geometry()  # NEW: Call to set initial geometry
+        self._is_startup = (
+            False  # Set to false after initial startup geometry is handled
+        )
+        self._configure_logging()  # Ensure this call is still present
+
+        self.after(10, self.toggle_processing_settings_fields)  # Set initial state
+        self.after(10, self._toggle_sidecar_update_button_state)
+
+        # If Multi-Map is enabled at startup (from config), we must populate the map selector UI once.
+        # (Initial BooleanVar state does not trigger the checkbox command.)
+        if self.multi_map_var.get():
+            self.after(15, self._on_multi_map_toggle)
+
+        # Apply preview-only overlay toggles (Crosshair / D/P) at startup.
+        # (Initial BooleanVar state does not trigger the checkbox command.)
+        self.after(20, self._apply_preview_overlay_toggles)
+
+        self.after(100, self.check_queue)  # Start checking progress queue
+
+        # Bind closing protocol
+        self.protocol("WM_DELETE_WINDOW", self.exit_app)
+
+        # --- NEW: Add slider release binding for preview updates ---
+        # We will add this to the sliders in _create_widgets
+        self.slider_widgets = []
+
+    def _adjust_window_height_for_content(self):
+        """Adjusts the window height to fit the current content, preserving user-set width."""
+        if self._is_startup:  # Don't adjust during initial setup
+            return
+
+        current_actual_width = self.winfo_width()
+        if current_actual_width <= 1:  # Fallback for very first call
+            current_actual_width = self.window_width
+
+        # --- NEW: More accurate height calculation ---
+        # --- FIX: Calculate base_height by summing widgets *other* than the previewer ---
+        # This is more stable than subtracting a potentially out-of-sync canvas height.
+        base_height = 0
+        for widget in self.winfo_children():
+            if widget is not self.previewer:
+                # --- FIX: Correctly handle tuple and int for pady ---
+                try:
+                    pady_value = widget.pack_info().get("pady", 0)
+                    total_pady = 0
+                    if isinstance(pady_value, int):
+                        total_pady = pady_value * 2
+                    elif isinstance(pady_value, (tuple, list)):
+                        total_pady = sum(pady_value)
+                    base_height += widget.winfo_reqheight() + total_pady
+                except tk.TclError:
+                    # This widget (e.g., the menubar) is not packed, so it has no pady.
+                    base_height += widget.winfo_reqheight()
+        # --- END FIX ---
+
+        # Get the actual height of the displayed preview image, if it exists
+        preview_image_height = 0
+        if (
+            hasattr(self.previewer, "preview_image_tk")
+            and self.previewer.preview_image_tk
+        ):
+            preview_image_height = self.previewer.preview_image_tk.height()
+
+        # Add a small buffer for padding/borders
+        padding = 10
+
+        # The new total height is the base UI height + the actual image height + padding
+        new_height = base_height + preview_image_height + padding
+        # --- END NEW ---
+
+        self.geometry(f"{current_actual_width}x{new_height}")
+        logger.debug(
+            f"Content resize applied geometry: {current_actual_width}x{new_height}"
+        )
+        self.window_width = current_actual_width  # Update stored width
+
+    def _apply_theme(self, is_startup: bool = False):
+        """Applies the selected theme (dark or light) to the GUI."""
+        # 1. Define color palettes
+        dark_colors = {
+            "bg": "#2b2b2b",
+            "fg": "white",
+            "entry_bg": "#3c3c3c",
+            "menu_bg": "#3c3c3c",
+            "menu_fg": "white",
+            "active_bg": "#555555",
+            "active_fg": "white",
+            "theme": "black",
+        }
+        light_colors = {
+            "bg": "#d9d9d9",
+            "fg": "black",
+            "entry_bg": "#ffffff",
+            "menu_bg": "#f0f0f0",
+            "menu_fg": "black",
+            "active_bg": "#dddddd",
+            "active_fg": "black",
+            "theme": "default",
+        }
+
+        # 2. Select the current palette and theme
+        if self.dark_mode_var.get():
+            colors = dark_colors
+        else:
+            colors = light_colors
+
+        self.style.theme_use(colors["theme"])
+        self.configure(bg=colors["bg"])
+
+        # 3. Apply styles to ttk widgets
+        self.style.configure("TFrame", background=colors["bg"], foreground=colors["fg"])
+        self.style.configure(
+            "TLabelframe", background=colors["bg"], foreground=colors["fg"]
+        )
+        self.style.configure(
+            "TLabelframe.Label", background=colors["bg"], foreground=colors["fg"]
+        )
+        self.style.configure("TLabel", background=colors["bg"], foreground=colors["fg"])
+        self.style.configure(
+            "TCheckbutton", background=colors["bg"], foreground=colors["fg"]
+        )
+        self.style.map(
+            "TCheckbutton",
+            foreground=[("active", colors["fg"])],
+            background=[("active", colors["bg"])],
+        )
+
+        # 4. Configure Entry and Combobox widgets using style.map for robust background override
+        self.style.map(
+            "TEntry",
+            fieldbackground=[("", colors["entry_bg"])],
+            foreground=[("", colors["fg"])],
+        )
+        self.style.configure("TEntry", insertcolor=colors["fg"])
+        self.style.map(
+            "TCombobox",
+            fieldbackground=[("readonly", colors["entry_bg"])],
+            foreground=[("readonly", colors["fg"])],
+            selectbackground=[("readonly", colors["entry_bg"])],
+            selectforeground=[("readonly", colors["fg"])],
+        )
+
+        # Manually set the background for the previewer's canvas widget
+        if hasattr(self, "previewer") and hasattr(self.previewer, "preview_canvas"):
+            self.previewer.preview_canvas.config(bg=colors["bg"], highlightthickness=0)
+
+        # 5. Manually configure non-ttk widgets (Menu, tk.Label)
+        if hasattr(self, "menubar"):
+            for menu in [self.menubar, self.file_menu, self.help_menu]:
+                menu.config(
+                    bg=colors["menu_bg"],
+                    fg=colors["menu_fg"],
+                    activebackground=colors["active_bg"],
+                    activeforeground=colors["active_fg"],
+                )
+        if hasattr(self, "info_frame"):
+            for label in self.info_labels:
+                label.config(bg=colors["bg"], fg=colors["fg"])
+
+        # 6. Handle window geometry adjustment (only after startup)
+        self.update_idletasks()  # Ensure all theme changes are rendered for accurate reqheight
+
+        # --- Apply geometry only if not during startup (NEW conditional block) ---
+        # if not is_startup:
+        #     self._adjust_window_height_for_content()
+
+    def _auto_converge_worker(
+        self, rgb_path, depth_map_path, process_length, batch_size, fallback_value, gamma, mode
+    ):
+        """Worker thread for running the Auto-Convergence calculation."""
+
+        # Run the NEW auto-convergence logic using Neural Estimator
+        new_anchor_avg, new_anchor_peak = self._determine_auto_convergence(
+            rgb_path,
+            depth_map_path,
+            process_length,
+            batch_size,
+            fallback_value,
+            gamma=gamma,
+        )
+
+        # Use self.after to safely update the GUI from the worker thread
+        self.after(
+            0,
+            lambda: self._complete_auto_converge_update(
+                new_anchor_avg,
+                new_anchor_peak,
+                fallback_value,
+                mode,  # Still pass the current mode to know which value to select immediately
+            ),
+        )
+
+    def _determine_auto_convergence(
+        self,
+        rgb_path: str,
+        depth_map_path: str,
+        process_length: int,
+        batch_size: int,
+        fallback_value: float = 0.5,
+        gamma: float = 1.0,
+    ) -> Tuple[float, float]:
+        """
+        Uses the ConvergenceEstimator (U2NETP) to recommend a zero-parallax plane.
+        Returns: (average_convergence, peak_convergence)
+        """
+        if hasattr(self, "stop_event") and not self.stop_event:
+            self.stop_event = threading.Event()
+        
+        # Initialize Estimator
+        try:
+            # Lazy import to avoid circular dependency issues
+            from dependency.convergence_estimator import ConvergenceEstimator
+            estimator = ConvergenceEstimator()
+            if estimator.model is None:
+                 logger.error("ConvergenceEstimator model failed to load. Using fallback.")
+                 return fallback_value, fallback_value
+        except ImportError as e:
+            logger.error(f"Could not import ConvergenceEstimator: {e}")
+            return fallback_value, fallback_value
+            
+        try:
+            # Initialize Readers
+            # Use CPU context for safety and simplicity in this thread
+            vr_rgb = VideoReader(rgb_path, ctx=cpu(0))
+            vr_depth = VideoReader(depth_map_path, ctx=cpu(0))
+            
+            len_rgb = len(vr_rgb)
+            len_depth = len(vr_depth)
+            
+            # Sanity check
+            if len_rgb == 0 or len_depth == 0:
+                logger.warning("Empty video or depth map found.")
+                return fallback_value, fallback_value
+                
+            total_frames = min(len_rgb, len_depth)
+            
+            # Respect process_length if set > 0
+            if process_length > 0:
+                total_frames = min(total_frames, process_length)
+
+            # Sample frames (e.g., every 6 frames, 4 times per second at 24fps)
+            # This provides better coverage than a fixed small number
+            sample_stride = 6
+            indices = list(range(0, total_frames, sample_stride))
+            
+            # Ensure at least one frame is sampled
+            if not indices:
+                indices = [0]
+            
+            estimates = []
+            
+            logger.info(f"Auto-Converge: Sampling {len(indices)} frames from {os.path.basename(rgb_path)}...")
+
+            for idx in indices:
+                if self.stop_event.is_set():
+                    logger.info("Auto-Converge scan cancelled.")
+                    break
+                    
+                # Read RGB
+                rgb_frame = vr_rgb[idx].asnumpy() # H, W, 3 (uint8)
+                # Read Depth
+                depth_frame = vr_depth[idx].asnumpy() # H, W, C or H, W
+                
+                # Preprocess for Torch
+                # RGB: 0-255 -> 0-1, Permute to C, H, W
+                # ToTensor handles /255 but here we do manual for clarity/control over numpy
+                rgb_t = torch.from_numpy(rgb_frame).float().permute(2, 0, 1) / 255.0
+                
+                # Depth: Handle various formats (Gray8, Gray16, RGB-encoding)
+                if depth_frame.ndim == 3:
+                     # If it looks like RGB depth (grayscale repeated), take mean
+                     depth_mono = depth_frame.mean(axis=2)
+                else:
+                     depth_mono = depth_frame
+                
+                depth_t = torch.from_numpy(depth_mono).float()
+                # Normalize if not 0-1
+                if depth_t.max() > 1.0:
+                    depth_t = depth_t / 255.0
+                
+
+                # Clamp and apply the same gamma curve used by the render path:
+                # 1 - (1 - depth) ** gamma
+                try:
+                    gamma_f = float(gamma)
+                except Exception:
+                    gamma_f = 1.0
+                depth_t = torch.clamp(depth_t, 0.0, 1.0)
+                if gamma_f != 1.0:
+                    depth_t = 1.0 - torch.pow((1.0 - depth_t), gamma_f)
+
+                # Format: 1, C, H, W
+                depth_t = depth_t.unsqueeze(0).unsqueeze(0) 
+                rgb_b = rgb_t.unsqueeze(0) 
+                
+                # Predict
+                res = estimator.predict(rgb_b, depth_t)
+                estimates.extend(res)
+                
+            if not estimates:
+                return fallback_value, fallback_value
+                
+            avg_val = sum(estimates) / len(estimates)
+            # Using Max as 'Peak' estimate
+            peak_val = max(estimates)
+            
+            logger.info(f"Auto-Converge Result: Avg={avg_val:.3f}, Peak={peak_val:.3f}")
+            return avg_val, peak_val
+
+        except Exception as e:
+            logger.error(f"Auto convergence determination failed: {e}", exc_info=True)
+            return fallback_value, fallback_value
+
+    def _auto_save_current_sidecar(self):
+        """
+        Saves the current GUI values to the sidecar file without user interaction.
+        Only runs if self.auto_save_sidecar_var is True.
+        """
+        if not self.auto_save_sidecar_var.get():
+            return
+
+        self._save_current_sidecar_data(is_auto_save=True)
+
+    def _browse_folder(self, var):
+        """Opens a folder dialog and updates a StringVar."""
+        current_path = var.get()
+        if os.path.isdir(current_path):
+            initial_dir = current_path
+        elif os.path.exists(current_path):
+            initial_dir = os.path.dirname(current_path)
+        else:
+            initial_dir = None
+
+        folder = filedialog.askdirectory(initialdir=initial_dir)
+        if folder:
+            var.set(folder)
+
+    def _browse_file(self, var, filetypes_list):
+        """Opens a file dialog and updates a StringVar."""
+        current_path = var.get()
+        if os.path.exists(current_path):
+            initial_dir = (
+                os.path.dirname(current_path)
+                if os.path.isfile(current_path)
+                else current_path
+            )
+        else:
+            initial_dir = None
+
+        file_path = filedialog.askopenfilename(
+            initialdir=initial_dir, filetypes=filetypes_list
+        )
+        if file_path:
+            var.set(file_path)
+
+    def _safe_float(self, var, default=0.0):
+        """Safely convert StringVar/BooleanVar to float."""
+        try:
+            val = var.get()
+            if isinstance(val, bool):
+                return float(val)
+            return float(val)
+        except (ValueError, TypeError, tk.TclError):
+            return default
+
+    def _compute_clip_global_depth_stats(
+        self, depth_map_path: str, chunk_size: int = 100
+    ) -> Tuple[float, float]:
+        """
+        [NEW HELPER] Computes the global min and max depth values from a depth video
+        by reading it in chunks. Used only for the preview's GN cache.
+        """
+        logger.info(
+            f"==> Starting clip-local depth stats pre-pass for {os.path.basename(depth_map_path)}..."
+        )
+        global_min, global_max = np.inf, -np.inf
+
+        try:
+            temp_reader = VideoReader(depth_map_path, ctx=cpu(0))
+            total_frames = len(temp_reader)
+
+            if total_frames == 0:
+                logger.error("Depth reader found 0 frames for global stats.")
+                return 0.0, 1.0  # Fallback
+
+            for i in range(0, total_frames, chunk_size):
+                if self.stop_event.is_set():
+                    logger.warning("Global stats scan stopped by user.")
+                    return 0.0, 1.0
+
+                current_indices = list(range(i, min(i + chunk_size, total_frames)))
+                chunk_numpy_raw = temp_reader.get_batch(current_indices).asnumpy()
+
+                # Handle RGB vs Grayscale depth maps
+                if chunk_numpy_raw.ndim == 4:
+                    if chunk_numpy_raw.shape[-1] == 3:  # RGB
+                        chunk_numpy = chunk_numpy_raw.mean(axis=-1)
+                    else:  # Grayscale with channel dim
+                        chunk_numpy = chunk_numpy_raw.squeeze(-1)
+                else:
+                    chunk_numpy = chunk_numpy_raw
+
+                chunk_min = chunk_numpy.min()
+                chunk_max = chunk_numpy.max()
+
+                if chunk_min < global_min:
+                    global_min = chunk_min
+                if chunk_max > global_max:
+                    global_max = chunk_max
+
+                # Skip progress bar for speed, use console log if needed
+
+            logger.info(
+                f"==> Clip-local depth stats computed: min_raw={global_min:.3f}, max_raw={global_max:.3f}"
+            )
+
+            # Cache the result before returning
+            self._clip_norm_cache[depth_map_path] = (
+                float(global_min),
+                float(global_max),
+            )
+
+            return float(global_min), float(global_max)
+
+        except Exception as e:
+            logger.error(f"Error during clip-local depth stats scan for preview: {e}")
+            return 0.0, 1.0  # Fallback
+        finally:
+            gc.collect()
+
+    def _on_multi_map_toggle(self):
+        """Called when Multi-Map checkbox is toggled."""
+        if self.multi_map_var.get():
+            # Multi-Map enabled - scan for subfolders
+            self._scan_depth_map_folders()
+        else:
+            # Multi-Map disabled - clear radio buttons
+            self._clear_depth_map_radio_buttons()
+            self.selected_depth_map_var.set("")
+
+    def _on_depth_map_folder_changed(self):
+        """Called when the Input Depth Maps folder path changes."""
+        if self.multi_map_var.get():
+            # Re-scan if Multi-Map is enabled
+            self._scan_depth_map_folders()
+
+    def _on_convergence_or_disparity_changed(self, *args):
+        """Web of traces: Update border width when convergence or disparity changes, if in Auto Basic mode."""
+        if self.border_mode_var.get() != "Auto Basic":
+            return
+
+        try:
+            # width = (1.0 - convergence) * 2.0 * (max_disp / 20.0)
+            c_val = self.zero_disparity_anchor_var.get()
+            d_val = self.max_disp_var.get()
+            if not c_val or not d_val:
+                return
+            c = float(c_val)
+            d = float(d_val)
+
+
+            # TV-range 10-bit depth maps preserve the 64‚Äì940 code window; compensate so set_disparity feels the same as full-range.
+            tv_disp_comp = 1.0
+            try:
+                if getattr(self, "previewer", None) is not None:
+                    _bd = int(getattr(self.previewer, "_depth_bit_depth", 8) or 8)
+                    _dpath = getattr(self.previewer, "_depth_path", None)
+                    if _bd > 8 and _dpath:
+                        if not hasattr(self, "_depth_color_range_cache"):
+                            self._depth_color_range_cache = {}
+                        if _dpath not in self._depth_color_range_cache:
+                            _info = get_video_stream_info(_dpath)
+                            self._depth_color_range_cache[_dpath] = str((_info or {}).get("color_range", "unknown")).lower()
+                        if self._depth_color_range_cache.get(_dpath) == "tv":
+                            tv_disp_comp = 1.0 / (DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM)
+            except Exception:
+                tv_disp_comp = 1.0
+
+            width = max(0.0, (1.0 - c) * 2.0 * (d / 20.0) * tv_disp_comp)
+            width = min(5.0, width)
+
+            self.border_width_var.set(f"{width:.2f}")
+            self.border_bias_var.set("0.0")
+
+            if (
+                hasattr(self, "set_border_width_programmatically")
+                and self.set_border_width_programmatically
+            ):
+                self.set_border_width_programmatically(width)
+            if (
+                hasattr(self, "set_border_bias_programmatically")
+                and self.set_border_bias_programmatically
+            ):
+                self.set_border_bias_programmatically(0.0)
+        except (ValueError, TypeError):
+            pass
+
+    def _sync_sliders_to_auto_borders(self, l_val=None, r_val=None):
+        """Updates Manual Width/Bias sliders to match the given or current Auto Border values."""
+        if l_val is None:
+            l_val = self._safe_float(self.auto_border_L_var)
+        if r_val is None:
+            r_val = self._safe_float(self.auto_border_R_var)
+
+        w = max(l_val, r_val)
+        if w > 0:
+            if l_val >= r_val:
+                b = (r_val / l_val) - 1.0
+            else:
+                b = 1.0 - (l_val / r_val)
+        else:
+            b = 0.0
+
+        self.border_width_var.set(f"{w:.2f}")
+        self.border_bias_var.set(f"{b:.2f}")
+        if hasattr(self, "set_border_width_programmatically"):
+            self.set_border_width_programmatically(w)
+        if hasattr(self, "set_border_bias_programmatically"):
+            self.set_border_bias_programmatically(b)
+
+    def _on_border_mode_change(self, *args):
+        """Called when the 'Border' mode pulldown is changed."""
+        mode = self.border_mode_var.get()
+        state = "normal" if mode == "Manual" else "disabled"
+
+        # Enable/Disable sliders
+        if hasattr(self, "border_sliders_row_frame"):
+            for subframe in self.border_sliders_row_frame.winfo_children():
+                for child in subframe.winfo_children():
+                    if isinstance(child, (tk.Scale, ttk.Scale, ttk.Entry, ttk.Label)):
+                        try:
+                            # Labels don't have state, but some themes allow it or we skip
+                            if hasattr(child, "configure"):
+                                child.configure(state=state)
+                        except Exception:
+                            pass
+
+        if mode == "Auto Basic":
+            self._on_convergence_or_disparity_changed()
+        elif mode == "Auto Adv.":
+            # If we have a clip, check if we already have scan data
+            result = self._get_current_sidecar_paths_and_data()
+            if result:
+                json_sidecar_path, _, data = result
+                l_val = data.get("auto_border_L")
+                r_val = data.get("auto_border_R")
+
+                if l_val is not None and r_val is not None:
+                    # We have data (even if 0.0), just load it
+                    self.auto_border_L_var.set(str(l_val))
+                    self.auto_border_R_var.set(str(r_val))
+                    self._sync_sliders_to_auto_borders(l_val, r_val)
+                else:
+                    # No data, perform scan
+                    scan_result = self._scan_borders_for_current_clip()
+                    if scan_result:
+                        l_val, r_val = scan_result
+                        self._sync_sliders_to_auto_borders(l_val, r_val)
+                        self._save_current_sidecar_data(is_auto_save=True)
+            else:
+                scan_result = self._scan_borders_for_current_clip()
+                if scan_result:
+                    l_val, r_val = scan_result
+                    self._sync_sliders_to_auto_borders(l_val, r_val)
+                    self._save_current_sidecar_data(is_auto_save=True)
+        elif mode == "Off":
+            self.border_width_var.set("0.0")
+            self.border_bias_var.set("0.0")
+            if hasattr(self, "set_border_width_programmatically"):
+                self.set_border_width_programmatically(0.0)
+            if hasattr(self, "set_border_bias_programmatically"):
+                self.set_border_bias_programmatically(0.0)
+
+        # Trigger preview update
+        if getattr(self, "previewer", None):
+            self.previewer.update_preview()
+
+    def _on_border_rescan_click(self):
+        """Handler for the Rescan button."""
+        mode = self.border_mode_var.get()
+
+        # 1. Perform scan
+        scan_result = self._scan_borders_for_current_clip(force=True)
+        newL, newR = (0.0, 0.0)
+        if scan_result:
+            newL, newR = scan_result
+
+        # 2. Implement state transition logic
+        if mode == "Off":
+            # Switch to Manual and set sliders to match scan
+            self._sync_sliders_to_auto_borders(newL, newR)
+            self.border_mode_var.set("Manual")
+
+        elif mode == "Auto Basic":
+            # Switch to Auto Adv.
+            self.border_mode_var.set("Auto Adv.")
+            self._sync_sliders_to_auto_borders(newL, newR)
+        else:
+            # We are already in Auto Adv. (or Manual), still update sliders to reflect fresh scan
+            self._sync_sliders_to_auto_borders(newL, newR)
+
+        # Force a sidecar save with new auto values IMMEDIATELY
+        # Pass values explicitly to ensure they are saved even if mode isn't Auto Adv yet
+        self._save_current_sidecar_data(
+            is_auto_save=True, force_auto_L=newL, force_auto_R=newR
+        )
+
+        if getattr(self, "previewer", None):
+            self.previewer.update_preview()
+
+    def _scan_borders_for_current_clip(self, force=False):
+        """Advanced border scanning: Samples edges of depth map."""
+        result = self._get_current_sidecar_paths_and_data()
+        if not result:
+            return
+
+        json_sidecar_path, depth_path, _ = result
+        if not depth_path or not os.path.exists(depth_path):
+            return
+
+        try:
+            vr = VideoReader(depth_path, ctx=cpu(0))
+            total_frames = len(vr)
+            if total_frames == 0:
+                return
+
+            # Show scanning status
+            old_status = self.status_label.cget("text")
+            self.status_label.config(
+                text=f"Scanning borders for {os.path.basename(depth_path)}..."
+            )
+            self.update_idletasks()
+
+            step = 5
+            max_L = 0.0
+            max_R = 0.0
+
+            conv = self._safe_float(self.zero_disparity_anchor_var, 0.5)
+            max_disp = self._safe_float(self.max_disp_var, 20.0)
+
+            gamma = self._safe_float(self.depth_gamma_var, 1.0)
+
+            tv_disp_comp = 1.0
+            try:
+                _info = get_video_stream_info(depth_path)
+                if _infer_depth_bit_depth(_info) > 8 and str((_info or {}).get("color_range", "unknown")).lower() == "tv":
+                    tv_disp_comp = 1.0 / (DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM)
+            except Exception:
+                tv_disp_comp = 1.0
+
+
+            for i in range(0, total_frames, step):
+                if self.stop_event.is_set():
+                    break
+
+                frame_raw = vr[i].asnumpy()
+                if frame_raw.ndim == 3:
+                    # Simple average for grayscale
+                    frame = frame_raw.mean(axis=2)
+                else:
+                    frame = frame_raw
+
+                # Sample 5px wide at each edge
+                L_sample = frame[:, :5]
+                R_sample = frame[:, -5:]
+
+                # 99th percentile to ignore noise
+                d_L = np.percentile(L_sample, 99) / 255.0
+                d_R = np.percentile(R_sample, 99) / 255.0
+
+                # Apply the same gamma curve used by the render path
+                if gamma and float(gamma) != 1.0:
+                    d_L = float(np.clip(d_L, 0.0, 1.0))
+                    d_R = float(np.clip(d_R, 0.0, 1.0))
+                    d_L = 1.0 - (1.0 - d_L) ** float(gamma)
+                    d_R = 1.0 - (1.0 - d_R) ** float(gamma)
+
+                # Scaling matches Auto Basic but localized to depth
+                b_L = max(0.0, (d_L - conv) * 2.0 * (max_disp / 20.0) * tv_disp_comp)
+                b_R = max(0.0, (d_R - conv) * 2.0 * (max_disp / 20.0) * tv_disp_comp)
+
+                max_L = max(max_L, b_L)
+                max_R = max(max_R, b_R)
+
+            max_L = min(5.0, round(float(max_L), 3))
+            max_R = min(5.0, round(float(max_R), 3))
+
+            self.auto_border_L_var.set(str(max_L))
+            self.auto_border_R_var.set(str(max_R))
+
+            logger.info(
+                f"Border scan complete: L={max_L}, R={max_R} (Conv={conv:.2f}, Disp={max_disp:.1f})"
+            )
+
+            self.status_label.config(text=f"Scan complete: L={max_L}%, R={max_R}%")
+            # Wait a bit so user can see the result before status might be clobbered
+            self.after(2000, lambda: self.status_label.config(text=old_status))
+
+            return max_L, max_R
+
+        except Exception as e:
+            logger.error(f"Border scan failed: {e}")
+            self.status_label.config(text="Border scan failed.")
+            return None
+
+
+    def _scan_borders_for_depth_path(
+        self,
+        depth_map_path: str,
+        conv: float,
+        max_disp: float,
+        gamma: float = 1.0,
+    ) -> Optional[Tuple[float, float]]:
+        """Thread-safe helper for AUTO-PASS: scans a depth-map video and returns (L, R) border %."""
+        try:
+            vr_depth = VideoReader(depth_map_path, ctx=cpu(0))
+            total_frames = len(vr_depth)
+            if total_frames <= 0:
+                return None
+
+            step = 5
+            max_L = 0.0
+            max_R = 0.0
+
+            gamma_f = 1.0
+            try:
+                gamma_f = float(gamma)
+            except Exception:
+                gamma_f = 1.0
+
+
+            tv_disp_comp = 1.0
+            try:
+                _info = get_video_stream_info(depth_map_path)
+                if _infer_depth_bit_depth(_info) > 8 and str((_info or {}).get("color_range", "unknown")).lower() == "tv":
+                    tv_disp_comp = 1.0 / (DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM)
+            except Exception:
+                tv_disp_comp = 1.0
+
+            for i in range(0, total_frames, step):
+                if hasattr(self, "stop_event") and self.stop_event and self.stop_event.is_set():
+                    break
+
+                frame_raw = vr_depth[i].asnumpy()
+                if frame_raw.ndim == 3:
+                    frame = frame_raw.mean(axis=2)
+                else:
+                    frame = frame_raw
+
+                L_sample = frame[:, :5]
+                R_sample = frame[:, -5:]
+
+                d_L = np.percentile(L_sample, 99) / 255.0
+                d_R = np.percentile(R_sample, 99) / 255.0
+
+                if gamma_f != 1.0:
+                    d_L = float(np.clip(d_L, 0.0, 1.0))
+                    d_R = float(np.clip(d_R, 0.0, 1.0))
+                    d_L = 1.0 - (1.0 - d_L) ** gamma_f
+                    d_R = 1.0 - (1.0 - d_R) ** gamma_f
+
+                b_L = max(0.0, (d_L - conv) * 2.0 * (max_disp / 20.0) * tv_disp_comp)
+                b_R = max(0.0, (d_R - conv) * 2.0 * (max_disp / 20.0) * tv_disp_comp)
+
+                max_L = max(max_L, b_L)
+                max_R = max(max_R, b_R)
+
+            max_L = min(5.0, round(float(max_L), 3))
+            max_R = min(5.0, round(float(max_R), 3))
+            return max_L, max_R
+
+        except Exception as e:
+            logger.error(f"Border scan failed: {e}")
+            return None
+
+    def _scan_depth_map_folders(self):
+        """Scans the Input Depth Maps folder for subfolders containing *_depth.mp4 files."""
+        base_folder = self.input_depth_maps_var.get()
+
+        # Clear existing radio buttons
+        self._clear_depth_map_radio_buttons()
+        self.depth_map_subfolders = []
+
+        if not os.path.isdir(base_folder):
+            return
+
+        # Find all subfolders that contain depth map files
+        try:
+            for item in sorted(os.listdir(base_folder)):
+                subfolder_path = os.path.join(base_folder, item)
+                if os.path.isdir(subfolder_path):
+                    # Check if this subfolder contains *_depth.mp4 files
+                    depth_files = glob.glob(os.path.join(subfolder_path, "*_depth.mp4"))
+                    if depth_files:
+                        self.depth_map_subfolders.append(item)
+        except Exception as e:
+            logger.error(f"Error scanning depth map subfolders: {e}")
+            return
+
+        if self.depth_map_subfolders:
+            # Select the first one by default (alphabetically first)
+            self.selected_depth_map_var.set(self.depth_map_subfolders[0])
+            # Create radio buttons in the previewer
+            self._create_depth_map_radio_buttons()
+            # Trigger preview update
+            self.on_slider_release(None)
+        else:
+            logger.warning("No valid depth map subfolders found")
+            self.selected_depth_map_var.set("")
+
+    def _update_map_selector_highlight(self):
+        """Visually emphasize the currently selected map radio button (no layout change)."""
+        try:
+            sel = (self.selected_depth_map_var.get() or "").strip()
+            # Only applies when radio buttons exist
+            rb_dict = getattr(self, "depth_map_radio_dict", {}) or {}
+            for name, rb in rb_dict.items():
+                if not rb:
+                    continue
+                try:
+                    rb.configure(
+                        style="MapSelSelected.TRadiobutton"
+                        if name == sel
+                        else "MapSel.TRadiobutton"
+                    )
+                except Exception:
+                    pass
+        except Exception:
+            pass
+
+    def _clear_depth_map_radio_buttons(self):
+        """Removes all depth map radio buttons from the GUI."""
+        for widget in self.depth_map_radio_buttons:
+            widget.destroy()
+        self.depth_map_radio_buttons = []
+
+    def _create_depth_map_radio_buttons(self):
+        """Creates radio buttons for each valid depth map subfolder."""
+        logger.info(
+            f"Creating radio buttons, current selected_depth_map_var = {self.selected_depth_map_var.get()}"
+        )
+        self._clear_depth_map_radio_buttons()
+
+        if not hasattr(self, "previewer") or self.previewer is None:
+            return
+
+        # Get the preview button frame from the previewer
+        # The radio buttons should be added to the same frame as preview_size_combo
+        preview_button_frame = self.previewer.preview_size_combo.master
+
+        # --- Map selector styles (selected vs unselected) ---
+        if not getattr(self, "_map_selector_styles_inited", False):
+            try:
+                style = ttk.Style()
+                base_font = tkfont.nametofont("TkDefaultFont")
+                bold_font = tkfont.Font(
+                    root=preview_button_frame,
+                    family=base_font.cget("family"),
+                    size=base_font.cget("size"),
+                    weight="bold",
+                )
+                style.configure("MapSel.TRadiobutton", font=base_font)
+                style.configure("MapSelSelected.TRadiobutton", font=bold_font)
+            except Exception:
+                # Fail quietly; styles are optional
+                pass
+            self._map_selector_styles_inited = True
+
+        for subfolder_name in self.depth_map_subfolders:
+            rb = ttk.Radiobutton(
+                preview_button_frame,
+                text=subfolder_name,
+                variable=self.selected_depth_map_var,
+                value=subfolder_name,
+                style="MapSel.TRadiobutton",
+                command=self._on_map_selection_changed,
+            )
+            rb.pack(side="left", padx=5)
+            self.depth_map_radio_buttons.append(rb)
+            self.depth_map_radio_dict[subfolder_name] = rb
+
+        # Apply visual emphasis to the current selection
+        self._update_map_selector_highlight()
+
+    def _on_map_selection_changed(self, from_sidecar=False):
+        """
+        Called when the user changes the depth map selection (radio buttons),
+        or when a sidecar restores a map (from_sidecar=True).
+
+        In Multi-Map mode this now ONLY updates the CURRENT video‚Äôs depth map
+        path instead of iterating over every video.
+        """
+        _sel = self.selected_depth_map_var.get()
+        _last = getattr(self, "_last_map_change_log", None)
+        if _sel != _last:
+            logger.info(
+                f"Depth map selection changed -> {_sel} (from_sidecar={from_sidecar})"
+            )
+            self._last_map_change_log = _sel
+        if not from_sidecar:
+            # User clicked a radio button ‚Äì suppress sidecar overwrites
+            self._suppress_sidecar_map_update = True
+
+        # Keep UI + info panel in sync with the currently selected map (no heavy work)
+        try:
+            self._update_map_selector_highlight()
+        except Exception:
+            pass
+        try:
+            self._update_processing_info_for_preview_clip()
+        except Exception:
+            pass
+
+        # Compute the folder for the newly selected map
+        new_depth_folder = self._get_effective_depth_map_folder()
+
+        # If there is no previewer / no videos, nothing to do
+        if not hasattr(self, "previewer") or self.previewer is None:
+            return
+
+        current_index = getattr(self.previewer, "current_video_index", None)
+        if current_index is None:
+            return
+        if current_index < 0 or current_index >= len(self.previewer.video_list):
+            return
+
+        # Work only on the CURRENT video entry
+        video_entry = self.previewer.video_list[current_index]
+        source_video = video_entry.get("source_video", "")
+        if not source_video:
+            return
+
+        video_name = os.path.splitext(os.path.basename(source_video))[0]
+        depth_mp4 = os.path.join(new_depth_folder, f"{video_name}_depth.mp4")
+        depth_npz = os.path.join(new_depth_folder, f"{video_name}_depth.npz")
+
+        depth_path = None
+        if os.path.exists(depth_mp4):
+            depth_path = depth_mp4
+        elif os.path.exists(depth_npz):
+            depth_path = depth_npz
+
+        # Update the current entry only
+        video_entry["depth_map"] = depth_path
+
+        # Only log for the current video, and only if it‚Äôs missing
+        if depth_path is None:
+            logger.info(
+                f"Depth map for current video {video_name} not found in "
+                f"{os.path.basename(new_depth_folder)}"
+            )
+
+        # Refresh previewer so the current video immediately reflects the new map
+        try:
+            self.previewer.replace_source_path_for_current_video(
+                "depth_map", depth_path or ""
+            )
+        except Exception as e:
+            logger.exception(f"Error refreshing preview after map switch: {e}")
+
+        # Keep the processing queue entry (if present) in sync for this one video
+        if hasattr(self, "resolution_output_list") and 0 <= current_index < len(
+            self.resolution_output_list
+        ):
+            self.resolution_output_list[current_index].depth_map = depth_path
+
+        # Post-switch: update info panel + visual emphasis
+        try:
+            self._update_processing_info_for_preview_clip()
+        except Exception:
+            pass
+        try:
+            self._update_map_selector_highlight()
+        except Exception:
+            pass
+
+    def _get_effective_depth_map_folder(self, base_folder=None):
+        """Returns the effective depth map folder based on Multi-Map settings.
+
+        Args:
+            base_folder: Optional override for base folder (used during processing)
+
+        Returns:
+            str: The folder path to use for depth maps
+        """
+        if base_folder is None:
+            base_folder = self.input_depth_maps_var.get()
+
+        # If the user has selected a single depth MAP FILE, treat its directory as the folder.
+        if base_folder and os.path.isfile(base_folder):
+            base_folder = os.path.dirname(base_folder)
+
+        if self.multi_map_var.get() and self.selected_depth_map_var.get().strip():
+            # Multi-Map is enabled and a subfolder is selected
+            return os.path.join(base_folder, self.selected_depth_map_var.get().strip())
+        else:
+            # Normal mode - use the base folder directly
+            return base_folder
+
+    def _get_sidecar_base_folder(self):
+        """Returns the folder where sidecars should be stored.
+
+        When Multi-Map is enabled, sidecars are stored in a 'sidecars' subfolder.
+        When Multi-Map is disabled, sidecars are stored alongside depth maps.
+
+        Returns:
+            str: The folder path for sidecar storage
+        """
+        if self.multi_map_var.get():
+            # Multi-Map mode: store sidecars in 'sidecars' subfolder
+            base_folder = self.input_depth_maps_var.get()
+            sidecar_folder = os.path.join(base_folder, "sidecars")
+            # Create the sidecars folder if it doesn't exist
+            os.makedirs(sidecar_folder, exist_ok=True)
+            return sidecar_folder
+        else:
+            # Normal mode: store sidecars with depth maps
+            return self._get_effective_depth_map_folder()
+
+    def _get_sidecar_selected_map_for_video(self, video_path):
+        """
+        Returns the Multi-Map subfolder name for a given video based on its sidecar,
+        or None if there is no sidecar / no selected_depth_map entry.
+        """
+        try:
+            # Derive expected sidecar name from *video name* (matches your depth sidecars)
+            video_name = os.path.splitext(os.path.basename(video_path))[0]
+            sidecar_ext = self.APP_CONFIG_DEFAULTS.get("SIDECAR_EXT", ".fssidecar")
+
+            # In Multi-Map mode, sidecars live in <InputDepthMaps>/sidecars
+            sidecar_folder = self._get_sidecar_base_folder()
+            sidecar_path = os.path.join(
+                sidecar_folder, f"{video_name}_depth{sidecar_ext}"
+            )
+
+            if not os.path.exists(sidecar_path):
+                return None
+
+            sidecar_config = self.sidecar_manager.load_sidecar_data(sidecar_path) or {}
+            selected_map_val = sidecar_config.get("selected_depth_map", "")
+            if selected_map_val:
+                return selected_map_val
+
+        except Exception as e:
+            logger.error(f"Error reading sidecar map for {video_path}: {e}")
+
+        return None
+
+    def check_queue(self):
+        """Periodically checks the progress queue for updates to the GUI."""
+        try:
+            while True:
+                message = self.progress_queue.get_nowait()
+                if message == "finished":
+                    self.status_label.config(text="Processing finished")
+                    self.start_button.config(state="normal")
+                    self.start_single_button.config(state="normal")
+                    self.stop_button.config(state="disabled")
+                    self.progress_var.set(0)
+                    # --- NEW: Enable all inputs at finish ---
+                    self._set_input_state("normal")
+                    logger.info(f"==> All process completed.")
+                    break
+
+                elif message[0] == "total":
+                    total_tasks = message[1]
+                    self.progress_bar.config(maximum=total_tasks)
+                    self.progress_var.set(0)
+                    self.status_label.config(
+                        text=f"Processing 0 of {total_tasks} tasks"
+                    )
+                elif message[0] == "processed":
+                    processed_tasks = message[1]
+                    total_tasks = self.progress_bar["maximum"]
+                    self.progress_var.set(processed_tasks)
+                    self.status_label.config(
+                        text=f"Processed tasks: {processed_tasks}/{total_tasks} (overall)"
+                    )
+                elif message[0] == "status":
+                    self.status_label.config(
+                        text=f"Overall: {self.progress_var.get()}/{self.progress_bar['maximum']} - {message[1].split(':', 1)[-1].strip()}"
+                    )
+                elif message[0] == "update_info":
+                    info_data = message[1]
+                    if "filename" in info_data:
+                        self.processing_filename_var.set(info_data["filename"])
+                    if "resolution" in info_data:
+                        self.processing_resolution_var.set(info_data["resolution"])
+                    if "frames" in info_data:
+                        self.processing_frames_var.set(str(info_data["frames"]))
+                    if "disparity" in info_data:
+                        self.processing_disparity_var.set(info_data["disparity"])
+                    if "convergence" in info_data:
+                        self.processing_convergence_var.set(info_data["convergence"])
+                    if "gamma" in info_data:
+                        self.processing_gamma_var.set(info_data["gamma"])
+                    if "map" in info_data:
+                        self.processing_map_var.set(info_data["map"])
+                    if "task_name" in info_data:
+                        self.processing_task_name_var.set(info_data["task_name"])
+
+        except queue.Empty:
+            pass
+        self.after(100, self.check_queue)
+
+    def clear_processing_info(self):
+        """Resets all 'Current Processing Information' labels to default 'N/A'."""
+        self.processing_filename_var.set("N/A")
+        self.processing_resolution_var.set("N/A")
+        self.processing_frames_var.set("N/A")
+        self.processing_disparity_var.set("N/A")
+        self.processing_convergence_var.set("N/A")
+        self.processing_gamma_var.set("N/A")
+        self.processing_task_name_var.set("N/A")
+        self.processing_map_var.set("N/A")
+
+    def _complete_auto_converge_update(
+        self,
+        new_anchor_avg: float,
+        new_anchor_peak: float,
+        fallback_value: float,
+        mode: str,
+    ):
+        """
+        Safely updates the GUI and preview after Auto-Convergence worker is done.
+
+        Now receives both calculated values.
+        """
+        # Re-enable inputs
+        self._is_auto_conv_running = False
+        self.btn_auto_converge_preview.config(state="normal")
+        self.start_button.config(state="normal")
+        self.start_single_button.config(state="normal")
+        self.auto_convergence_combo.config(state="readonly")
+
+        if self.stop_event.is_set():
+            self.status_label.config(text="Auto-Converge pre-pass was stopped.")
+            self.stop_event.clear()
+            return
+
+        # Check if EITHER calculation yielded a result different from the fallback
+        if new_anchor_avg != fallback_value or new_anchor_peak != fallback_value:
+            # 1. Cache BOTH results
+            self._auto_conv_cache["Average"] = new_anchor_avg
+            self._auto_conv_cache["Peak"] = new_anchor_peak
+
+            # CRITICAL: Store the path of the file that was just scanned
+            current_index = self.previewer.current_video_index
+            if 0 <= current_index < len(self.previewer.video_list):
+                depth_map_path = self.previewer.video_list[current_index].get(
+                    "depth_map"
+                )
+                self._auto_conv_cached_path = depth_map_path
+
+            # 2. Determine which value to apply immediately (based on the current 'mode' selection)
+            if mode == "Average":
+                anchor_to_apply = new_anchor_avg
+            elif mode == "Peak":
+                anchor_to_apply = new_anchor_peak
+            elif mode == "Hybrid":
+                anchor_to_apply = (new_anchor_avg + new_anchor_peak) / 2.0
+            else:
+                anchor_to_apply = fallback_value
+
+            # 3. Update the Tkinter variable and refresh the slider/label
+
+            is_setter_successful = False
+            if self.set_convergence_value_programmatically:
+                try:
+                    # Pass the numeric value. The setter handles setting var and updating the label.
+                    self.set_convergence_value_programmatically(anchor_to_apply)
+                    is_setter_successful = True
+                except Exception as e:
+                    logger.error(f"Error calling convergence setter: {e}")
+
+            # Fallback if setter failed (should not happen if fixed)
+            if not is_setter_successful:
+                self.zero_disparity_anchor_var.set(f"{anchor_to_apply:.2f}")
+
+            self.status_label.config(
+                text=f"Auto-Converge: Avg Cached at {new_anchor_avg:.2f}, Peak Cached at {new_anchor_peak:.2f}. Applied: {mode} ({anchor_to_apply:.2f})"
+            )
+
+            # 4. Immediately trigger a preview update to show the change
+            self.on_slider_release(None)
+
+        else:
+            # Calculation failed (both returned fallback)
+            self.status_label.config(
+                text=f"Auto-Converge: Failed to find a valid anchor. Value remains {fallback_value:.2f}"
+            )
+            messagebox.showwarning(
+                "Auto-Converge Preview",
+                f"Failed to find a valid anchor point in any mode. No changes were made.",
+            )
+            # If it was triggered by the combo box, reset the combo box to "Off"
+            if self.auto_convergence_mode_var.get() == mode:
+                self.auto_convergence_combo.set("Off")
+
+    def _configure_logging(self):
+        """Sets the logging level for the stereocrafter_util logger based on debug_mode_var."""
+        if self.debug_mode_var.get():
+            level = logging.DEBUG
+            # Also set the root logger if it hasn't been configured to debug, to catch other messages
+            if logging.root.level > logging.DEBUG:
+                logging.root.setLevel(logging.DEBUG)
+        else:
+            level = logging.INFO
+            # Reset root logger if it was temporarily set to debug by this GUI
+            if logging.root.level == logging.DEBUG:  # Check if this GUI set it
+                logging.root.setLevel(logging.INFO)  # Reset to a less verbose default
+
+        # Make sure 'set_util_logger_level' is imported and available.
+        # It's already in dependency/stereocrafter_util, ensure it's imported at the top.
+        # Add 'import logging' at the top of splatting_gui.py if not already present.
+        set_util_logger_level(level)  # Call the function from stereocrafter_util.py
+        logger.info(f"Logging level set to {logging.getLevelName(level)}.")
+
+    def _create_hover_tooltip(self, widget, key):
+        """Creates a tooltip for a given widget based on a key from help_texts."""
+        if not key:
+            return
+        try:
+            # Try the provided key first, then a few common variants (no file writes).
+            candidates = [
+                key,
+                f"{key}_button" if not str(key).endswith("_button") else str(key).replace("_button", ""),
+                str(key).replace("btn_", ""),
+                str(key).replace("button_", ""),
+            ]
+            for k in candidates:
+                if k in self.help_texts:
+                    Tooltip(widget, self.help_texts[k])
+                    return
+        except Exception:
+            pass
+
+    def _create_widgets(self):
+        """Initializes and places all GUI widgets."""
+
+        current_row = 0
+
+        # --- Menu Bar ---
+        self.menubar = tk.Menu(self)
+        self.config(menu=self.menubar)
+
+        self.file_menu = tk.Menu(self.menubar, tearoff=0)
+        self.menubar.add_cascade(label="File", menu=self.file_menu)
+
+        # Add new commands to the File menu
+        self.file_menu.add_command(
+            label="Load Settings from File...", command=self.load_settings
+        )
+        self.file_menu.add_command(
+            label="Save Settings", command=self._save_current_settings_and_notify
+        )
+        self.file_menu.add_command(
+            label="Save Settings to File...", command=self.save_settings
+        )
+        self.file_menu.add_separator()  # Separator for organization
+
+        self.file_menu.add_command(
+            label="Load Fusion Export (.fsexport)...",
+            command=self.run_fusion_sidecar_generator,
+        )
+        self.file_menu.add_command(
+            label="FSExport to custom sidecar...",
+            command=self.run_fusion_sidecar_generator_custom,
+        )
+        self.file_menu.add_separator()
+
+        self.file_menu.add_checkbutton(
+            label="Dark Mode", variable=self.dark_mode_var, command=self._apply_theme
+        )
+        self.file_menu.add_separator()
+
+        # Update Slider from Sidecar Toggle (Existing)
+        self.file_menu.add_checkbutton(
+            label="Update Slider from Sidecar",
+            variable=self.update_slider_from_sidecar_var,
+        )
+
+        # --- Auto Save Sidecar Toggle ---
+        self.file_menu.add_checkbutton(
+            label="Auto Save Sidecar on Next", variable=self.auto_save_sidecar_var
+        )
+        self.file_menu.add_separator()
+
+        self.file_menu.add_command(
+            label="Reset to Default", command=self.reset_to_defaults
+        )
+        self.file_menu.add_command(
+            label="Restore Finished", command=self.restore_finished_files
+        )
+
+        self.help_menu = tk.Menu(self.menubar, tearoff=0)
+        self.debug_logging_var = tk.BooleanVar(value=self._debug_logging_enabled)
+        self.help_menu.add_checkbutton(
+            label="Debug Logging",
+            variable=self.debug_logging_var,
+            command=self._toggle_debug_logging,
+        )
+        self.help_menu.add_command(label="User Guide", command=self.show_user_guide)
+        self.help_menu.add_separator()
+
+        # Add "About" submenu (after "Debug Logging")
+        self.help_menu.add_command(
+            label="About Stereocrafter Splatting", command=self.show_about
+        )
+        self.menubar.add_cascade(label="Help", menu=self.help_menu)
+
+        # --- Folder selection frame ---
+        self.folder_frame = ttk.LabelFrame(self, text="Input/Output Folders")
+        self.folder_frame.pack(pady=2, padx=10, fill="x")
+        self.folder_frame.grid_columnconfigure(1, weight=1)
+
+        # Settings Container (NEW)
+        self.settings_container_frame = ttk.Frame(
+            self
+        )  # <-- ADD self. to settings_container_frame
+        self.settings_container_frame.pack(pady=2, padx=10, fill="x")
+
+        # Input Source Clips Row
+        self.lbl_source_clips = ttk.Label(self.folder_frame, text="Input Source Clips:")
+        self.lbl_source_clips.grid(
+            row=current_row, column=0, sticky="e", padx=5, pady=0
+        )
+        self.entry_source_clips = ttk.Entry(
+            self.folder_frame, textvariable=self.input_source_clips_var
+        )
+        self.entry_source_clips.grid(
+            row=current_row, column=1, padx=5, pady=0, sticky="ew"
+        )
+        self.btn_browse_source_clips_folder = ttk.Button(
+            self.folder_frame,
+            text="Browse Folder",
+            command=lambda: self._browse_folder(self.input_source_clips_var),
+        )
+        self.btn_browse_source_clips_folder.grid(
+            row=current_row, column=2, padx=2, pady=0
+        )
+        self.btn_select_source_clips_file = ttk.Button(
+            self.folder_frame,
+            text="Select File",
+            command=lambda: self._browse_file(
+                self.input_source_clips_var,
+                [("Video Files", "*.mp4 *.avi *.mov *.mkv"), ("All files", "*.*")],
+            ),
+        )
+        self.btn_select_source_clips_file.grid(
+            row=current_row, column=3, padx=2, pady=0
+        )
+        self._create_hover_tooltip(self.lbl_source_clips, "input_source_clips")
+        self._create_hover_tooltip(self.entry_source_clips, "input_source_clips")
+        self._create_hover_tooltip(
+            self.btn_browse_source_clips_folder, "input_source_clips_folder"
+        )
+        self._create_hover_tooltip(
+            self.btn_select_source_clips_file, "input_source_clips_file"
+        )
+        current_row += 1
+
+        # Input Depth Maps Row
+        self.lbl_input_depth_maps = ttk.Label(
+            self.folder_frame, text="Input Depth Maps:"
+        )
+        self.lbl_input_depth_maps.grid(
+            row=current_row, column=0, sticky="e", padx=5, pady=0
+        )
+        self.entry_input_depth_maps = ttk.Entry(
+            self.folder_frame, textvariable=self.input_depth_maps_var
+        )
+        self.entry_input_depth_maps.grid(
+            row=current_row, column=1, padx=5, pady=0, sticky="ew"
+        )
+        self.btn_browse_input_depth_maps_folder = ttk.Button(
+            self.folder_frame,
+            text="Browse Folder",
+            command=lambda: self._browse_folder(self.input_depth_maps_var),
+        )
+        self.btn_browse_input_depth_maps_folder.grid(
+            row=current_row, column=2, padx=2, pady=0
+        )
+        self.btn_select_input_depth_maps_file = ttk.Button(
+            self.folder_frame,
+            text="Select File",
+            command=lambda: self._browse_file(
+                self.input_depth_maps_var,
+                [("Depth Files", "*.mp4 *.npz"), ("All files", "*.*")],
+            ),
+        )
+        self.btn_select_input_depth_maps_file.grid(
+            row=current_row, column=3, padx=2, pady=0
+        )
+        self._create_hover_tooltip(self.lbl_input_depth_maps, "input_depth_maps")
+        self._create_hover_tooltip(self.entry_input_depth_maps, "input_depth_maps")
+        self._create_hover_tooltip(
+            self.btn_browse_input_depth_maps_folder, "input_depth_maps_folder"
+        )
+        self._create_hover_tooltip(
+            self.btn_select_input_depth_maps_file, "input_depth_maps_file"
+        )
+        current_row += 1
+
+        # Output Splatted Row
+        self.lbl_output_splatted = ttk.Label(self.folder_frame, text="Output Splatted:")
+        self.lbl_output_splatted.grid(
+            row=current_row, column=0, sticky="e", padx=5, pady=0
+        )
+        self.entry_output_splatted = ttk.Entry(
+            self.folder_frame, textvariable=self.output_splatted_var
+        )
+        self.entry_output_splatted.grid(
+            row=current_row, column=1, padx=5, pady=0, sticky="ew"
+        )
+        self.btn_browse_output_splatted = ttk.Button(
+            self.folder_frame,
+            text="Browse Folder",
+            command=lambda: self._browse_folder(self.output_splatted_var),
+        )
+        self.btn_browse_output_splatted.grid(row=current_row, column=2, padx=5, pady=0)
+        self.chk_multi_map = ttk.Checkbutton(
+            self.folder_frame,
+            text="Multi-Map",
+            variable=self.multi_map_var,
+            command=self._on_multi_map_toggle,
+        )
+        self.chk_multi_map.grid(row=current_row, column=3, padx=5, pady=0)
+        self._create_hover_tooltip(self.lbl_output_splatted, "output_splatted")
+        self._create_hover_tooltip(self.entry_output_splatted, "output_splatted")
+        self._create_hover_tooltip(self.chk_multi_map, "multi_map")
+        self._create_hover_tooltip(self.btn_browse_output_splatted, "output_splatted")
+        # Reset current_row for next frame
+        current_row = 0
+
+        # --- NEW: PREVIEW FRAME ---
+        self.previewer = VideoPreviewer(
+            self,
+            processing_callback=self._preview_processing_callback,
+            find_sources_callback=self._find_preview_sources_callback,
+            get_params_callback=self.get_current_preview_settings,
+            preview_size_var=self.preview_size_var,  # Pass the preview size variable
+            resize_callback=self._adjust_window_height_for_content,  # Pass the resize callback
+            update_clip_callback=self._update_clip_state_and_text,
+            on_clip_navigate_callback=self._auto_save_current_sidecar,
+            help_data=self.help_texts,
+        )
+        self.previewer.pack(fill="both", expand=True, padx=10, pady=1)
+        self.previewer.preview_source_combo.configure(
+            textvariable=self.preview_source_var
+        )
+
+        # Set the preview options ONCE at startup
+        self.previewer.preview_source_combo["values"] = [
+            "Splat Result",
+            "Splat Result(Low)",
+            "Occlusion Mask",
+            "Occlusion Mask(Low)",
+            "Original (Left Eye)",
+            "Depth Map",
+            "Depth Map (Color)",
+            "Anaglyph 3D",
+            "Dubois Anaglyph",
+            "Optimized Anaglyph",
+            "Wigglegram",
+        ]
+        if not self.preview_source_var.get():
+            self.preview_source_var.set("Splat Result")
+
+        # --- NEW: MAIN LAYOUT CONTAINER (Holds Settings Left and Info Right) ---
+        self.main_layout_frame = ttk.Frame(self)
+        self.main_layout_frame.pack(pady=2, padx=10, fill="x")
+        self.main_layout_frame.grid_columnconfigure(0, weight=1)  # Left settings column
+        self.main_layout_frame.grid_columnconfigure(
+            1, weight=0
+        )  # Right info column (fixed width)
+
+        # --- LEFT COLUMN: Settings Stack Frame ---
+        self.settings_stack_frame = ttk.Frame(self.main_layout_frame)
+        self.settings_stack_frame.grid(row=0, column=0, sticky="nsew", padx=(0, 10))
+
+        # --- Settings Container Frame (to hold two side-by-side frames) ---
+        self.settings_container_frame = ttk.Frame(self.settings_stack_frame)
+        self.settings_container_frame.pack(
+            pady=(0, 2), fill="x"
+        )  # Pack it inside the stack frame
+        # Spacer columns on BOTH SIDES of the depth/sliders column:
+        #   - col 0: left/process settings (fixed)
+        #   - col 1: spacer (expands)
+        #   - col 2: depth/sliders (expands more than spacers)
+        #   - col 3: spacer (expands)
+        #
+        # This keeps the slider column visually centered within the available space
+        # (while the process/settings column stays pinned left).
+        self.settings_container_frame.grid_columnconfigure(
+            0, weight=0, minsize=int(self.UI_PROCESS_COL_MIN)
+        )
+        self.settings_container_frame.grid_columnconfigure(1, weight=1, minsize=0)
+        self.settings_container_frame.grid_columnconfigure(
+            2, weight=2, minsize=int(self.UI_DEPTH_COL_MIN)
+        )
+        self.settings_container_frame.grid_columnconfigure(3, weight=1, minsize=0)
+
+        self.settings_container_spacer_left = ttk.Frame(self.settings_container_frame)
+        self.settings_container_spacer_left.grid(row=0, column=1, sticky="nsew")
+
+        self.settings_container_spacer_right = ttk.Frame(self.settings_container_frame)
+        self.settings_container_spacer_right.grid(row=0, column=3, sticky="nsew")
+
+        # ===================================================================
+        # LEFT SIDE: Process Resolution and Settings Frame
+        # ===================================================================
+
+        # This container holds both the resolution settings (top) and the splatting/output settings (bottom)
+        self.process_settings_container = ttk.Frame(self.settings_container_frame)
+        self.process_settings_container.grid(
+            row=0, column=0, padx=(5, 0), sticky="nsew"
+        )
+        self.process_settings_container.grid_columnconfigure(0, weight=1)
+
+        # --- 1. Process Resolution Frame (Top Left) ---
+        self.preprocessing_frame = ttk.LabelFrame(
+            self.process_settings_container, text="Process Resolution"
+        )
+        self.preprocessing_frame.grid(
+            row=0, column=0, padx=(0, 5), sticky="nsew"
+        )  # <-- Grid 0,0 in process_settings_container
+        self.preprocessing_frame.grid_columnconfigure(
+            1, weight=0
+        )  # Allow Entry to expand
+
+        current_row = 0
+
+        # --- Enable Full/Low Resolution + Tests (Compact 3x3 Grid) ---
+        # Layout:
+        #   Row 0: [Enable Full Res]  [Batch Size]  [Dual Output Only]
+        #   Row 1: [Enable Low Res ]  [Batch Size]  [Splat Test]
+        #   Row 2: [Width]           [Height]      [Map Test]
+        # Keep vertical padding minimal so preview area can expand.
+
+        # Ensure 3 primary columns exist
+        self.preprocessing_frame.grid_columnconfigure(0, weight=0)
+        self.preprocessing_frame.grid_columnconfigure(1, weight=0)
+        self.preprocessing_frame.grid_columnconfigure(2, weight=0)
+
+        # --- Row 0: Full res ---
+        self.enable_full_res_checkbox = ttk.Checkbutton(
+            self.preprocessing_frame,
+            text="Enable Full Res",
+            variable=self.enable_full_res_var,
+            command=self.toggle_processing_settings_fields,
+            width=15,
+        )
+        self.enable_full_res_checkbox.grid(row=0, column=0, sticky="w", padx=5, pady=0)
+        self._create_hover_tooltip(self.enable_full_res_checkbox, "enable_full_res")
+
+        self.full_batch_frame = ttk.Frame(self.preprocessing_frame)
+        self.full_batch_frame.grid(row=0, column=1, sticky="w", padx=5, pady=0)
+        self.lbl_full_res_batch_size = ttk.Label(
+            self.full_batch_frame, text="Batch Size:"
+        )
+        self.lbl_full_res_batch_size.pack(side="left", padx=(0, 2))
+        self.entry_full_res_batch_size = ttk.Entry(
+            self.full_batch_frame, textvariable=self.batch_size_var, width=5
+        )
+        self.entry_full_res_batch_size.pack(side="left")
+        self._create_hover_tooltip(self.lbl_full_res_batch_size, "full_res_batch_size")
+        self._create_hover_tooltip(
+            self.entry_full_res_batch_size, "full_res_batch_size"
+        )
+
+        self.dual_output_checkbox = ttk.Checkbutton(
+            self.preprocessing_frame,
+            text="Dual Output Only",
+            variable=self.dual_output_var,
+        )
+        self.dual_output_checkbox.grid(row=0, column=2, sticky="w", padx=5, pady=0)
+        self._create_hover_tooltip(self.dual_output_checkbox, "dual_output")
+
+        # --- Row 1: Low res ---
+        self.enable_low_res_checkbox = ttk.Checkbutton(
+            self.preprocessing_frame,
+            text="Enable Low Res",
+            variable=self.enable_low_res_var,
+            command=self.toggle_processing_settings_fields,
+            width=15,
+        )
+        self.enable_low_res_checkbox.grid(row=1, column=0, sticky="w", padx=5, pady=0)
+        self._create_hover_tooltip(self.enable_low_res_checkbox, "enable_low_res")
+
+        self.low_batch_frame = ttk.Frame(self.preprocessing_frame)
+        self.low_batch_frame.grid(row=1, column=1, sticky="w", padx=5, pady=0)
+        self.lbl_low_res_batch_size = ttk.Label(
+            self.low_batch_frame, text="Batch Size:"
+        )
+        self.lbl_low_res_batch_size.pack(side="left", padx=(0, 2))
+        self.entry_low_res_batch_size = ttk.Entry(
+            self.low_batch_frame, textvariable=self.low_res_batch_size_var, width=5
+        )
+        self.entry_low_res_batch_size.pack(side="left")
+        self._create_hover_tooltip(self.lbl_low_res_batch_size, "low_res_batch_size")
+        self._create_hover_tooltip(self.entry_low_res_batch_size, "low_res_batch_size")
+
+        # Tests (mutually exclusive)
+        self.splat_test_var = tk.BooleanVar(value=False)
+        self.map_test_var = tk.BooleanVar(value=False)
+
+        # --- Row 2: Width/Height + Map Test ---
+        self.width_frame = ttk.Frame(self.preprocessing_frame)
+        self.width_frame.grid(row=2, column=0, sticky="w", padx=5, pady=0)
+        self.pre_res_width_label = ttk.Label(self.width_frame, text="Width:")
+        self.pre_res_width_label.pack(side="left", padx=(0, 2))
+        self.pre_res_width_entry = ttk.Entry(
+            self.width_frame, textvariable=self.pre_res_width_var, width=8
+        )
+        self.pre_res_width_entry.pack(side="left")
+        self._create_hover_tooltip(self.pre_res_width_label, "low_res_width")
+        self._create_hover_tooltip(self.pre_res_width_entry, "low_res_width")
+
+        self.height_frame = ttk.Frame(self.preprocessing_frame)
+        self.height_frame.grid(row=2, column=1, sticky="w", padx=5, pady=0)
+        self.pre_res_height_label = ttk.Label(self.height_frame, text="Height:")
+        self.pre_res_height_label.pack(side="left", padx=(0, 2))
+        self.pre_res_height_entry = ttk.Entry(
+            self.height_frame, textvariable=self.pre_res_height_var, width=8
+        )
+        self.pre_res_height_entry.pack(side="left")
+        self._create_hover_tooltip(self.pre_res_height_label, "low_res_height")
+        self._create_hover_tooltip(self.pre_res_height_entry, "low_res_height")
+        # --- 2. Splatting & Output Settings Frame (Bottom Left) ---
+        # Compact 2x2 layout:
+        #   Row 0: Process Length | Auto-Convergence
+        #   Row 1: Output CRF Full | Output CRF Low
+
+        self.output_settings_frame = ttk.LabelFrame(
+            self.process_settings_container, text="Splatting & Output Settings"
+        )
+        self.output_settings_frame.grid(
+            row=1, column=0, padx=(0, 5), sticky="ew", pady=(2, 0)
+        )
+        self.output_settings_frame.grid_columnconfigure(0, weight=0)
+        self.output_settings_frame.grid_columnconfigure(1, weight=0)
+
+        # Row 0, Col 0: Process Length
+        self.proc_len_frame = ttk.Frame(self.output_settings_frame)
+        self.proc_len_frame.grid(row=0, column=0, sticky="w", padx=5, pady=0)
+        self.lbl_process_length = ttk.Label(self.proc_len_frame, text="Process Length:")
+        self.lbl_process_length.pack(side="left", padx=(0, 3))
+        self.entry_process_length = ttk.Entry(
+            self.proc_len_frame, textvariable=self.process_length_var, width=6
+        )
+        self.entry_process_length.pack(side="left")
+        self._create_hover_tooltip(self.lbl_process_length, "process_length")
+        self._create_hover_tooltip(self.entry_process_length, "process_length")
+
+        # Row 0, Col 1: Auto-Convergence
+        self.auto_conv_frame = ttk.Frame(self.output_settings_frame)
+        self.auto_conv_frame.grid(row=0, column=1, sticky="w", padx=5, pady=0)
+        self.lbl_auto_convergence = ttk.Label(
+            self.auto_conv_frame, text="Auto-Convergence:"
+        )
+        self.lbl_auto_convergence.pack(side="left", padx=(0, 3))
+        self.auto_convergence_combo = ttk.Combobox(
+            self.auto_conv_frame,
+            textvariable=self.auto_convergence_mode_var,
+            values=["Off", "Average", "Peak", "Hybrid"],
+            state="readonly",
+            width=10,
+        )
+        self.auto_convergence_combo.pack(side="left")
+        self._create_hover_tooltip(self.lbl_auto_convergence, "auto_convergence_toggle")
+        self._create_hover_tooltip(
+            self.auto_convergence_combo, "auto_convergence_toggle"
+        )
+        self.auto_convergence_combo.bind(
+            "<<ComboboxSelected>>", self.on_auto_convergence_mode_select
+        )
+
+        # Row 1, Col 0: Output CRF Full
+        self.crf_full_frame = ttk.Frame(self.output_settings_frame)
+        self.crf_full_frame.grid(row=1, column=0, sticky="w", padx=5, pady=0)
+        self.lbl_output_crf_full = ttk.Label(
+            self.crf_full_frame, text="Output CRF Full:"
+        )
+        self.lbl_output_crf_full.pack(side="left", padx=(0, 3))
+        self.entry_output_crf_full = ttk.Entry(
+            self.crf_full_frame, textvariable=self.output_crf_full_var, width=3
+        )
+        self.entry_output_crf_full.pack(side="left")
+        self._create_hover_tooltip(self.lbl_output_crf_full, "output_crf")
+        self._create_hover_tooltip(self.entry_output_crf_full, "output_crf")
+
+        # Row 1, Col 1: Output CRF Low
+        self.crf_low_frame = ttk.Frame(self.output_settings_frame)
+        self.crf_low_frame.grid(row=1, column=1, sticky="w", padx=5, pady=0)
+        self.lbl_output_crf_low = ttk.Label(self.crf_low_frame, text="Output CRF Low:")
+        self.lbl_output_crf_low.pack(side="left", padx=(0, 3))
+        self.entry_output_crf_low = ttk.Entry(
+            self.crf_low_frame, textvariable=self.output_crf_low_var, width=3
+        )
+        self.entry_output_crf_low.pack(side="left")
+        self._create_hover_tooltip(self.lbl_output_crf_low, "output_crf")
+        self._create_hover_tooltip(self.entry_output_crf_low, "output_crf")
+
+        # Row 2, Col 0: Output color tag mode (metadata-only; written into output file headers)
+        self.color_tags_frame = ttk.Frame(self.output_settings_frame)
+        self.color_tags_frame.grid(row=2, column=0, sticky="w", padx=5, pady=0)
+        self.lbl_color_tags_mode = ttk.Label(self.color_tags_frame, text="Color Tags:")
+        self.lbl_color_tags_mode.pack(side="left", padx=(0, 3))
+        self.combo_color_tags_mode = ttk.Combobox(
+            self.color_tags_frame,
+            textvariable=self.color_tags_mode_var,
+            values=["Off", "Auto", "BT.709", "BT.2020"],
+            state="readonly",
+            width=10,
+        )
+        self.combo_color_tags_mode.pack(side="left")
+        self._create_hover_tooltip(self.lbl_color_tags_mode, "color_tags_mode")
+        self._create_hover_tooltip(self.combo_color_tags_mode, "color_tags_mode")
+
+        # Row 2, Col 1: Border Mode Pulldown + Rescan Button
+        self.border_mode_frame = ttk.Frame(self.output_settings_frame)
+        self.border_mode_frame.grid(row=2, column=1, sticky="w", padx=5, pady=0)
+        self.lbl_border_mode = ttk.Label(self.border_mode_frame, text="Border:")
+        self.lbl_border_mode.pack(side="left", padx=(0, 3))
+        self.combo_border_mode = ttk.Combobox(
+            self.border_mode_frame,
+            textvariable=self.border_mode_var,
+            values=["Manual", "Auto Basic", "Auto Adv.", "Off"],
+            state="readonly",
+            width=10,
+            takefocus=False,
+        )
+        self.combo_border_mode.pack(side="left")
+
+        # Rescan Button (similar to loop button)
+        self.btn_border_rescan = ttk.Button(
+            self.border_mode_frame,
+            text="‚ü≥",  # Unicode rescan-like symbol
+            width=2,
+            command=self._on_border_rescan_click,
+            takefocus=False,
+        )
+        self.btn_border_rescan.pack(side="left", padx=(3, 0))
+
+        self._create_hover_tooltip(self.lbl_border_mode, "border_mode")
+        self._create_hover_tooltip(self.combo_border_mode, "border_mode")
+        self._create_hover_tooltip(self.btn_border_rescan, "border_rescan")
+
+        # Track these for disabling during processing
+        self.widgets_to_disable.extend(
+            [self.combo_color_tags_mode, self.combo_border_mode, self.btn_border_rescan]
+        )
+
+        current_row = 0  # Reset for next frame
+        # ===================================================================
+        # RIGHT SIDE: Depth Map Pre-processing Frame
+        # ===================================================================
+        self.depth_settings_container = ttk.Frame(self.settings_container_frame)
+        self.depth_settings_container.grid(row=0, column=2, padx=(5, 0), sticky="nsew")
+        self.depth_settings_container.grid_columnconfigure(0, weight=1)
+
+        # --- Hi-Res Depth Pre-processing Frame (Top-Right) ---
+        current_depth_row = 0  # Use a new counter for this container
+        self.depth_prep_frame = ttk.LabelFrame(
+            self.depth_settings_container, text="Depth Map Pre-processing"
+        )
+        self.depth_prep_frame.grid(
+            row=current_depth_row, column=0, sticky="ew"
+        )  # Use grid here for placement inside container
+        self.depth_prep_frame.grid_columnconfigure(1, weight=1)
+
+        # Slider Implementation for dilate and blur
+        # --- MODIFIED: Dilation Slider with Expanded Erosion Range (Slider goes to 40) ---
+        row_inner = 0
+        _, _, _ = create_dual_slider_layout(
+            self,
+            self.depth_prep_frame,
+            "Dilate X:",
+            "Y:",
+            self.depth_dilate_size_x_var,
+            self.depth_dilate_size_y_var,
+            -10.0,
+            30.0,
+            row_inner,
+            decimals=1,
+            is_integer=False,
+            tooltip_key_x="depth_dilate_size_x",
+            tooltip_key_y="depth_dilate_size_y",
+            trough_increment=0.5,
+            display_next_odd_integer=False,
+            default_x=0.0,
+            default_y=0.0,
+        )
+        row_inner += 1
+        _, _, _ = create_dual_slider_layout(
+            self,
+            self.depth_prep_frame,
+            "   Blur X:",
+            "Y:",
+            self.depth_blur_size_x_var,
+            self.depth_blur_size_y_var,
+            0,
+            35,
+            row_inner,
+            decimals=0,
+            is_integer=True,
+            tooltip_key_x="depth_blur_size_x",
+            tooltip_key_y="depth_blur_size_y",
+            trough_increment=1.0,
+            default_x=0.0,
+            default_y=0.0,
+        )
+
+        row_inner += 1
+
+        # Dilate Left (0.5 steps) + Blur Left (integer steps)
+        _, _, (_, left_blur_frame) = create_dual_slider_layout(
+            self,
+            self.depth_prep_frame,
+            "Dilate Left:",
+            "Blur Left:",
+            self.depth_dilate_left_var,
+            self.depth_blur_left_var,
+            0.0,
+            20.0,
+            row_inner,
+            decimals=1,
+            decimals_y=0,
+            tooltip_key_x="depth_dilate_left",
+            tooltip_key_y="depth_blur_left",
+            trough_increment=0.5,
+            default_x=0.0,
+            default_y=0.0,
+        )
+
+        # Blur Left H‚ÜîV balance (0.0 = all horizontal, 1.0 = all vertical; 0.5 = 50/50)
+        try:
+            mix_values = [f"{i / 10:.1f}" for i in range(0, 11)]
+        except Exception:
+            mix_values = [
+                "0.0",
+                "0.1",
+                "0.2",
+                "0.3",
+                "0.4",
+                "0.5",
+                "0.6",
+                "0.7",
+                "0.8",
+                "0.9",
+                "1.0",
+            ]
+
+        # Place a small selector immediately after the Blur Left slider (keep it compact so the row doesn't grow).
+        left_blur_mix_combo = ttk.Combobox(
+            left_blur_frame,
+            textvariable=self.depth_blur_left_mix_var,
+            values=mix_values,
+            width=4,
+            state="readonly",
+        )
+        # Most slider layouts use columns 0-2; put this in the next free column.
+        left_blur_mix_combo.grid(row=0, column=3, sticky="w", padx=(4, 0))
+        self._create_hover_tooltip(left_blur_mix_combo, "depth_blur_left_mix")
+        # Trigger an immediate preview refresh when the mix selector changes
+        left_blur_mix_combo.bind(
+            "<<ComboboxSelected>>",
+            lambda e: self.previewer.update_preview()
+            if getattr(self, "previewer", None)
+            else None,
+        )
+
+        # --- NEW: Depth Pre-processing (All) Frame (Bottom-Right) ---
+        current_depth_row += 1
+        self.depth_all_settings_frame = ttk.LabelFrame(
+            self.depth_settings_container, text="Stereo Projection"
+        )
+        self.depth_all_settings_frame.grid(
+            row=current_depth_row, column=0, sticky="ew", pady=(2, 0)
+        )  # Pack it below Hi-Res frame
+        self.depth_all_settings_frame.grid_columnconfigure(0, weight=0)
+        self.depth_all_settings_frame.grid_columnconfigure(1, weight=1)
+        self.depth_all_settings_frame.grid_columnconfigure(2, weight=0)
+
+        all_settings_row = 0
+
+        # Gamma + Disparity (same row)
+        gamma_disp_row_frame = ttk.Frame(self.depth_all_settings_frame)
+        gamma_disp_row_frame.grid(
+            row=all_settings_row, column=0, columnspan=3, sticky="ew"
+        )
+        gamma_disp_row_frame.grid_columnconfigure(0, weight=1)
+        gamma_disp_row_frame.grid_columnconfigure(1, weight=1)
+
+        gamma_subframe = ttk.Frame(gamma_disp_row_frame)
+        gamma_subframe.grid(row=0, column=0, sticky="ew", padx=(0, 8))
+        gamma_subframe.grid_columnconfigure(1, weight=1)
+
+        disp_subframe = ttk.Frame(gamma_disp_row_frame)
+        disp_subframe.grid(row=0, column=1, sticky="ew")
+        disp_subframe.grid_columnconfigure(1, weight=1)
+
+        create_single_slider_with_label_updater(
+            self,
+            gamma_subframe,
+            "Gamma:",
+            self.depth_gamma_var,
+            0.1,
+            3.0,
+            0,
+            decimals=2,
+            tooltip_key="depth_gamma",
+            trough_increment=0.05,
+            step_size=0.05,
+            default_value=1.0,
+        )
+
+        self.set_disparity_value_programmatically = (
+            create_single_slider_with_label_updater(
+                self,
+                disp_subframe,
+                "Disparity:",
+                self.max_disp_var,
+                0.0,
+                100.0,
+                0,
+                decimals=0,
+                tooltip_key="max_disp",
+                step_size=1.0,
+                default_value=30.0,
+            )
+        )
+
+        # --- Estimate Max Total(D+P) (quick sampled scan) ---
+        try:
+            self.btn_est_dp_total = ttk.Button(
+                disp_subframe,
+                text="‚âà",
+                width=2,
+                command=self.run_estimate_dp_total_max,
+                takefocus=False,
+            )
+            self.btn_est_dp_total.grid(row=0, column=3, sticky="w", padx=(6, 0))
+            if hasattr(self, "_create_hover_tooltip"):
+                self._create_hover_tooltip(self.btn_est_dp_total, "estimate_dp_total")
+        except Exception:
+            pass
+        all_settings_row += 1
+
+        # Convergence Point Slider
+        setter_func_conv = create_single_slider_with_label_updater(
+            self,
+            self.depth_all_settings_frame,
+            "Convergence:",
+            self.zero_disparity_anchor_var,
+            0.0,
+            2.0,
+            all_settings_row,
+            decimals=2,
+            tooltip_key="convergence_point",
+            step_size=0.01,
+            default_value=0.5,
+        )
+        self.set_convergence_value_programmatically = setter_func_conv
+        all_settings_row += 1
+
+        # Border Width & Bias Sliders
+        # The improved create_dual_slider_layout returns the frame, setters, and subframes.
+        (
+            self.border_sliders_row_frame,
+            (
+                self.set_border_width_programmatically,
+                self.set_border_bias_programmatically,
+            ),
+            _,
+        ) = create_dual_slider_layout(
+            self,
+            self.depth_all_settings_frame,
+            "Border Width:",
+            "Bias:",
+            self.border_width_var,
+            self.border_bias_var,
+            0.0,
+            5.0,
+            all_settings_row,
+            decimals=2,
+            tooltip_key_x="border_width",
+            tooltip_key_y="border_bias",
+            trough_increment=0.1,
+            step_size_x=0.01,
+            step_size_y=0.01,
+            default_x=0.0,
+            default_y=0.0,
+            from_y=-1.0,
+            to_y=1.0,
+        )
+        # Store the frame containing the sliders for the manual toggle logic
+        # Since create_dual_slider_layout now returns the frame directly,
+        # the lookup loop is no longer needed.
+        # for child in self.depth_all_settings_frame.winfo_children():
+        #     info = child.grid_info()
+        #     if info.get("row") == str(all_settings_row):
+        #         self.border_sliders_row_frame = child
+        #         break
+
+        all_settings_row += 1
+        # --- Global Normalization + Resume (packed in a sub-frame so slider columns stay aligned) ---
+        checkbox_row = ttk.Frame(self.depth_all_settings_frame)
+        checkbox_row.grid(
+            row=all_settings_row, column=0, columnspan=3, sticky="w", padx=5, pady=0
+        )
+
+        self.global_norm_checkbox = ttk.Checkbutton(
+            checkbox_row,
+            text="Enable Global Normalization",
+            variable=self.enable_global_norm_var,
+            command=self.on_slider_release,
+        )
+        self.global_norm_checkbox.pack(side="left")
+        self._create_hover_tooltip(
+            self.global_norm_checkbox, "enable_global_normalization"
+        )
+
+        self.move_to_finished_checkbox = ttk.Checkbutton(
+            checkbox_row, text="Resume", variable=self.move_to_finished_var
+        )
+        self.move_to_finished_checkbox.pack(side="left", padx=(10, 0))
+        self._create_hover_tooltip(
+            self.move_to_finished_checkbox, "move_to_finished_folder"
+        )
+        # Crosshair overlay controls (preview only)
+        self.crosshair_checkbox = ttk.Checkbutton(
+            checkbox_row,
+            text="Crosshair",
+            variable=self.crosshair_enabled_var,
+            takefocus=False,
+            command=lambda: (
+                self.previewer.set_crosshair_settings(
+                    self.crosshair_enabled_var.get(),
+                    self.crosshair_white_var.get(),
+                    self.crosshair_multi_var.get(),
+                ),
+                getattr(self.previewer, "preview_canvas", self.previewer).focus_set(),
+            )
+            if getattr(self, "previewer", None)
+            else None,
+        )
+        self.crosshair_checkbox.pack(side="left", padx=(24, 0))
+        self._create_hover_tooltip(self.crosshair_checkbox, "crosshair_enabled")
+
+        self.crosshair_white_checkbox = ttk.Checkbutton(
+            checkbox_row,
+            text="White",
+            variable=self.crosshair_white_var,
+            takefocus=False,
+            command=lambda: (
+                self.previewer.set_crosshair_settings(
+                    self.crosshair_enabled_var.get(),
+                    self.crosshair_white_var.get(),
+                    self.crosshair_multi_var.get(),
+                ),
+                getattr(self.previewer, "preview_canvas", self.previewer).focus_set(),
+            )
+            if getattr(self, "previewer", None)
+            else None,
+        )
+        self.crosshair_white_checkbox.pack(side="left", padx=(6, 0))
+        self._create_hover_tooltip(self.crosshair_white_checkbox, "crosshair_white")
+
+        self.crosshair_multi_checkbox = ttk.Checkbutton(
+            checkbox_row,
+            text="Multi",
+            variable=self.crosshair_multi_var,
+            takefocus=False,
+            command=lambda: (
+                self.previewer.set_crosshair_settings(
+                    self.crosshair_enabled_var.get(),
+                    self.crosshair_white_var.get(),
+                    self.crosshair_multi_var.get(),
+                ),
+                getattr(self.previewer, "preview_canvas", self.previewer).focus_set(),
+            )
+            if getattr(self, "previewer", None)
+            else None,
+        )
+        self.crosshair_multi_checkbox.pack(side="left", padx=(6, 0))
+        self._create_hover_tooltip(self.crosshair_multi_checkbox, "crosshair_multi")
+        self.depth_pop_checkbox = ttk.Checkbutton(
+            checkbox_row,
+            text="D/P",
+            variable=self.depth_pop_enabled_var,
+            takefocus=False,
+            command=lambda: (
+                getattr(self.previewer, "set_depth_pop_enabled", lambda *_: None)(
+                    self.depth_pop_enabled_var.get()
+                ),
+                getattr(self.previewer, "preview_canvas", self.previewer).focus_set(),
+            )
+            if getattr(self, "previewer", None)
+            else None,
+        )
+        self.depth_pop_checkbox.pack(side="left", padx=(24, 0))
+        self._create_hover_tooltip(self.depth_pop_checkbox, "depth_pop_readout")
+
+        all_settings_row += 1
+
+        current_row = 0  # Reset for next frame
+        # ===================================================================
+        # --- RIGHT COLUMN: Current Processing Information frame ---
+        # ===================================================================
+        # --- RIGHT COLUMN STACK (Current Processing Information + Dev Tools) ---
+        self.right_column_stack = ttk.Frame(self.main_layout_frame)
+        self.right_column_stack.grid(row=0, column=1, sticky="nsew", padx=(0, 0))
+        self.right_column_stack.grid_columnconfigure(0, weight=1, minsize=0)
+        self.right_column_stack.grid_rowconfigure(0, weight=0)
+        self.right_column_stack.grid_rowconfigure(1, weight=0)
+
+        # --- Current Processing Information frame ---
+        self.info_frame = ttk.LabelFrame(
+            self.right_column_stack, text="Current Processing Information"
+        )
+        self.info_frame.grid(row=0, column=0, sticky="ew", padx=0, pady=(0, 2))
+
+        # Two-column pairs: [Label, Value] [Label, Value]
+        self.info_frame.grid_columnconfigure(0, weight=0)
+        self.info_frame.grid_columnconfigure(1, weight=1, minsize=0)
+        self.info_frame.grid_columnconfigure(2, weight=0)
+        self.info_frame.grid_columnconfigure(3, weight=1, minsize=0)
+
+        self.info_labels = []  # List to hold the tk.Label widgets for easy iteration
+        LABEL_VALUE_WIDTH = 18
+        info_row = 0
+
+        # Row 0: Filename (span across)
+        lbl_filename_static = tk.Label(self.info_frame, text="Filename:")
+        lbl_filename_static.grid(row=info_row, column=0, sticky="e", padx=5, pady=1)
+        lbl_filename_value = tk.Label(
+            self.info_frame,
+            textvariable=self.processing_filename_var,
+            anchor="w",
+            width=LABEL_VALUE_WIDTH,
+        )
+        lbl_filename_value.grid(
+            row=info_row, column=1, columnspan=3, sticky="ew", padx=5, pady=1
+        )
+        self.info_labels.extend([lbl_filename_static, lbl_filename_value])
+        info_row += 1
+
+        # Row 1: Task (span across)
+        lbl_task_static = tk.Label(self.info_frame, text="Task:")
+        lbl_task_static.grid(row=info_row, column=0, sticky="e", padx=5, pady=1)
+        lbl_task_value = tk.Label(
+            self.info_frame,
+            textvariable=self.processing_task_name_var,
+            anchor="w",
+            width=LABEL_VALUE_WIDTH,
+        )
+        lbl_task_value.grid(
+            row=info_row, column=1, columnspan=3, sticky="ew", padx=5, pady=1
+        )
+        self.info_labels.extend([lbl_task_static, lbl_task_value])
+        info_row += 1
+
+        # Row 2: Resolution | Disparity
+        lbl_resolution_static = tk.Label(self.info_frame, text="Resolution:")
+        lbl_resolution_static.grid(row=info_row, column=0, sticky="e", padx=5, pady=1)
+        lbl_resolution_value = tk.Label(
+            self.info_frame,
+            textvariable=self.processing_resolution_var,
+            anchor="w",
+            width=LABEL_VALUE_WIDTH,
+        )
+        lbl_resolution_value.grid(row=info_row, column=1, sticky="ew", padx=5, pady=1)
+
+        lbl_disparity_static = tk.Label(self.info_frame, text="Disparity:")
+        lbl_disparity_static.grid(row=info_row, column=2, sticky="e", padx=5, pady=1)
+        lbl_disparity_value = tk.Label(
+            self.info_frame,
+            textvariable=self.processing_disparity_var,
+            anchor="w",
+            width=LABEL_VALUE_WIDTH,
+        )
+        lbl_disparity_value.grid(row=info_row, column=3, sticky="ew", padx=5, pady=1)
+
+        self.info_labels.extend(
+            [
+                lbl_resolution_static,
+                lbl_resolution_value,
+                lbl_disparity_static,
+                lbl_disparity_value,
+            ]
+        )
+        info_row += 1
+
+        # Row 3: Frames | Converge
+        lbl_frames_static = tk.Label(self.info_frame, text="Frames:")
+        lbl_frames_static.grid(row=info_row, column=0, sticky="e", padx=5, pady=1)
+        lbl_frames_value = tk.Label(
+            self.info_frame,
+            textvariable=self.processing_frames_var,
+            anchor="w",
+            width=LABEL_VALUE_WIDTH,
+        )
+        lbl_frames_value.grid(row=info_row, column=1, sticky="ew", padx=5, pady=1)
+
+        lbl_converge_static = tk.Label(self.info_frame, text="Converge:")
+        lbl_converge_static.grid(row=info_row, column=2, sticky="e", padx=5, pady=1)
+        lbl_converge_value = tk.Label(
+            self.info_frame,
+            textvariable=self.processing_convergence_var,
+            anchor="w",
+            width=LABEL_VALUE_WIDTH,
+        )
+        lbl_converge_value.grid(row=info_row, column=3, sticky="ew", padx=5, pady=1)
+
+        self.info_labels.extend(
+            [
+                lbl_frames_static,
+                lbl_frames_value,
+                lbl_converge_static,
+                lbl_converge_value,
+            ]
+        )
+        info_row += 1
+
+        # Row 4: Gamma | Map
+        lbl_gamma_static = tk.Label(self.info_frame, text="Gamma:")
+        lbl_gamma_static.grid(row=info_row, column=0, sticky="e", padx=5, pady=1)
+        lbl_gamma_value = tk.Label(
+            self.info_frame,
+            textvariable=self.processing_gamma_var,
+            anchor="w",
+            width=LABEL_VALUE_WIDTH,
+        )
+        lbl_gamma_value.grid(row=info_row, column=1, sticky="ew", padx=5, pady=1)
+
+        lbl_map_static = tk.Label(self.info_frame, text="Map:")
+        lbl_map_static.grid(row=info_row, column=2, sticky="e", padx=5, pady=1)
+        lbl_map_value = tk.Label(
+            self.info_frame,
+            textvariable=self.processing_map_var,
+            anchor="w",
+            width=LABEL_VALUE_WIDTH,
+        )
+        lbl_map_value.grid(row=info_row, column=3, sticky="ew", padx=5, pady=1)
+
+        self.info_labels.extend(
+            [lbl_gamma_static, lbl_gamma_value, lbl_map_static, lbl_map_value]
+        )
+
+        # --- Dev Tools (right column) ---
+        self.dev_tools_frame = ttk.LabelFrame(self.right_column_stack, text="Dev Tools")
+        self.dev_tools_frame.grid(row=1, column=0, sticky="ew", padx=0, pady=(0, 0))
+        self.dev_tools_frame.grid_columnconfigure(0, weight=0)
+        self.dev_tools_frame.grid_columnconfigure(1, weight=0)
+        self.dev_tools_frame.grid_columnconfigure(2, weight=0)
+        self.dev_tools_frame.grid_columnconfigure(3, weight=1)
+
+        self.chk_skip_lowres_preproc = ttk.Checkbutton(
+            self.dev_tools_frame,
+            text="Skip Low-Res Pre-proc",
+            variable=self.skip_lowres_preproc_var,
+        )
+        self.chk_skip_lowres_preproc.grid(row=0, column=0, sticky="w", padx=5, pady=0)
+        self._create_hover_tooltip(self.chk_skip_lowres_preproc, "skip_lowres_preproc")
+
+
+        self.chk_track_dp_total_true = ttk.Checkbutton(
+            self.dev_tools_frame,
+            text="True Max",
+            variable=self.track_dp_total_true_on_render_var,
+        )
+        self.chk_track_dp_total_true.grid(row=0, column=3, sticky="w", padx=(10, 0), pady=0)
+        self._create_hover_tooltip(self.chk_track_dp_total_true, "track_dp_total_true_on_render")
+        self.chk_map_test = ttk.Checkbutton(
+            self.dev_tools_frame,
+            text="Map Test",
+            variable=self.map_test_var,
+            command=lambda: self.splat_test_var.set(False)
+            if self.map_test_var.get()
+            else None,
+        )
+        self.chk_map_test.grid(row=0, column=1, sticky="w", padx=(10, 0), pady=0)
+        self._create_hover_tooltip(self.chk_map_test, "map_test")
+
+        self.chk_splat_test = ttk.Checkbutton(
+            self.dev_tools_frame,
+            text="Splat Test",
+            variable=self.splat_test_var,
+            command=lambda: self.map_test_var.set(False)
+            if self.splat_test_var.get()
+            else None,
+        )
+        self.chk_splat_test.grid(row=0, column=2, sticky="w", padx=(10, 0), pady=0)
+        self._create_hover_tooltip(self.chk_splat_test, "splat_test")
+
+        progress_frame = ttk.LabelFrame(self, text="Progress")
+        progress_frame.pack(pady=2, padx=10, fill="x")
+        self.progress_var = tk.DoubleVar()
+        self.progress_bar = ttk.Progressbar(
+            progress_frame, variable=self.progress_var, maximum=100
+        )
+        self.progress_bar.pack(fill="x", expand=True, padx=5, pady=2)
+        self.status_label = ttk.Label(progress_frame, text="Ready")
+        self.status_label.pack(padx=5, pady=2)
+
+        # --- Button frame ---
+        button_frame = ttk.Frame(self)
+        button_frame.pack(pady=2)
+
+        # --- Single Process Button ---
+        self.start_single_button = ttk.Button(
+            button_frame, text="SINGLE", command=self.start_single_processing
+        )
+        self.start_single_button.pack(side="left", padx=5)
+        self._create_hover_tooltip(self.start_single_button, "start_single_button")
+
+        # --- Start Process Button ---
+        self.start_button = ttk.Button(
+            button_frame, text="START", command=self.start_processing
+        )
+        self.start_button.pack(side="left", padx=5)
+        self._create_hover_tooltip(self.start_button, "start_button")
+
+        # --- From/To Process Range ---
+        ttk.Label(button_frame, text="From:").pack(side="left", padx=(15, 2))
+        self.entry_process_from = ttk.Entry(
+            button_frame, textvariable=self.process_from_var, width=6
+        )
+        self.entry_process_from.pack(side="left", padx=2)
+        self._create_hover_tooltip(self.entry_process_from, "process_from")
+
+        ttk.Label(button_frame, text="To:").pack(side="left", padx=(5, 2))
+        self.entry_process_to = ttk.Entry(
+            button_frame, textvariable=self.process_to_var, width=6
+        )
+        self.entry_process_to.pack(side="left", padx=2)
+        self._create_hover_tooltip(self.entry_process_to, "process_to")
+
+        # --- Stop Process Button ---
+        self.stop_button = ttk.Button(
+            button_frame, text="STOP", command=self.stop_processing, state="disabled"
+        )
+        self.stop_button.pack(side="left", padx=5)
+        self._create_hover_tooltip(self.stop_button, "stop_button")
+
+        # --- Preview Auto-Converge Button ---
+        self.btn_auto_converge_preview = ttk.Button(
+            button_frame,
+            text="Preview Auto-Converge",
+            command=self.run_preview_auto_converge,
+            takefocus=False,
+        )
+        self.btn_auto_converge_preview.pack(side="left", padx=5)
+        self._create_hover_tooltip(
+            self.btn_auto_converge_preview, "preview_auto_converge"
+        )
+
+        # --- AUTO-PASS Button ---
+        self.btn_auto_pass = ttk.Button(
+            button_frame,
+            text="AUTO-PASS",
+            command=self.run_auto_pass,
+            takefocus=False,
+        )
+        self.btn_auto_pass.pack(side="left", padx=(9, 9))
+        self._create_hover_tooltip(self.btn_auto_pass, "auto_pass_button")
+
+        # --- Update Sidecar Button ---
+        self.update_sidecar_button = ttk.Button(
+            button_frame, text="Update Sidecar", command=self.update_sidecar_file
+        )
+        self.update_sidecar_button.pack(side="left", padx=5)
+        self._create_hover_tooltip(self.update_sidecar_button, "update_sidecar_button")
+
+    def _setup_keyboard_shortcuts(self):
+        """Sets up keyboard shortcuts for quick adjustments.
+
+        Shortcuts only work when NOT in a text entry field:
+        - 7/9: Previous/Next depth map (Multi-Map)
+        - 4/6: Decrease/Increase Max Disparity
+        - 1/3: Decrease/Increase Convergence
+        - 2: Cycle Border Mode
+        """
+        self.bind("<KeyPress>", self._handle_keypress)
+
+    def _handle_keypress(self, event):
+        """Handles keyboard shortcuts, but only when not in a text entry."""
+        # Check if focus is in an Entry or Text widget
+        focused_widget = self.focus_get()
+        if isinstance(
+            focused_widget, (tk.Entry, tk.Text, ttk.Entry, ttk.Combobox, ttk.Spinbox)
+        ):
+            # User is typing in a text field - don't intercept
+            return
+
+        # ENTER: Update sidecar (confirmation dialog will appear as usual)
+        if event.keysym in ("Return", "KP_Enter"):
+            try:
+                self.update_sidecar_file()
+            except Exception as e:
+                logger.error(f"Enter-key sidecar update failed: {e}", exc_info=True)
+            return "break"
+
+        # Map shortcuts
+        if event.char == "7":
+            self._cycle_depth_map(-1)  # Previous map
+        elif event.char == "9":
+            self._cycle_depth_map(1)  # Next map
+        elif event.char == "4":
+            self._adjust_disparity(-1)  # Decrease disparity
+        elif event.char == "6":
+            self._adjust_disparity(1)  # Increase disparity
+        elif event.char == "1":
+            self._adjust_convergence(-0.01)  # Decrease convergence
+        elif event.char == "3":
+            self._adjust_convergence(0.01)  # Increase convergence
+        elif event.char == "2":
+            self._cycle_border_mode()  # Cycle border mode
+
+    def _cycle_depth_map(self, direction):
+        """Cycles through depth map subfolders.
+
+        Args:
+            direction: -1 for previous, 1 for next
+        """
+        if not self.multi_map_var.get():
+            return  # Multi-Map not enabled
+
+        if not self.depth_map_subfolders:
+            return  # No subfolders
+
+        current_value = self.selected_depth_map_var.get()
+        try:
+            current_index = self.depth_map_subfolders.index(current_value)
+        except ValueError:
+            current_index = 0
+
+        # Calculate new index with wrapping
+        new_index = (current_index + direction) % len(self.depth_map_subfolders)
+        new_value = self.depth_map_subfolders[new_index]
+
+        # Update the selection
+        self.selected_depth_map_var.set(new_value)
+
+        # Trigger the map change
+        self._on_map_selection_changed()
+
+    def _cycle_border_mode(self):
+        """Cycles through border modes: Manual -> Auto Basic -> Auto Adv. -> Off."""
+        modes = ["Manual", "Auto Basic", "Auto Adv.", "Off"]
+        current = self.border_mode_var.get()
+        try:
+            idx = modes.index(current)
+        except ValueError:
+            idx = modes.index("Off")
+
+        new_idx = (idx + 1) % len(modes)
+        self.border_mode_var.set(modes[new_idx])
+
+    def _adjust_disparity(self, direction):
+        """Adjusts Max Disparity value.
+
+        Args:
+            direction: -1 to decrease, 1 to increase
+        """
+        try:
+            current = float(self.max_disp_var.get())
+            new_value = max(0, min(100, current + direction))  # Clamp 0-100
+
+            # Use the proper setter function which updates both slider AND label
+            if (
+                hasattr(self, "set_disparity_value_programmatically")
+                and self.set_disparity_value_programmatically
+            ):
+                self.set_disparity_value_programmatically(new_value)
+            else:
+                self.max_disp_var.set(f"{new_value:.1f}")
+
+            # Trigger preview update
+            self.on_slider_release(None)
+        except ValueError:
+            pass  # Invalid current value
+
+    def _adjust_convergence(self, delta):
+        """Adjusts Convergence Plane value.
+
+        Args:
+            delta: Amount to change (e.g., 0.01 or -0.01)
+        """
+        try:
+            current = float(self.zero_disparity_anchor_var.get())
+            new_value = max(0.0, min(1.0, current + delta))  # Clamp between 0 and 1
+
+            # Use the proper setter function which updates both slider AND label
+            if self.set_convergence_value_programmatically:
+                self.set_convergence_value_programmatically(new_value)
+            else:
+                self.zero_disparity_anchor_var.set(f"{new_value:.2f}")
+
+            # Trigger preview update
+            self.on_slider_release(None)
+        except ValueError:
+            pass  # Invalid current value
+
+    def depthSplatting(
+        self: "SplatterGUI",
+        input_video_reader: VideoReader,
+        depth_map_reader: VideoReader,
+        total_frames_to_process: int,
+        processed_fps: float,
+        output_video_path_base: str,
+        target_output_height: int,
+        target_output_width: int,
+        max_disp: float,
+        process_length: int,
+        batch_size: int,
+        dual_output: bool,
+        zero_disparity_anchor_val: float,
+        video_stream_info: Optional[dict],
+        input_bias: Optional[float],
+        assume_raw_input: bool,
+        global_depth_min: float,
+        global_depth_max: float,
+        depth_stream_info: Optional[dict],
+        user_output_crf: Optional[int] = None,
+        is_low_res_task: bool = False,
+        depth_gamma: float = 1.0,
+        depth_dilate_size_x: float = 0.0,
+        depth_dilate_size_y: float = 0.0,
+        depth_blur_size_x: float = 0.0,
+        depth_blur_size_y: float = 0.0,
+        depth_dilate_left: float = 0.0,
+        depth_blur_left: float = 0.0,
+    ):
+        logger.debug("==> Initializing ForwardWarpStereo module")
+        stereo_projector = ForwardWarpStereo(occlu_map=True).cuda()
+
+        num_frames = total_frames_to_process
+        height, width = target_output_height, target_output_width
+        os.makedirs(os.path.dirname(output_video_path_base), exist_ok=True)
+
+        # --- Determine output grid dimensions and final path ---
+        grid_height, grid_width = (
+            (height, width * 2) if dual_output else (height * 2, width * 2)
+        )
+        suffix = "_splatted2" if dual_output else "_splatted4"
+        res_suffix = f"_{width}"
+        final_output_video_path = (
+            f"{os.path.splitext(output_video_path_base)[0]}{res_suffix}{suffix}.mp4"
+        )
+
+        # --- DEBUG: Log ffprobe-derived color metadata for this pass (Hi/Lo parity checks) ---
+        task_name = "LowRes" if is_low_res_task else "HiRes"
+        try:
+            logger.info(
+                f"[COLOR_META][{task_name}] input ffprobe: "
+                f"pix_fmt={video_stream_info.get('pix_fmt') if video_stream_info else None}, "
+                f"range={video_stream_info.get('color_range') if video_stream_info else None}, "
+                f"primaries={video_stream_info.get('color_primaries') if video_stream_info else None}, "
+                f"trc={video_stream_info.get('transfer_characteristics') if video_stream_info else None}, "
+                f"matrix={video_stream_info.get('color_space') if video_stream_info else None}"
+            )
+        except Exception:
+            pass
+        # --- END DEBUG ---
+
+        # --- Start FFmpeg pipe process ---
+        is_test_mode = bool(
+            (getattr(self, "map_test_var", None) and self.map_test_var.get())
+            or (getattr(self, "splat_test_var", None) and self.splat_test_var.get())
+        )
+        ffmpeg_process = None
+        if is_test_mode:
+            # In test mode we don't want to create/overwrite any output video files.
+            logger.debug(
+                "Test mode active: skipping FFmpeg encoding (no output video will be written)."
+            )
+        else:
+            # --- Start FFmpeg pipe process ---
+            # Apply output color tag mode (metadata-only; does not change splat math)
+            encode_stream_info = (
+                dict(video_stream_info) if isinstance(video_stream_info, dict) else {}
+            )
+            try:
+                _ct_mode = (
+                    self.color_tags_mode_var.get()
+                    if hasattr(self, "color_tags_mode_var")
+                    else "Auto"
+                )
+            except Exception:
+                _ct_mode = "Auto"
+
+            def _ct_defaults(d: dict) -> dict:
+                d.setdefault("color_primaries", "bt709")
+                d.setdefault("transfer_characteristics", "bt709")
+                d.setdefault("color_space", "bt709")
+                # Only metadata; used for tagging if missing. Preserve source if present.
+                d.setdefault("color_range", "tv")
+                return d
+
+            if _ct_mode == "Auto":
+                encode_stream_info = _ct_defaults(encode_stream_info)
+            elif _ct_mode == "BT.709 L":
+                encode_stream_info.update(
+                    {
+                        "color_primaries": "bt709",
+                        "transfer_characteristics": "bt709",
+                        "color_space": "bt709",
+                        "color_range": "tv",
+                    }
+                )
+            elif _ct_mode == "BT.709 F":
+                encode_stream_info.update(
+                    {
+                        "color_primaries": "bt709",
+                        "transfer_characteristics": "bt709",
+                        "color_space": "bt709",
+                        "color_range": "pc",
+                    }
+                )
+            elif _ct_mode == "BT.2020 PQ":
+                encode_stream_info.update(
+                    {
+                        "color_primaries": "bt2020",
+                        "transfer_characteristics": "smpte2084",
+                        "color_space": "bt2020nc",
+                        "color_range": "tv",
+                    }
+                )
+            elif _ct_mode == "BT.2020 HLG":
+                encode_stream_info.update(
+                    {
+                        "color_primaries": "bt2020",
+                        "transfer_characteristics": "arib-std-b67",
+                        "color_space": "bt2020nc",
+                        "color_range": "tv",
+                    }
+                )
+            else:
+                # Unknown/legacy value; keep current behavior
+                encode_stream_info = _ct_defaults(encode_stream_info)
+
+            ffmpeg_process = start_ffmpeg_pipe_process(
+                content_width=grid_width,
+                content_height=grid_height,
+                final_output_mp4_path=final_output_video_path,
+                fps=processed_fps,
+                video_stream_info=encode_stream_info,
+                user_output_crf=user_output_crf,
+                output_format_str="splatted_grid",  # Pass a placeholder for the new argument
+                debug_label=task_name,
+            )
+            if ffmpeg_process is None:
+                logger.error("Failed to start FFmpeg pipe. Aborting splatting task.")
+                return False
+
+        # --- DEBUG: Capture and compare encoding flags between HiRes and LowRes passes ---
+        try:
+            flags = getattr(ffmpeg_process, "sc_encode_flags", None)
+            if flags:
+                if not hasattr(self, "_sc_color_encode_flags"):
+                    self._sc_color_encode_flags = {}
+                subset_keys = [
+                    "enc_codec",
+                    "enc_pix_fmt",
+                    "enc_profile",
+                    "enc_color_primaries",
+                    "enc_color_trc",
+                    "enc_colorspace",
+                    "quality_mode",
+                    "quality_value",
+                ]
+                subset = {k: flags.get(k) for k in subset_keys}
+                self._sc_color_encode_flags[task_name] = subset
+
+                other_name = "HiRes" if task_name == "LowRes" else "LowRes"
+                if other_name in self._sc_color_encode_flags:
+                    other = self._sc_color_encode_flags[other_name]
+                    diffs = {
+                        k: (other.get(k), subset.get(k))
+                        for k in subset_keys
+                        if other.get(k) != subset.get(k)
+                    }
+                    if diffs:
+                        logger.warning(
+                            f"[COLOR_META] Encoding flags differ ({other_name} vs {task_name}): {diffs}"
+                        )
+                    else:
+                        logger.info(
+                            f"[COLOR_META] Encoding flags match between {other_name} and {task_name}."
+                        )
+        except Exception:
+            pass
+        # --- END DEBUG ---
+
+        # --- Determine max_expected_raw_value for consistent Gamma ---
+        max_expected_raw_value = 1.0
+        depth_pix_fmt = depth_stream_info.get("pix_fmt") if depth_stream_info else None
+        depth_profile = depth_stream_info.get("profile") if depth_stream_info else None
+        is_source_10bit = False
+        if depth_pix_fmt:
+            if (
+                "10" in depth_pix_fmt
+                or "gray10" in depth_pix_fmt
+                or "12" in depth_pix_fmt
+                or (depth_profile and "main10" in depth_profile)
+            ):
+                is_source_10bit = True
+        if is_source_10bit:
+            max_expected_raw_value = 1023.0
+        elif depth_pix_fmt and (
+            "8" in depth_pix_fmt or depth_pix_fmt in ["yuv420p", "yuv422p", "yuv444p"]
+        ):
+            max_expected_raw_value = 255.0
+        elif isinstance(depth_pix_fmt, str) and "float" in depth_pix_fmt:
+            max_expected_raw_value = 1.0
+        logger.debug(
+            f"Determined max_expected_raw_value: {max_expected_raw_value:.1f} (Source: {depth_pix_fmt}/{depth_profile})"
+        )
+
+        frame_count = 0
+        encoding_successful = True  # Assume success unless an error occurs
+
+        try:
+            # Map/Splat Test: render the *same frame* the preview is showing.
+            test_target_frame_idx = None
+            try:
+                if (
+                    getattr(self, "map_test_var", None) and self.map_test_var.get()
+                ) or (
+                    getattr(self, "splat_test_var", None) and self.splat_test_var.get()
+                ):
+                    if (
+                        hasattr(self, "previewer")
+                        and getattr(self.previewer, "last_loaded_frame_index", None)
+                        is not None
+                    ):
+                        test_target_frame_idx = int(
+                            self.previewer.last_loaded_frame_index
+                        )
+                        test_target_frame_idx = max(
+                            0, min(int(num_frames) - 1, test_target_frame_idx)
+                        )
+            except Exception:
+                test_target_frame_idx = None
+
+            frame_index_iter = (
+                [test_target_frame_idx]
+                if test_target_frame_idx is not None
+                else range(0, num_frames, batch_size)
+            )
+            for i in frame_index_iter:
+                t_start_batch = time.perf_counter()  # <--- TIMER START: Total Batch
+                if self.stop_event.is_set() or (
+                    ffmpeg_process is not None and ffmpeg_process.poll() is not None
+                ):
+                    if ffmpeg_process is not None and ffmpeg_process.poll() is not None:
+                        logger.error(
+                            "FFmpeg process terminated unexpectedly. Stopping frame processing."
+                        )
+                    else:
+                        logger.warning(
+                            "Stop event received. Terminating FFmpeg process."
+                        )
+                    encoding_successful = False
+                    break
+
+                # --- TIMER 1: Video/Depth I/O (Disk/Decode/Resize) ---
+                t_start_io = time.perf_counter()
+
+                current_frame_indices = (
+                    [i]
+                    if test_target_frame_idx is not None
+                    else list(range(i, min(i + batch_size, num_frames)))
+                )
+                if not current_frame_indices:
+                    break
+
+                batch_frames_numpy = input_video_reader.get_batch(
+                    current_frame_indices
+                ).asnumpy()
+                # This often resolves issues where Decord/FFmpeg loses the internal stream position
+                try:
+                    # Seek to the first frame of the current batch
+                    depth_map_reader.seek(current_frame_indices[0])
+                    # Then read the full batch from that position
+                    batch_depth_numpy_raw = depth_map_reader.get_batch(
+                        current_frame_indices
+                    ).asnumpy()
+                except Exception as e:
+                    logger.error(
+                        f"Error seeking/reading depth map batch starting at index {i}: {e}. Falling back to a potentially blank read."
+                    )
+                    batch_depth_numpy_raw = depth_map_reader.get_batch(
+                        current_frame_indices
+                    ).asnumpy()
+                t_end_io = time.perf_counter()
+
+                file_frame_idx = current_frame_indices[0]
+                task_name = "LowRes" if is_low_res_task else "HiRes"
+
+                if batch_depth_numpy_raw.min() == batch_depth_numpy_raw.max() == 0:
+                    logger.warning(
+                        f"Depth map batch starting at index {i} is entirely blank/zero after read. **Seeking failed to resolve.**"
+                    )
+
+                if batch_depth_numpy_raw.min() == batch_depth_numpy_raw.max():
+                    logger.warning(
+                        f"Depth map batch starting at index {i} is entirely uniform/flat after read. Min/Max: {batch_depth_numpy_raw.min():.2f}"
+                    )
+
+                # Use the FIRST frame index for the file name (e.g., 00000.png)
+                file_frame_idx = current_frame_indices[0]
+
+                # self._save_debug_numpy(batch_depth_numpy_raw, "01_RAW_INPUT", i, file_frame_idx, task_name)
+                # --- TIMER 2: CPU Pre-processing (Dilate, Blur, Grayscale, Gamma, Min/Max Calc) ---
+                # --- 8-bit resize parity (Map Test Preview vs Render) ---
+                # For 8-bit depth videos, Decord's decode-time scaling can differ by ~1-2 code values vs OpenCV.
+                # To match the Preview path, decode at native res (see load_pre_rendered_depth) and resize here
+                # BEFORE preprocessing using the same OpenCV interpolation policy.
+                _bd = (
+                    _infer_depth_bit_depth(depth_stream_info)
+                    if depth_stream_info
+                    else 8
+                )
+                if _bd <= 8:
+                    # Determine pre-processing resolution (matches preview/render ordering).
+                    _target_w = int(batch_frames_numpy.shape[2])
+                    _target_h = int(batch_frames_numpy.shape[1])
+                    if is_low_res_task:
+                        _skip_lr = bool(
+                            getattr(self, "skip_lowres_preproc_var", None) is not None
+                            and self.skip_lowres_preproc_var.get()
+                        )
+                        if not _skip_lr:
+                            # For Low-Res tasks we want to apply preprocessing at the incoming depth-reader resolution
+                            # (typically clip-sized when match_resolution_to_target is enabled), then downscale after preprocessing.
+                            _target_w = int(batch_depth_numpy_raw.shape[2])
+                            _target_h = int(batch_depth_numpy_raw.shape[1])
+
+                    # Reduce to a single channel (depth maps are grayscale-encoded).
+                    if batch_depth_numpy_raw.ndim == 4:
+                        if batch_depth_numpy_raw.shape[-1] == 3:
+                            _depth_1c = batch_depth_numpy_raw[..., 0]
+                        elif batch_depth_numpy_raw.shape[-1] == 1:
+                            _depth_1c = batch_depth_numpy_raw[..., 0]
+                        else:
+                            _depth_1c = batch_depth_numpy_raw.squeeze(-1)
+                    else:
+                        _depth_1c = batch_depth_numpy_raw
+
+                    if (_depth_1c.shape[1] != _target_h) or (
+                        _depth_1c.shape[2] != _target_w
+                    ):
+                        _interp = (
+                            cv2.INTER_LINEAR
+                            if (
+                                _target_w > _depth_1c.shape[2]
+                                or _target_h > _depth_1c.shape[1]
+                            )
+                            else cv2.INTER_AREA
+                        )
+                        _resized = np.empty(
+                            (_depth_1c.shape[0], _target_h, _target_w),
+                            dtype=_depth_1c.dtype,
+                        )
+                        for _di in range(_depth_1c.shape[0]):
+                            _resized[_di] = cv2.resize(
+                                _depth_1c[_di],
+                                (_target_w, _target_h),
+                                interpolation=_interp,
+                            )
+                        batch_depth_numpy_raw = _resized[..., None]
+                    else:
+                        batch_depth_numpy_raw = _depth_1c[..., None]
+
+                t_start_preproc = time.perf_counter()
+
+                batch_depth_numpy = self._process_depth_batch(
+                    batch_depth_numpy_raw=batch_depth_numpy_raw,
+                    depth_stream_info=depth_stream_info,
+                    depth_gamma=depth_gamma,
+                    depth_dilate_size_x=depth_dilate_size_x,
+                    depth_dilate_size_y=depth_dilate_size_y,
+                    depth_blur_size_x=depth_blur_size_x,
+                    depth_blur_size_y=depth_blur_size_y,
+                    depth_dilate_left=depth_dilate_left,
+                    depth_blur_left=depth_blur_left,
+                    is_low_res_task=is_low_res_task,
+                    max_raw_value=max_expected_raw_value,
+                    global_depth_min=global_depth_min,
+                    global_depth_max=global_depth_max,
+                    debug_batch_index=i,
+                    debug_frame_index=file_frame_idx,
+                    debug_task_name=task_name,
+                )
+
+                # If the depth batch resolution differs from the source frames,
+                # resize AFTER preprocessing so dilation/erosion/blur parity is preserved.
+                if (batch_depth_numpy.shape[1] != batch_frames_numpy.shape[1]) or (
+                    batch_depth_numpy.shape[2] != batch_frames_numpy.shape[2]
+                ):
+                    target_h = batch_frames_numpy.shape[1]
+                    target_w = batch_frames_numpy.shape[2]
+                    # INTER_AREA seems best for downscaling depth maps (minimizes aliasing/jaggies-cv2.INTER_LANCZOS4 works too).
+                    if (
+                        target_w < batch_depth_numpy.shape[2]
+                        and target_h < batch_depth_numpy.shape[1]
+                    ):
+                        interp = cv2.INTER_AREA
+                    else:
+                        interp = cv2.INTER_LINEAR
+                    resized_depth = np.empty(
+                        (batch_depth_numpy.shape[0], target_h, target_w),
+                        dtype=batch_depth_numpy.dtype,
+                    )
+                    for _di in range(batch_depth_numpy.shape[0]):
+                        resized_depth[_di] = cv2.resize(
+                            batch_depth_numpy[_di],
+                            (target_w, target_h),
+                            interpolation=interp,
+                        )
+                    batch_depth_numpy = resized_depth
+
+                # self._save_debug_numpy(batch_depth_numpy, "02_PROCESSED_PRE_NORM", i, file_frame_idx, task_name)
+
+                batch_frames_float = batch_frames_numpy.astype("float32") / 255.0
+
+                # 1. Normalize based on mode
+                if assume_raw_input:
+                    if global_depth_max > 1.0:
+                        batch_depth_normalized = batch_depth_numpy / global_depth_max
+                    else:
+                        batch_depth_normalized = batch_depth_numpy.copy()
+                else:
+                    depth_range = global_depth_max - global_depth_min
+                    if depth_range > 1e-5:
+                        batch_depth_normalized = (
+                            batch_depth_numpy - global_depth_min
+                        ) / depth_range
+                    else:
+                        batch_depth_normalized = np.full_like(
+                            batch_depth_numpy,
+                            fill_value=zero_disparity_anchor_val,
+                            dtype=np.float32,
+                        )
+                        logger.warning(
+                            f"Normalization collapsed to zero range ({global_depth_min:.4f} - {global_depth_max:.4f})."
+                        )
+
+                # 2. Unified Inverted Gamma (Matches Preview Math exactly)
+                if round(float(depth_gamma), 2) != 1.0:
+                    # Math: 1.0 - (1.0 - depth)^gamma
+                    batch_depth_normalized = 1.0 - np.power(
+                        1.0 - np.clip(batch_depth_normalized, 0, 1), depth_gamma
+                    )
+                    # (Normal) Depth gamma applied in normalized space; no warning log.
+
+                batch_depth_normalized = np.clip(batch_depth_normalized, 0, 1)
+
+                # --- START OF DIAGNOSTIC SYNC BLOCK ---
+                # Keeps depth visualization at 16-bit to prevent banding in the Render output
+                batch_depth_vis_list = []
+                for d_frame in batch_depth_normalized:
+                    # We keep this as float32 for the assembly to prevent 8-bit quantization
+                    vis_gray_3ch = np.stack([d_frame] * 3, axis=-1)
+                    # Render-time: track measured per-clip max Total(D+P) (percent of width)
+                    # (Uses the same depth frames already being processed; no extra decode pass.)
+                    try:
+                        sig = getattr(self, "_dp_total_true_active_sig", None)
+                        _track_true = (
+                            getattr(self, "track_dp_total_true_on_render_var", None) is not None
+                            and self.track_dp_total_true_on_render_var.get()
+                        )
+                        if sig and _track_true:
+                            # Exclude hard holes (0 and 1) from min/max to avoid bogus extremes
+                            _min_arr = d_frame[d_frame > 0.001]
+                            _max_arr = d_frame[d_frame < 0.999]
+                            if _min_arr.size and _max_arr.size:
+                                dmin = float(_min_arr.min())
+                                dmax = float(_max_arr.max())
+                                # True max Total(D+P) (percent of width) for this frame
+                                # Uses full-frame min/max (holes excluded) with the same math as the preview overlay.
+                                _scale = 2.0 * float(max_disp) * 100.0 / float(d_frame.shape[1])
+                                _min_pct = (dmin - float(zero_disparity_anchor_val)) * _scale
+                                _max_pct = (dmax - float(zero_disparity_anchor_val)) * _scale
+                                _depth_pct = max(0.0, -_min_pct)
+                                _pop_pct = _max_pct
+                                total_pct = float(_depth_pct) + float(_pop_pct)
+
+                                cur = getattr(self, "_dp_total_true_active_val", None)
+                                if cur is None or total_pct > cur:
+                                    self._dp_total_true_active_val = float(total_pct)
+                                    try:
+                                        self._dp_total_true_cache[sig] = float(
+                                            self._dp_total_true_active_val
+                                        )
+                                    except Exception:
+                                        pass
+                    except Exception:
+                        pass
+                    batch_depth_vis_list.append(vis_gray_3ch.astype("float32"))
+
+                batch_depth_vis = np.stack(batch_depth_vis_list, axis=0)
+                # --- END OF DIAGNOSTIC SYNC BLOCK ---
+
+                t_end_preproc = time.perf_counter()
+                # --- END TIMER 2 ---
+
+                # --- TIMER 3: HtoD Transfer (CPU to GPU) ---
+                t_start_transfer_HtoD = time.perf_counter()
+
+                left_video_tensor = (
+                    torch.from_numpy(batch_frames_numpy)
+                    .permute(0, 3, 1, 2)
+                    .float()
+                    .cuda()
+                    / 255.0
+                )
+                disp_map_tensor = (
+                    torch.from_numpy(batch_depth_normalized).unsqueeze(1).float().cuda()
+                )
+                disp_map_tensor = (disp_map_tensor - zero_disparity_anchor_val) * 2.0
+                disp_map_tensor = disp_map_tensor * max_disp
+
+                torch.cuda.synchronize()  # Force synchronization before compute
+                t_end_transfer_HtoD = time.perf_counter()
+                # --- END TIMER 3 ---
+
+                # --- TIMER 4: GPU Compute (Core Splatting) ---
+                t_start_compute = time.perf_counter()
+
+                with torch.no_grad():
+                    right_video_tensor_raw, occlusion_mask_tensor = stereo_projector(
+                        left_video_tensor, disp_map_tensor
+                    )
+                    right_video_tensor = right_video_tensor_raw
+
+                right_video_numpy = right_video_tensor.cpu().permute(0, 2, 3, 1).numpy()
+                occlusion_mask_numpy = (
+                    occlusion_mask_tensor.cpu()
+                    .permute(0, 2, 3, 1)
+                    .numpy()
+                    .repeat(3, axis=-1)
+                )
+
+                t_end_transfer_DtoH = time.perf_counter()
+                # --- END TIMER 5 ---
+
+                # --- TIMER 6: FFmpeg Write (Blocking I/O) ---
+                t_start_write = time.perf_counter()
+
+                # --- START OF DIAGNOSTIC TEST ASSEMBLY ---
+                test_generated_this_task = False
+                if self.map_test_var.get() or self.splat_test_var.get():
+                    for j in range(len(batch_frames_numpy)):
+                        # Capture logic
+                        clean_base = (
+                            os.path.splitext(output_video_path_base)[0]
+                            .replace("_splatted2", "")
+                            .replace("_splatted4", "")
+                        )
+                        base_img_path = f"{clean_base}_{width}"
+
+                        # Emergency pump to ensure the 100% scale update_preview finished
+                        for _ in range(25):
+                            self.update_idletasks()
+                            self.update()
+                            if self.previewer.pil_image_for_preview is not None:
+                                break
+                            time.sleep(0.1)
+
+                        if self.previewer.pil_image_for_preview is None:
+                            logger.error(
+                                "Diagnostic Capture Failed: Preview image is missing."
+                            )
+                            return False
+
+                        preview_pil = self.previewer.pil_image_for_preview.convert(
+                            "RGB"
+                        )
+                        # Resize preview to match RENDER resolution for SBS parity
+                        preview_pil = preview_pil.resize(
+                            (width, height), Image.Resampling.LANCZOS
+                        )
+                        preview_side = np.array(preview_pil).astype(np.float32) / 255.0
+
+                        if self.map_test_var.get():
+                            # Render depth visualization (apply the same visualization-only TV->full expansion for 10-bit, when tagged 'tv')
+                            render_map_vis = batch_depth_vis[j]
+                            try:
+                                if DEPTH_VIS_APPLY_TV_RANGE_EXPANSION_10BIT:
+                                    bd = (
+                                        int(_infer_depth_bit_depth(depth_stream_info))
+                                        if depth_stream_info
+                                        else 8
+                                    )
+                                    rng = str(
+                                        (depth_stream_info or {}).get("color_range")
+                                        or (depth_stream_info or {}).get("range")
+                                        or ""
+                                    ).lower()
+                                    if bd >= 10 and rng == "tv":
+                                        render_map_vis = (
+                                            render_map_vis - DEPTH_VIS_TV10_BLACK_NORM
+                                        ) / (
+                                            DEPTH_VIS_TV10_WHITE_NORM
+                                            - DEPTH_VIS_TV10_BLACK_NORM
+                                        )
+                            except Exception:
+                                pass
+
+                            # Save labeled PREVIEW / RENDER single images
+                            try:
+                                prev_u8 = (np.clip(preview_side, 0, 1) * 255).astype(
+                                    np.uint8
+                                )
+                                prev_bgr = cv2.cvtColor(prev_u8, cv2.COLOR_RGB2BGR)
+                                cv2.putText(
+                                    prev_bgr,
+                                    "PREVIEW",
+                                    (10, 35),
+                                    cv2.FONT_HERSHEY_SIMPLEX,
+                                    1.2,
+                                    (0, 0, 255),
+                                    2,
+                                )
+                                cv2.imwrite(
+                                    f"{base_img_path}_map_test_preview.png", prev_bgr
+                                )
+
+                                rend_u8 = (np.clip(render_map_vis, 0, 1) * 255).astype(
+                                    np.uint8
+                                )
+                                rend_bgr = cv2.cvtColor(rend_u8, cv2.COLOR_RGB2BGR)
+                                cv2.putText(
+                                    rend_bgr,
+                                    "RENDER",
+                                    (10, 35),
+                                    cv2.FONT_HERSHEY_SIMPLEX,
+                                    1.2,
+                                    (0, 0, 255),
+                                    2,
+                                )
+                                cv2.imwrite(
+                                    f"{base_img_path}_map_test_render.png", rend_bgr
+                                )
+                            except Exception:
+                                pass
+
+                            # SBS (kept for convenience)
+                            sbs_map = np.concatenate(
+                                [preview_side, render_map_vis], axis=1
+                            )
+                            sbs_map_uint8 = (np.clip(sbs_map, 0, 1) * 255).astype(
+                                np.uint8
+                            )
+                            sbs_map_bgr = cv2.cvtColor(sbs_map_uint8, cv2.COLOR_RGB2BGR)
+                            cv2.putText(
+                                sbs_map_bgr,
+                                "PREVIEW",
+                                (10, 35),
+                                cv2.FONT_HERSHEY_SIMPLEX,
+                                1.2,
+                                (0, 0, 255),
+                                2,
+                            )
+                            cv2.putText(
+                                sbs_map_bgr,
+                                "RENDER",
+                                (width + 10, 35),
+                                cv2.FONT_HERSHEY_SIMPLEX,
+                                1.2,
+                                (0, 0, 255),
+                                2,
+                            )
+                            cv2.imwrite(f"{base_img_path}_map_test.png", sbs_map_bgr)
+                            logger.info(
+                                f"Saved Map Test: {os.path.basename(base_img_path)}_map_test.png"
+                            )
+
+                        if self.splat_test_var.get():
+                            # Save labeled PREVIEW / RENDER single images
+                            try:
+                                prev_u8 = (np.clip(preview_side, 0, 1) * 255).astype(
+                                    np.uint8
+                                )
+                                prev_bgr = cv2.cvtColor(prev_u8, cv2.COLOR_RGB2BGR)
+                                cv2.putText(
+                                    prev_bgr,
+                                    "PREVIEW",
+                                    (10, 35),
+                                    cv2.FONT_HERSHEY_SIMPLEX,
+                                    1.2,
+                                    (0, 0, 255),
+                                    2,
+                                )
+                                cv2.imwrite(
+                                    f"{base_img_path}_splat_test_preview.png", prev_bgr
+                                )
+
+                                rend_u8 = (
+                                    np.clip(right_video_numpy[j], 0, 1) * 255
+                                ).astype(np.uint8)
+                                rend_bgr = cv2.cvtColor(rend_u8, cv2.COLOR_RGB2BGR)
+                                cv2.putText(
+                                    rend_bgr,
+                                    "RENDER",
+                                    (10, 35),
+                                    cv2.FONT_HERSHEY_SIMPLEX,
+                                    1.2,
+                                    (0, 0, 255),
+                                    2,
+                                )
+                                cv2.imwrite(
+                                    f"{base_img_path}_splat_test_render.png", rend_bgr
+                                )
+                            except Exception:
+                                pass
+
+                            # SBS (kept for convenience)
+                            sbs_splat = np.concatenate(
+                                [preview_side, right_video_numpy[j]], axis=1
+                            )
+                            sbs_splat_uint8 = (np.clip(sbs_splat, 0, 1) * 255).astype(
+                                np.uint8
+                            )
+                            sbs_splat_bgr = cv2.cvtColor(
+                                sbs_splat_uint8, cv2.COLOR_RGB2BGR
+                            )
+                            cv2.putText(
+                                sbs_splat_bgr,
+                                "PREVIEW",
+                                (10, 35),
+                                cv2.FONT_HERSHEY_SIMPLEX,
+                                1.2,
+                                (0, 0, 255),
+                                2,
+                            )
+                            cv2.putText(
+                                sbs_splat_bgr,
+                                "RENDER",
+                                (width + 10, 35),
+                                cv2.FONT_HERSHEY_SIMPLEX,
+                                1.2,
+                                (0, 0, 255),
+                                2,
+                            )
+                            cv2.imwrite(
+                                f"{base_img_path}_splat_test.png", sbs_splat_bgr
+                            )
+                            logger.info(
+                                f"Saved Splat Test: {os.path.basename(base_img_path)}_splat_test.png"
+                            )
+
+                        test_generated_this_task = True
+                        break  # Only process one frame for images
+
+                    if test_generated_this_task:
+                        return True  # Exit early: NO VIDEO encoding occurs
+
+                # --- STANDARD VIDEO ENCODING (Only reached if NO tests enabled) ---
+                for j in range(len(batch_frames_numpy)):
+                    if dual_output:
+                        video_grid = np.concatenate(
+                            [occlusion_mask_numpy[j], right_video_numpy[j]], axis=1
+                        )
+                    else:
+                        video_grid_top = np.concatenate(
+                            [batch_frames_float[j], batch_depth_vis[j]], axis=1
+                        )
+                        video_grid_bottom = np.concatenate(
+                            [occlusion_mask_numpy[j], right_video_numpy[j]], axis=1
+                        )
+                        video_grid = np.concatenate(
+                            [video_grid_top, video_grid_bottom], axis=0
+                        )
+
+                    video_grid_uint16 = (
+                        np.clip(video_grid, 0.0, 1.0) * 65535.0
+                    ).astype(np.uint16)
+                    ffmpeg_process.stdin.write(
+                        cv2.cvtColor(video_grid_uint16, cv2.COLOR_RGB2BGR).tobytes()
+                    )
+                    frame_count += 1
+                # --- END OF FRAME ASSEMBLY ---
+
+                t_end_write = time.perf_counter()
+                # --- END TIMER 6 ---
+
+                del (
+                    left_video_tensor,
+                    disp_map_tensor,
+                    right_video_tensor,
+                    occlusion_mask_tensor,
+                )
+                torch.cuda.empty_cache()
+                draw_progress_bar(frame_count, num_frames, prefix=f"  Encoding:")
+
+                t_end_batch = time.perf_counter()  # <--- TIMER END: Total Batch
+
+                # --- LOG RESULTS: Conditionally log at DEBUG level ---
+                if logger.isEnabledFor(logging.DEBUG):
+                    batch_size_actual = len(current_frame_indices)
+                    task_tag = "LowRes" if is_low_res_task else "HiRes"
+
+                    io_time = t_end_io - t_start_io
+                    preproc_time = t_end_preproc - t_start_preproc
+                    htod_time = t_end_transfer_HtoD - t_start_transfer_HtoD
+                    compute_time = t_end_compute - t_start_compute
+                    dtoh_time = t_end_transfer_DtoH - t_start_transfer_DtoH
+                    write_time = t_end_write - t_start_write
+                    total_batch_time = t_end_batch - t_start_batch
+
+                    logger.info(
+                        f"[{task_tag} Batch {i // batch_size_actual + 1}] Frames={batch_size_actual} Total={total_batch_time * 1000:.0f}ms | "
+                        f"IO={io_time * 1000:.0f}ms | CPU_Proc={preproc_time * 1000:.0f}ms | HtoD={htod_time * 1000:.0f}ms | "
+                        f"GPU_Comp={compute_time * 1000:.0f}ms | DtoH={dtoh_time * 1000:.0f}ms | FFmpeg_Write={write_time * 1000:.0f}ms"
+                    )
+                # --- END LOG RESULTS ---
+
+        except (IOError, BrokenPipeError) as e:
+            logger.error(f"FFmpeg pipe error: {e}. Encoding may have failed.")
+            encoding_successful = False
+        finally:
+            del stereo_projector
+            torch.cuda.empty_cache()
+            gc.collect()
+
+            # --- Finalize FFmpeg process ---
+            if ffmpeg_process is not None:
+                if ffmpeg_process.stdin:
+                    try:
+                        ffmpeg_process.stdin.close()  # Close the pipe to signal end of input
+                    finally:
+                        ffmpeg_process.stdin = None   # <-- IMPORTANT: prevent communicate() flushing closed stdin
+
+                # Wait for the process to finish and get output
+                stdout, stderr = ffmpeg_process.communicate(timeout=120)
+
+                if self.stop_event.is_set():
+                    ffmpeg_process.terminate()
+                    logger.warning(
+                        f"FFmpeg encoding stopped by user for {os.path.basename(final_output_video_path)}."
+                    )
+                    encoding_successful = False
+                elif ffmpeg_process.returncode != 0:
+                    logger.error(
+                        f"FFmpeg encoding failed for {os.path.basename(final_output_video_path)} (return code {ffmpeg_process.returncode}):\n{stderr.decode()}"
+                    )
+                    encoding_successful = False
+                else:
+                    logger.info(
+                        f"Successfully encoded video to {final_output_video_path}"
+                    )
+                    logger.debug(f"FFmpeg stderr log:\n{stderr.decode()}")
+            else:
+                # Test mode: no encoding step was started, so nothing to finalize.
+                pass
+
+        if not encoding_successful:
+            return False
+
+        # --- Check for Low-Res Task BEFORE writing sidecar ---
+        if is_low_res_task:
+            # --- Write sidecar JSON after successful encoding ---
+            output_sidecar_data = {}
+
+            # Check and include frame_overlap and input_bias
+            has_non_zero_setting = False
+
+            if input_bias is not None and input_bias != 0.0:
+                output_sidecar_data["input_bias"] = input_bias
+                has_non_zero_setting = True
+
+            # Use the combined condition: non-zero setting AND is low-res
+            if has_non_zero_setting:
+                sidecar_ext = self.APP_CONFIG_DEFAULTS.get(
+                    "OUTPUT_SIDECAR_EXT", ".spsidecar"
+                )
+                output_sidecar_path = (
+                    f"{os.path.splitext(final_output_video_path)[0]}{sidecar_ext}"
+                )
+                try:
+                    with open(output_sidecar_path, "w", encoding="utf-8") as f:
+                        json.dump(output_sidecar_data, f, indent=4)
+                    logger.info(f"Created output sidecar file: {output_sidecar_path}")
+                except Exception as e:
+                    logger.error(
+                        f"Error creating output sidecar file '{output_sidecar_path}': {e}"
+                    )
+            else:
+                logger.debug(
+                    "Skipping output sidecar creation: frame_overlap and input_bias are zero."
+                )
+        else:
+            logger.debug(
+                "Skipping output sidecar creation: High-resolution output does not require spsidecar."
+            )
+
+        return True
+
+    # _determine_auto_convergence definition removed (moved to earlier in file)
+
+    def exit_app(self):
+        """Handles application exit, including stopping the processing thread."""
+        self._save_config()
+        self.stop_event.set()
+        if self.processing_thread and self.processing_thread.is_alive():
+            logger.info("==> Waiting for processing thread to finish...")
+            # --- NEW: Cleanup previewer resources ---
+            if hasattr(self, "previewer"):
+                self.previewer.cleanup()
+            self.processing_thread.join(timeout=5.0)
+            if self.processing_thread.is_alive():
+                logger.debug("==> Thread did not terminate gracefully within timeout.")
+        release_cuda_memory()
+        self.destroy()
+
+    def _find_preview_sources_callback(self) -> list:
+        """
+        Callback for VideoPreviewer. Scans for matching source video and depth map pairs.
+        Handles both folder (batch) and file (single) input modes.
+        """
+        source_path = self.input_source_clips_var.get()
+        depth_raw_path = self.input_depth_maps_var.get()
+
+        if not source_path or not depth_raw_path:
+            logger.warning("Preview Scan Failed: Source or depth path is empty.")
+            return []
+
+        # ------------------------------------------------------------
+        # 1) SINGLE-FILE MODE (both are actual files)
+        # ------------------------------------------------------------
+        is_source_file = os.path.isfile(source_path)
+        is_depth_file = os.path.isfile(depth_raw_path)
+
+        if is_source_file and is_depth_file:
+            logger.debug(
+                f"Preview Scan: Single file mode detected. "
+                f"Source: {source_path}, Depth: {depth_raw_path}"
+            )
+            return [
+                {
+                    "source_video": source_path,
+                    "depth_map": depth_raw_path,
+                }
+            ]
+
+        # ------------------------------------------------------------
+        # 2) FOLDER / BATCH MODE
+        # ------------------------------------------------------------
+        if not os.path.isdir(source_path) or not os.path.isdir(depth_raw_path):
+            logger.error(
+                "Preview Scan Failed: Inputs must either be two files or two valid directories."
+            )
+            return []
+
+        source_folder = source_path
+        base_depth_folder = depth_raw_path
+
+        # Collect all source videos
+        video_extensions = ("*.mp4", "*.avi", "*.mov", "*.mkv")
+        source_videos = []
+        for ext in video_extensions:
+            source_videos.extend(glob.glob(os.path.join(source_folder, ext)))
+
+        if not source_videos:
+            logger.warning(f"No source videos found in folder: {source_folder}")
+            return []
+
+        video_source_list = []
+
+        # ------------------------------------------------------------
+        # 2A) MULTI-MAP PREVIEW: search all map subfolders
+        # ------------------------------------------------------------
+        if self.multi_map_var.get():
+            depth_candidate_folders = []
+
+            # Treat each subdirectory (except 'sidecars') as a map folder
+            try:
+                for entry in os.listdir(base_depth_folder):
+                    full_sub = os.path.join(base_depth_folder, entry)
+                    if os.path.isdir(full_sub) and entry.lower() != "sidecars":
+                        depth_candidate_folders.append(full_sub)
+            except FileNotFoundError:
+                logger.error(
+                    f"Preview Scan Failed: Depth folder not found: {base_depth_folder}"
+                )
+                return []
+
+            if not depth_candidate_folders:
+                logger.warning(
+                    f"Preview Scan: No map subfolders found in Multi-Map base folder: {base_depth_folder}"
+                )
+
+            for video_path in sorted(source_videos):
+                base_name = os.path.splitext(os.path.basename(video_path))[0]
+                matched = False
+
+                for dpath in depth_candidate_folders:
+                    mp4 = os.path.join(dpath, f"{base_name}_depth.mp4")
+                    npz = os.path.join(dpath, f"{base_name}_depth.npz")
+
+                    if os.path.exists(mp4):
+                        video_source_list.append(
+                            {
+                                "source_video": video_path,
+                                "depth_map": mp4,
+                            }
+                        )
+                        matched = True
+                        break
+                    elif os.path.exists(npz):
+                        video_source_list.append(
+                            {
+                                "source_video": video_path,
+                                "depth_map": npz,
+                            }
+                        )
+                        matched = True
+                        break
+
+                if not matched:
+                    logger.debug(
+                        f"Preview Scan: No depth map found in any map folder for '{base_name}'."
+                    )
+
+        # ------------------------------------------------------------
+        # 2B) NORMAL MODE PREVIEW: single depth folder
+        # ------------------------------------------------------------
+        else:
+            depth_folder = base_depth_folder
+
+            for video_path in sorted(source_videos):
+                base_name = os.path.splitext(os.path.basename(video_path))[0]
+
+                candidates = [
+                    os.path.join(depth_folder, f"{base_name}_depth.mp4"),
+                    os.path.join(depth_folder, f"{base_name}_depth.npz"),
+                    os.path.join(depth_folder, f"{base_name}.mp4"),
+                    os.path.join(depth_folder, f"{base_name}.npz"),
+                ]
+
+                matching_depth_path = None
+                for dp in candidates:
+                    if os.path.exists(dp):
+                        matching_depth_path = dp
+                        break
+
+                if matching_depth_path:
+                    logger.debug(f"Preview Scan: Found pair for '{base_name}'.")
+                    video_source_list.append(
+                        {
+                            "source_video": video_path,
+                            "depth_map": matching_depth_path,
+                        }
+                    )
+
+        if not video_source_list:
+            logger.warning("Preview Scan: No matching source/depth pairs found.")
+        else:
+            logger.info(
+                f"Preview Scan: Found {len(video_source_list)} matching source/depth pairs."
+            )
+
+        # Ensure preview-only overlay toggles are applied on Refresh/Reload as well.
+        self._apply_preview_overlay_toggles()
+        return video_source_list
+
+    def _get_current_config(self):
+        """Collects all current GUI variable values into a single dictionary."""
+        config = {
+            # Folder Configurations
+            "input_source_clips": self.input_source_clips_var.get(),
+            "input_depth_maps": self.input_depth_maps_var.get(),
+            "output_splatted": self.output_splatted_var.get(),
+            "dark_mode_enabled": self.dark_mode_var.get(),
+            "window_width": self.winfo_width(),
+            "window_height": self.winfo_height(),
+            "window_x": self.winfo_x(),
+            "window_y": self.winfo_y(),
+            "update_slider_from_sidecar": self.update_slider_from_sidecar_var.get(),
+            "auto_save_sidecar": self.auto_save_sidecar_var.get(),
+            "multi_map_enabled": self.multi_map_var.get(),
+            "skip_lowres_preproc": self.skip_lowres_preproc_var.get(),
+            "crosshair_enabled": self.crosshair_enabled_var.get(),
+            "crosshair_white": self.crosshair_white_var.get(),
+            "crosshair_multi": self.crosshair_multi_var.get(),
+            "depth_pop_enabled": self.depth_pop_enabled_var.get(),
+            "debug_logging_enabled": self.debug_logging_var.get(),
+            "loop_playback": bool(
+                getattr(
+                    getattr(self, "previewer", None),
+                    "loop_playback_var",
+                    tk.BooleanVar(value=False),
+                ).get()
+            ),
+            "enable_full_resolution": self.enable_full_res_var.get(),
+            "batch_size": self.batch_size_var.get(),
+            "enable_low_resolution": self.enable_low_res_var.get(),
+            "pre_res_width": self.pre_res_width_var.get(),
+            "pre_res_height": self.pre_res_height_var.get(),
+            "low_res_batch_size": self.low_res_batch_size_var.get(),
+            "depth_dilate_size_x": self.depth_dilate_size_x_var.get(),
+            "depth_dilate_size_y": self.depth_dilate_size_y_var.get(),
+            "depth_blur_size_x": self.depth_blur_size_x_var.get(),
+            "depth_blur_size_y": self.depth_blur_size_y_var.get(),
+            "depth_dilate_left": self.depth_dilate_left_var.get(),
+            "depth_blur_left": self.depth_blur_left_var.get(),
+            "depth_blur_left_mix": self.depth_blur_left_mix_var.get(),
+            "preview_size": self.preview_size_var.get(),
+            "preview_source": self.preview_source_var.get(),
+            "process_length": self.process_length_var.get(),
+            "output_crf": self.output_crf_full_var.get(),  # legacy
+            "output_crf_full": self.output_crf_full_var.get(),
+            "output_crf_low": self.output_crf_low_var.get(),
+            "color_tags_mode": getattr(
+                self, "color_tags_mode_var", tk.StringVar(value="Auto")
+            ).get(),
+            "dual_output": self.dual_output_var.get(),
+            "auto_convergence_mode": self.auto_convergence_mode_var.get(),
+            "depth_gamma": self.depth_gamma_var.get(),
+            "max_disp": self.max_disp_var.get(),
+            "convergence_point": self.zero_disparity_anchor_var.get(),
+            "enable_global_norm": self.enable_global_norm_var.get(),  # Renamed
+            "move_to_finished": self.move_to_finished_var.get(),
+        }
+        # Avoid persisting test-forced preview settings
+        try:
+            test_active = bool(self.splat_test_var.get()) or bool(
+                self.map_test_var.get()
+            )
+        except Exception:
+            test_active = False
+        if test_active:
+            config["preview_source"] = self.app_config.get(
+                "preview_source", "Splat Result"
+            )
+            config["preview_size"] = self.app_config.get("preview_size", "75%")
+
+        return config
+
+    def get_current_preview_settings(self) -> dict:
+        """Gathers settings from the GUI needed for the preview callback."""
+        try:
+            settings = {
+                "max_disp": float(self.max_disp_var.get()),
+                "convergence_point": float(self.zero_disparity_anchor_var.get()),
+                "depth_gamma": float(self.depth_gamma_var.get()),
+                "depth_dilate_size_x": self._safe_float(self.depth_dilate_size_x_var),
+                "depth_dilate_size_y": self._safe_float(self.depth_dilate_size_y_var),
+                "depth_blur_size_x": self._safe_float(self.depth_blur_size_x_var),
+                "depth_blur_size_y": self._safe_float(self.depth_blur_size_y_var),
+                "depth_dilate_left": self._safe_float(self.depth_dilate_left_var),
+                "depth_blur_left": self._safe_float(self.depth_blur_left_var),
+                "depth_blur_left_mix": self._safe_float(self.depth_blur_left_mix_var),
+                "preview_size": self.preview_size_var.get(),
+                "preview_source": self.preview_source_var.get(),
+                "enable_global_norm": self.enable_global_norm_var.get(),
+            }
+
+            # Resolve Border Percentages based on Mode
+            mode = self.border_mode_var.get()
+            l_pct, r_pct = 0.0, 0.0
+
+            if mode == "Auto Basic":
+                # uses Trace-updated border_width_var
+                w = self._safe_float(self.border_width_var)
+                l_pct = w
+                r_pct = w
+            elif mode == "Auto Adv.":
+                l_pct = self._safe_float(self.auto_border_L_var)
+                r_pct = self._safe_float(self.auto_border_R_var)
+            elif mode == "Manual":
+                w = self._safe_float(self.border_width_var)
+                b = self._safe_float(self.border_bias_var)
+                if b <= 0:
+                    l_pct = w
+                    r_pct = w * (1.0 + b)
+                else:
+                    r_pct = w
+                    l_pct = w * (1.0 - b)
+            # Off mode stays 0.0, 0.0
+
+            settings["left_border_pct"] = l_pct
+            settings["right_border_pct"] = r_pct
+
+            return settings
+
+        except (ValueError, tk.TclError) as e:
+            logger.error(f"Invalid preview setting value: {e}")
+            return None
+
+    def _get_current_sidecar_paths_and_data(self) -> Optional[Tuple[str, str, dict]]:
+        """Helper to get current file path, sidecar path, and existing data (merged with defaults)."""
+        if (
+            not hasattr(self, "previewer")
+            or not self.previewer.video_list
+            or self.previewer.current_video_index == -1
+        ):
+            return None
+
+        current_index = self.previewer.current_video_index
+        depth_map_path = self.previewer.video_list[current_index].get("depth_map")
+
+        if not depth_map_path:
+            return None
+
+        depth_map_basename = os.path.splitext(os.path.basename(depth_map_path))[0]
+        sidecar_ext = self.APP_CONFIG_DEFAULTS["SIDECAR_EXT"]
+        # Use base folder for sidecars when Multi-Map is enabled
+        sidecar_folder = self._get_sidecar_base_folder()
+        json_sidecar_path = os.path.join(
+            sidecar_folder, f"{depth_map_basename}{sidecar_ext}"
+        )
+
+        # Load existing data (merged with defaults) to preserve non-GUI parameters like overlap/bias
+        current_data = self.sidecar_manager.load_sidecar_data(json_sidecar_path)
+
+        return json_sidecar_path, depth_map_path, current_data
+
+    def _get_defined_tasks(self, settings):
+        """Helper to return a list of processing tasks based on GUI settings."""
+        processing_tasks = []
+        if settings["enable_full_resolution"]:
+            processing_tasks.append(
+                {
+                    "name": "Full-Resolution",
+                    "output_subdir": "hires",
+                    "set_pre_res": False,
+                    "target_width": -1,
+                    "target_height": -1,
+                    "batch_size": settings["full_res_batch_size"],
+                    "is_low_res": False,
+                }
+            )
+        if settings["enable_low_resolution"]:
+            processing_tasks.append(
+                {
+                    "name": "Low-Resolution",
+                    "output_subdir": "lowres",
+                    "set_pre_res": True,
+                    "target_width": settings["low_res_width"],
+                    "target_height": settings["low_res_height"],
+                    "batch_size": settings["low_res_batch_size"],
+                    "is_low_res": True,
+                }
+            )
+        return processing_tasks
+
+    def _get_video_specific_settings(
+        self,
+        video_path,
+        input_depth_maps_path_setting,
+        default_zero_disparity_anchor,
+        gui_max_disp,
+        is_single_file_mode,
+    ):
+        """
+        Determine the actual depth map path and video-specific settings.
+
+        Behavior in Multi-Map mode:
+          * If a sidecar exists for this video and contains 'selected_depth_map',
+            that subfolder is used for the depth map lookup.
+          * Otherwise, we fall back to the map selected in the GUI when Start was pressed.
+        """
+        video_name = os.path.splitext(os.path.basename(video_path))[0]
+        base_name = video_name
+
+        # ------------------------------------------------------------------
+        # 1) Locate sidecar for this video (if any)
+        # ------------------------------------------------------------------
+        sidecar_ext = self.APP_CONFIG_DEFAULTS["SIDECAR_EXT"]
+        sidecar_folder = self._get_sidecar_base_folder()
+        json_sidecar_path = os.path.join(
+            sidecar_folder, f"{video_name}_depth{sidecar_ext}"
+        )
+
+        merged_config = None
+        sidecar_exists = False
+        selected_map_for_video = None
+
+        if os.path.exists(json_sidecar_path):
+            try:
+                merged_config = (
+                    self.sidecar_manager.load_sidecar_data(json_sidecar_path) or {}
+                )
+                sidecar_exists = True
+            except Exception as e:
+                logger.error(f"Failed to load sidecar for {video_name}: {e}")
+                merged_config = None
+
+            if isinstance(merged_config, dict):
+                selected_map_for_video = merged_config.get("selected_depth_map") or None
+
+        # ------------------------------------------------------------------
+        # 2) GUI defaults used when sidecar is missing or incomplete
+        # ------------------------------------------------------------------
+        gui_config = {
+            "convergence_plane": float(default_zero_disparity_anchor),
+            "max_disparity": float(gui_max_disp),
+            "gamma": float(
+                self.depth_gamma_var.get() or self.APP_CONFIG_DEFAULTS["DEPTH_GAMMA"]
+            ),
+        }
+
+        # ------------------------------------------------------------------
+        # 3) Resolve per-video depth map path
+        # ------------------------------------------------------------------
+
+        base_folder = input_depth_maps_path_setting
+        video_name = os.path.splitext(os.path.basename(video_path))[0]
+        actual_depth_map_path = None
+
+        # --- Single-file mode: depth path setting is the actual file ---
+        if is_single_file_mode:
+            # Here input_depth_maps_path_setting is expected to be the
+            # depth map *file* path, not a directory.
+            if os.path.isfile(base_folder):
+                actual_depth_map_path = base_folder
+                logger.info(
+                    f"Single-file mode: using depth map file '{actual_depth_map_path}'"
+                )
+                # Info panel: in Multi-Map mode, still show the clip's sidecar-selected map (if any).
+                _mm_map_info = "Direct file"
+                if self.multi_map_var.get() and selected_map_for_video:
+                    _mm_map_info = f"{selected_map_for_video} (Sidecar)"
+                    logger.info(
+                        f"[MM] USING sidecar map '{selected_map_for_video}' for '{video_name}'"
+                    )
+                self.progress_queue.put(("update_info", {"map": _mm_map_info}))
+            else:
+                return {
+                    "error": (
+                        f"Single-file mode: depth map file '{base_folder}' does not exist."
+                    )
+                }
+
+        # --- Batch / folder mode ---
+        else:
+            #
+            # MULTI-MAP MODE
+            #
+            if self.multi_map_var.get():
+                # 1) First try sidecar‚Äôs selected map for this video
+                sidecar_map = self._get_sidecar_selected_map_for_video(video_path)
+
+                if sidecar_map:
+                    candidate_dir = os.path.join(base_folder, sidecar_map)
+                    c_mp4 = os.path.join(candidate_dir, f"{video_name}_depth.mp4")
+                    c_npz = os.path.join(candidate_dir, f"{video_name}_depth.npz")
+
+                    if os.path.exists(c_mp4):
+                        actual_depth_map_path = c_mp4
+                    elif os.path.exists(c_npz):
+                        actual_depth_map_path = c_npz
+
+                    if actual_depth_map_path:
+                        logger.info(
+                            f"[MM] USING sidecar map '{sidecar_map}' for '{video_name}'"
+                        )
+                        # Show map name PLUS source (Sidecar)
+                        self.progress_queue.put(
+                            ("update_info", {"map": f"{sidecar_map} (Sidecar)"})
+                        )
+                    else:
+                        logger.warning(
+                            f"[MM] sidecar map '{sidecar_map}' has no depth file for '{video_name}'"
+                        )
+
+                # 2) If sidecar FAILED, fall back to GUI-selected map
+                if not actual_depth_map_path:
+                    gui_map = self.selected_depth_map_var.get().strip()
+                    if gui_map:
+                        candidate_dir = os.path.join(base_folder, gui_map)
+                        c_mp4 = os.path.join(candidate_dir, f"{video_name}_depth.mp4")
+                        c_npz = os.path.join(candidate_dir, f"{video_name}_depth.npz")
+
+                        if os.path.exists(c_mp4):
+                            actual_depth_map_path = c_mp4
+                        elif os.path.exists(c_npz):
+                            actual_depth_map_path = c_npz
+
+                        if actual_depth_map_path:
+                            logger.info(
+                                f"[MM] USING GUI map '{gui_map}' for '{video_name}'"
+                            )
+                            # Show map name PLUS source (GUI/Default)
+                            self.progress_queue.put(
+                                ("update_info", {"map": f"{gui_map} (GUI/Default)"})
+                            )
+
+                # 3) Absolute hard fallback: look in base folder
+                if not actual_depth_map_path:
+                    c_mp4 = os.path.join(base_folder, f"{video_name}_depth.mp4")
+                    c_npz = os.path.join(base_folder, f"{video_name}_depth.npz")
+                    if os.path.exists(c_mp4):
+                        actual_depth_map_path = c_mp4
+                    elif os.path.exists(c_npz):
+                        actual_depth_map_path = c_npz
+
+                if not actual_depth_map_path:
+                    return {
+                        "error": f"No depth map for '{video_name}' in ANY multimap source"
+                    }
+
+            #
+            # NORMAL (non-multi-map) MODE
+            #
+            else:
+                # Here base_folder is expected to be a directory containing all depth maps.
+                c_mp4 = os.path.join(base_folder, f"{video_name}_depth.mp4")
+                c_npz = os.path.join(base_folder, f"{video_name}_depth.npz")
+
+                if os.path.exists(c_mp4):
+                    actual_depth_map_path = c_mp4
+                elif os.path.exists(c_npz):
+                    actual_depth_map_path = c_npz
+                else:
+                    return {"error": f"No depth for '{video_name}' in '{base_folder}'"}
+
+        actual_depth_map_path = os.path.normpath(actual_depth_map_path)
+
+        # ------------------------------------------------------------
+        # Multi-Map: resolve map folder from sidecar per-video
+        # ------------------------------------------------------------
+        depth_map_path = None
+
+        if self.multi_map_var.get():
+            # new helper we already added earlier
+            selected_map = self._get_sidecar_selected_map_for_video(video_path)
+
+            if selected_map:
+                candidate_folder = os.path.join(
+                    self.input_depth_maps_var.get(), selected_map
+                )
+                candidate_mp4 = os.path.join(candidate_folder, f"{base_name}_depth.mp4")
+                candidate_npz = os.path.join(candidate_folder, f"{base_name}_depth.npz")
+
+                if os.path.exists(candidate_mp4):
+                    depth_map_path = candidate_mp4
+                elif os.path.exists(candidate_npz):
+                    depth_map_path = candidate_npz
+
+        # ------------------------------------------------------------------
+        # 4) Build merged settings (sidecar values with GUI defaults)
+        # ------------------------------------------------------------------
+        if not merged_config or not isinstance(merged_config, dict):
+            merged_config = {
+                "convergence_plane": gui_config["convergence_plane"],
+                "max_disparity": gui_config["max_disparity"],
+                "gamma": gui_config["gamma"],
+                "input_bias": 0.0,
+            }
+
+        # Determine map source label for Multi-Map status display
+        if self.multi_map_var.get():
+            map_source = "Sidecar" if sidecar_exists else "GUI/Default"
+        else:
+            map_source = "N/A"
+
+        # --- NEW: Determine Global Normalization Policy ---
+        enable_global_norm_policy = self.enable_global_norm_var.get()
+        if sidecar_exists:
+            # Policy: If a sidecar exists, GN is DISABLED (manual mode)
+            enable_global_norm_policy = False
+            logger.debug(f"GN Policy: Sidecar exists for {video_name}. GN forced OFF.")
+
+        # Determine the source for GN info
+        gn_source = (
+            "Sidecar"
+            if sidecar_exists
+            else ("GUI/ON" if enable_global_norm_policy else "GUI/OFF")
+        )
+
+        settings = {
+            "actual_depth_map_path": actual_depth_map_path,
+            "convergence_plane": merged_config.get(
+                "convergence_plane", gui_config["convergence_plane"]
+            ),
+            "max_disparity_percentage": merged_config.get(
+                "max_disparity", gui_config["max_disparity"]
+            ),
+            "input_bias": merged_config.get("input_bias"),
+            "depth_gamma": merged_config.get("gamma", gui_config["gamma"]),
+            # GUI-derived depth pre-processing settings
+            "depth_dilate_size_x": float(self.depth_dilate_size_x_var.get()),
+            "depth_dilate_size_y": float(self.depth_dilate_size_y_var.get()),
+            "depth_blur_size_x": int(float(self.depth_blur_size_x_var.get())),
+            "depth_blur_size_y": int(float(self.depth_blur_size_y_var.get())),
+            # Left-edge depth pre-processing defaults (must behave like other GUI settings)
+            "depth_dilate_left": float(self.depth_dilate_left_var.get()),
+            "depth_blur_left": int(round(float(self.depth_blur_left_var.get()))),
+            "depth_blur_left_mix": float(self.depth_blur_left_mix_var.get()),
+            # Tracking / info sources
+            "sidecar_found": sidecar_exists,
+            "anchor_source": "Sidecar" if sidecar_exists else "GUI/Default",
+            "max_disp_source": "Sidecar" if sidecar_exists else "GUI/Default",
+            "gamma_source": "Sidecar" if sidecar_exists else "GUI/Default",
+            "map_source": map_source,
+            "enable_global_norm": enable_global_norm_policy,
+            "gn_source": gn_source,
+        }
+
+        # If a sidecar exists and Sidecar Blur/Dilate is enabled, use its depth pre-processing values.
+        # (Render should respect sidecars for both Single and Batch processing.)
+        if (
+            sidecar_exists
+            and getattr(self, "enable_sidecar_blur_dilate_var", None)
+            and self.enable_sidecar_blur_dilate_var.get()
+        ):
+            try:
+                settings["depth_dilate_size_x"] = float(
+                    merged_config.get(
+                        "depth_dilate_size_x", settings["depth_dilate_size_x"]
+                    )
+                )
+                settings["depth_dilate_size_y"] = float(
+                    merged_config.get(
+                        "depth_dilate_size_y", settings["depth_dilate_size_y"]
+                    )
+                )
+                settings["depth_blur_size_x"] = int(
+                    float(
+                        merged_config.get(
+                            "depth_blur_size_x", settings["depth_blur_size_x"]
+                        )
+                    )
+                )
+                settings["depth_blur_size_y"] = int(
+                    float(
+                        merged_config.get(
+                            "depth_blur_size_y", settings["depth_blur_size_y"]
+                        )
+                    )
+                )
+                settings["depth_dilate_left"] = float(
+                    merged_config.get(
+                        "depth_dilate_left", settings["depth_dilate_left"]
+                    )
+                )
+                settings["depth_blur_left"] = int(
+                    float(
+                        merged_config.get(
+                            "depth_blur_left", settings["depth_blur_left"]
+                        )
+                    )
+                )
+                settings["depth_blur_left_mix"] = float(
+                    merged_config.get(
+                        "depth_blur_left_mix", settings["depth_blur_left_mix"]
+                    )
+                )
+            except Exception:
+                pass
+
+        # If no sidecar file exists at all, enforce GUI values explicitly
+        if not sidecar_exists:
+            settings["convergence_plane"] = gui_config["convergence_plane"]
+            settings["max_disparity_percentage"] = gui_config["max_disparity"]
+            settings["depth_gamma"] = gui_config["gamma"]
+
+        return settings
+
+    def _initialize_video_and_depth_readers(
+        self,
+        video_path,
+        actual_depth_map_path,
+        process_length,
+        task_settings,
+        match_depth_res,
+    ):
+        """
+        Initializes VideoReader objects for source video and depth map,
+        and returns their metadata.
+        Returns: (video_reader, depth_reader, processed_fps, original_vid_h, original_vid_w, current_processed_height, current_processed_width,
+                  video_stream_info, total_frames_input, total_frames_depth, actual_depth_height, actual_depth_width,
+                  depth_stream_info)
+        """
+        video_reader_input = None
+        processed_fps = 0.0
+        original_vid_h, original_vid_w = 0, 0
+        current_processed_height, current_processed_width = 0, 0
+        video_stream_info = None
+        total_frames_input = 0
+
+        depth_reader_input = None
+        total_frames_depth = 0
+        actual_depth_height, actual_depth_width = 0, 0
+        depth_stream_info = None  # Initialize to None
+
+        try:
+            # 1. Initialize input video reader
+            (
+                video_reader_input,
+                processed_fps,
+                original_vid_h,
+                original_vid_w,
+                current_processed_height,
+                current_processed_width,
+                video_stream_info,
+                total_frames_input,
+            ) = read_video_frames(
+                video_path,
+                process_length,
+                set_pre_res=task_settings["set_pre_res"],
+                pre_res_width=task_settings["target_width"],
+                pre_res_height=task_settings["target_height"],
+            )
+        except Exception as e:
+            logger.error(
+                f"==> Error initializing input video reader for {os.path.basename(video_path)} {task_settings['name']} pass: {e}. Skipping this pass."
+            )
+            return (
+                None,
+                None,
+                0.0,
+                0,
+                0,
+                0,
+                0,
+                None,
+                0,
+                0,
+                0,
+                0,
+                None,
+            )  # Return None for depth_stream_info
+            # Determine map source for Multi-Map
+            map_display = "N/A"
+            if self.multi_map_var.get():
+                if self._current_video_sidecar_map:
+                    map_display = f"Sidecar > {self._current_video_sidecar_map}"
+                elif self.selected_depth_map_var.get():
+                    map_display = f"Default > {self.selected_depth_map_var.get()}"
+
+        self.progress_queue.put(
+            (
+                "update_info",
+                {
+                    "resolution": f"{current_processed_width}x{current_processed_height}",
+                    "frames": total_frames_input,
+                },
+            )
+        )
+
+        try:
+            # 2. Initialize depth maps reader and capture depth_stream_info
+            # For low-res rendering we need depth preprocessing parity with preview/full-res:
+            # preprocess at the original clip resolution, then downscale to the low-res splat size.
+            depth_target_h = current_processed_height
+            depth_target_w = current_processed_width
+            depth_match = match_depth_res
+            if task_settings.get("is_low_res"):
+                if (
+                    getattr(self, "skip_lowres_preproc_var", None) is not None
+                    and self.skip_lowres_preproc_var.get()
+                ):
+                    # Dev Tools: load depth directly at low-res and skip parity pre-proc path.
+                    depth_target_h = current_processed_height
+                    depth_target_w = current_processed_width
+                    depth_match = match_depth_res
+                else:
+                    # Default: load depth at clip resolution so low-res pre-processing matches full-res preview/render.
+                    depth_target_h = original_vid_h
+                    depth_target_w = original_vid_w
+                    depth_match = True
+
+            (
+                depth_reader_input,
+                total_frames_depth,
+                actual_depth_height,
+                actual_depth_width,
+                depth_stream_info,
+            ) = load_pre_rendered_depth(
+                actual_depth_map_path,
+                process_length=process_length,
+                target_height=depth_target_h,
+                target_width=depth_target_w,
+                match_resolution_to_target=depth_match,
+            )
+        except Exception as e:
+            logger.error(
+                f"==> Error initializing depth map reader for {os.path.basename(video_path)} {task_settings['name']} pass: {e}. Skipping this pass."
+            )
+            if video_reader_input:
+                del video_reader_input
+            return (
+                None,
+                None,
+                0.0,
+                0,
+                0,
+                0,
+                0,
+                None,
+                0,
+                0,
+                0,
+                0,
+                None,
+            )  # Return None for depth_stream_info
+
+        # CRITICAL CHECK: Ensure input video and depth map have consistent frame counts
+        if total_frames_input != total_frames_depth:
+            logger.error(
+                f"==> Frame count mismatch for {os.path.basename(video_path)} {task_settings['name']} pass: Input video has {total_frames_input} frames, Depth map has {total_frames_depth} frames. Skipping."
+            )
+            if video_reader_input:
+                del video_reader_input
+            if depth_reader_input:
+                del depth_reader_input
+            return (
+                None,
+                None,
+                0.0,
+                0,
+                0,
+                0,
+                0,
+                None,
+                0,
+                0,
+                0,
+                0,
+                None,
+            )  # Return None for depth_stream_info
+
+        return (
+            video_reader_input,
+            depth_reader_input,
+            processed_fps,
+            original_vid_h,
+            original_vid_w,
+            current_processed_height,
+            current_processed_width,
+            video_stream_info,
+            total_frames_input,
+            total_frames_depth,
+            actual_depth_height,
+            actual_depth_width,
+            depth_stream_info,
+        )
+
+    def _load_config(self):
+        """Loads configuration from default config file."""
+        config_filename = self.APP_CONFIG_DEFAULTS["DEFAULT_CONFIG_FILENAME"]
+        # --- MODIFIED: Use the new dictionary constant ---
+        if os.path.exists(config_filename):
+            try:
+                with open(config_filename, "r") as f:
+                    self.app_config = json.load(f)
+
+                # --- BACKWARD COMPATIBILITY FIX: Handle the old 'enable_autogain' key ---
+                # Old meaning: True = Raw Input / Disable Normalization (GN OFF)
+                # New meaning: True = Enable Global Normalization (GN ON)
+                if "enable_autogain" in self.app_config:
+                    old_value = self.app_config.pop("enable_autogain")  # Remove old key
+                    # New value is the inverse of the old value
+                    self.app_config["enable_global_norm"] = not bool(old_value)
+                # --- END FIX ---
+            except Exception as e:
+                logger.error(f"Failed to load config file: {e}. Using defaults.")
+                self.app_config = {}
+
+    def _load_help_texts(self):
+        """Loads help texts from a JSON file."""
+        try:
+            with open(
+                os.path.join("dependency", "splatter_help.json"), "r", encoding="utf-8"
+            ) as f:
+                self.help_texts = json.load(f)
+        except FileNotFoundError:
+            logger.error(
+                "Error: splatter_help.json not found. Tooltips will not be available."
+            )
+            self.help_texts = {}
+        except json.JSONDecodeError:
+            logger.error(
+                "Error: Could not decode splatter_help.json. Check file format."
+            )
+            self.help_texts = {}
+
+    def load_settings(self):
+        """Loads settings from a user-selected JSON file."""
+        filename = filedialog.askopenfilename(
+            defaultextension=".json",
+            filetypes=[("JSON files", "*.json")],
+            title="Load Settings from File",
+        )
+        if not filename:
+            return
+
+        try:
+            with open(filename, "r") as f:
+                loaded_config = json.load(f)
+            # Apply loaded config values to the variables
+            for (
+                config_key,
+                config_value,
+            ) in loaded_config.items():  # Iterate over loaded keys
+                # --- NEW MAPPING LOGIC ---
+                # Construct the expected name of the Tkinter variable
+                tk_var_attr_name = config_key + "_var"
+
+                if hasattr(self, tk_var_attr_name):
+                    tk_var_object = getattr(self, tk_var_attr_name)
+
+                    if isinstance(tk_var_object, tk.BooleanVar):
+                        # Ensure value is converted to a proper boolean/int before setting BooleanVar
+                        tk_var_object.set(bool(config_value))
+                    elif isinstance(tk_var_object, tk.StringVar):
+                        # Set StringVar directly
+                        tk_var_object.set(str(config_value))
+
+            # Apply loaded config values to the variables
+            for key, var in self.__dict__.items():
+                if key.endswith("_var") and key in loaded_config:
+                    # Logic to safely set values:
+                    # For tk.StringVar, set()
+                    # For tk.BooleanVar, use set() with the bool/int value
+                    if isinstance(var, tk.BooleanVar):
+                        var.set(bool(loaded_config[key]))
+                    elif isinstance(var, tk.StringVar):
+                        var.set(str(loaded_config[key]))
+
+            self._apply_theme()  # Re-apply theme in case dark mode setting was loaded
+            self.toggle_processing_settings_fields()  # Update state of dependent fields
+            messagebox.showinfo(
+                "Settings Loaded",
+                f"Successfully loaded settings from:\n{os.path.basename(filename)}",
+            )
+            self.status_label.config(text="Settings loaded.")
+
+        except Exception as e:
+            messagebox.showerror(
+                "Load Error",
+                f"Failed to load settings from {os.path.basename(filename)}:\n{e}",
+            )
+            self.status_label.config(text="Settings load failed.")
+
+    def _move_processed_files(
+        self,
+        video_path,
+        actual_depth_map_path,
+        finished_source_folder,
+        finished_depth_folder,
+    ):
+        """Moves source video, depth map, and its sidecar file to 'finished' folders."""
+        max_retries = 5
+        retry_delay_sec = 0.5  # Wait half a second between retries
+
+        # Move source video
+        if finished_source_folder:
+            dest_path_src = os.path.join(
+                finished_source_folder, os.path.basename(video_path)
+            )
+            for attempt in range(max_retries):
+                try:
+                    if os.path.exists(dest_path_src):
+                        logger.warning(
+                            f"File '{os.path.basename(video_path)}' already exists in '{finished_source_folder}'. Overwriting."
+                        )
+                        os.remove(dest_path_src)
+                    shutil.move(video_path, finished_source_folder)
+                    logger.debug(
+                        f"==> Moved processed video '{os.path.basename(video_path)}' to: {finished_source_folder}"
+                    )
+                    break
+                except PermissionError as e:
+                    logger.warning(
+                        f"Attempt {attempt + 1}/{max_retries}: PermissionError (file in use) when moving '{os.path.basename(video_path)}'. Retrying in {retry_delay_sec}s..."
+                    )
+                    time.sleep(retry_delay_sec)
+                except Exception as e:
+                    logger.error(
+                        f"==> Failed to move source video '{os.path.basename(video_path)}' to '{finished_source_folder}': {e}",
+                        exc_info=True,
+                    )
+                    break
+            else:
+                logger.error(
+                    f"==> Failed to move source video '{os.path.basename(video_path)}' after {max_retries} attempts due to PermissionError."
+                )
+        else:
+            logger.warning(
+                f"==> Cannot move source video '{os.path.basename(video_path)}': 'finished_source_folder' is not set (not in batch mode)."
+            )
+
+        # Move depth map and its sidecar file
+        if actual_depth_map_path and finished_depth_folder:
+            dest_path_depth = os.path.join(
+                finished_depth_folder, os.path.basename(actual_depth_map_path)
+            )
+            # --- Retry for Depth Map ---
+            for attempt in range(max_retries):
+                try:
+                    if os.path.exists(dest_path_depth):
+                        logger.warning(
+                            f"File '{os.path.basename(actual_depth_map_path)}' already exists in '{finished_depth_folder}'. Overwriting."
+                        )
+                        os.remove(dest_path_depth)
+                    shutil.move(actual_depth_map_path, finished_depth_folder)
+                    logger.debug(
+                        f"==> Moved depth map '{os.path.basename(actual_depth_map_path)}' to: {finished_depth_folder}"
+                    )
+                    break
+                except PermissionError as e:
+                    logger.warning(
+                        f"Attempt {attempt + 1}/{max_retries}: PermissionError (file in use) when moving depth map '{os.path.basename(actual_depth_map_path)}'. Retrying in {retry_delay_sec}s..."
+                    )
+                    time.sleep(retry_delay_sec)
+                except Exception as e:
+                    logger.error(
+                        f"==> Failed to move depth map '{os.path.basename(actual_depth_map_path)}' to '{finished_depth_folder}': {e}",
+                        exc_info=True,
+                    )
+                    break
+            else:
+                logger.error(
+                    f"==> Failed to move depth map '{os.path.basename(actual_depth_map_path)}' after {max_retries} attempts due to PermissionError."
+                )
+
+            # --- Retry for Sidecar file (if it exists) ---
+            depth_map_dirname = os.path.dirname(actual_depth_map_path)
+            depth_map_basename_without_ext = os.path.splitext(
+                os.path.basename(actual_depth_map_path)
+            )[0]
+            input_sidecar_ext = self.APP_CONFIG_DEFAULTS.get(
+                "SIDECAR_EXT", ".fssidecar"
+            )  # Fallback to .fssidecar
+
+            json_sidecar_path_to_move = os.path.join(
+                depth_map_dirname,
+                f"{depth_map_basename_without_ext}{input_sidecar_ext}",
+            )
+            dest_path_json = os.path.join(
+                finished_depth_folder,
+                f"{depth_map_basename_without_ext}{input_sidecar_ext}",
+            )
+
+            if os.path.exists(json_sidecar_path_to_move):
+                for attempt in range(max_retries):
+                    try:
+                        if os.path.exists(dest_path_json):
+                            logger.warning(
+                                f"Sidecar file '{os.path.basename(json_sidecar_path_to_move)}' already exists in '{finished_depth_folder}'. Overwriting."
+                            )
+                            os.remove(dest_path_json)
+                        shutil.move(json_sidecar_path_to_move, finished_depth_folder)
+                        logger.debug(
+                            f"==> Moved sidecar file '{os.path.basename(json_sidecar_path_to_move)}' to: {finished_depth_folder}"
+                        )
+                        break
+                    except PermissionError as e:
+                        logger.warning(
+                            f"Attempt {attempt + 1}/{max_retries}: PermissionError (file in use) when moving file '{os.path.basename(json_sidecar_path_to_move)}'. Retrying in {retry_delay_sec}s..."
+                        )
+                        time.sleep(retry_delay_sec)
+                    except Exception as e:
+                        logger.error(
+                            f"==> Failed to move sidecar file '{os.path.basename(json_sidecar_path_to_move)}' to '{finished_depth_folder}': {e}",
+                            exc_info=True,
+                        )
+                        break
+                else:
+                    logger.error(
+                        f"==> Failed to move sidecar file '{os.path.basename(json_sidecar_path_to_move)}' after {max_retries} attempts due to PermissionError."
+                    )
+            else:
+                logger.debug(
+                    f"==> No sidecar file '{json_sidecar_path_to_move}' found to move."
+                )
+        elif actual_depth_map_path:
+            logger.info(
+                f"==> Cannot move depth map '{os.path.basename(actual_depth_map_path)}': 'finished_depth_folder' is not set (not in batch mode)."
+            )
+
+    def on_auto_convergence_mode_select(self, event):
+        """
+        Handles selection in the Auto-Convergence combo box.
+        If a mode is selected, it checks the cache and runs the calculation if needed.
+        """
+        mode = self.auto_convergence_mode_var.get()
+
+        if mode == "Off":
+            # self._auto_conv_cache = {"Average": None, "Peak": None} # Clear cache on Off
+            return
+
+        if self._is_auto_conv_running:
+            logger.warning("Auto-Converge calculation is already running. Please wait.")
+            return
+
+        # Check cache logic
+        cached_value = None
+        if mode == "Hybrid":
+             if self._auto_conv_cache.get("Average") is not None and self._auto_conv_cache.get("Peak") is not None:
+                 cached_value = (self._auto_conv_cache["Average"] + self._auto_conv_cache["Peak"]) / 2.0
+        elif self._auto_conv_cache.get(mode) is not None:
+             cached_value = self._auto_conv_cache[mode]
+
+        if cached_value is not None:
+            # Value is cached, apply it immediately
+            
+            # 1. Set the Tkinter variable to the cached value (needed for the setter)
+            self.zero_disparity_anchor_var.set(f"{cached_value:.2f}")
+
+            # 2. Call the programmatic setter to update the slider position and its label
+            if self.set_convergence_value_programmatically:
+                try:
+                    self.set_convergence_value_programmatically(cached_value)
+                except Exception as e:
+                    logger.error(f"Error calling convergence setter on cache hit: {e}")
+
+            # 3. Update status label
+            self.status_label.config(
+                text=f"Auto-Converge ({mode}): Loaded cached value {cached_value:.2f}"
+            )
+
+            # 4. Refresh preview
+            self.on_slider_release(None)
+
+            return
+
+        # Cache miss, run the calculation (using the existing run_preview_auto_converge logic)
+        self.run_preview_auto_converge(force_run=True)
+
+    def on_slider_release(self, event=None):
+        """Called when a slider is released. Updates the preview with DEBOUNCING."""
+        # 1. Stop any current wigglegram animation immediately for responsiveness
+        if hasattr(self, "previewer"):
+            self.previewer._stop_wigglegram_animation()
+
+        # 2. Cancel any pending update timer (this is the "debounce" logic)
+        if self._preview_debounce_timer is not None:
+            self.after_cancel(self._preview_debounce_timer)
+            self._preview_debounce_timer = None
+
+        # 3. Start a new timer.
+        # 350ms is a good "norm" for responsiveness vs. stability.
+        # If you click 10 times quickly, this only fires after the 10th click.
+        self._preview_debounce_timer = self.after(
+            350, self._perform_delayed_preview_update
+        )
+
+    def _perform_delayed_preview_update(self):
+        """Actually triggers the heavy preview processing once the delay expires."""
+        self._preview_debounce_timer = None  # Clear timer reference
+
+        if hasattr(self, "previewer") and self.previewer.source_readers:
+            # Trigger the standard preview update
+            self.previewer.update_preview()
+
+            # IMPORTANT: Do not refresh the "Current Processing Information" panel here.
+            # This function is called by the slider debounce and would cause that panel to
+            # flash on every slider move. Clip/state + info should update only when the clip changes.
+
+    def _process_depth_batch(
+        self,
+        batch_depth_numpy_raw: np.ndarray,
+        depth_stream_info: Optional[dict],
+        depth_gamma: float,
+        depth_dilate_size_x: float,
+        depth_dilate_size_y: float,
+        depth_blur_size_x: float,
+        depth_blur_size_y: float,
+        is_low_res_task: bool,
+        max_raw_value: float,
+        global_depth_min: float,
+        global_depth_max: float,
+        depth_dilate_left: float = 0.0,
+        depth_blur_left: float = 0.0,
+        debug_batch_index: int = 0,
+        debug_frame_index: int = 0,
+        debug_task_name: str = "PreProcess",
+    ) -> np.ndarray:
+        """
+        Unified depth processor. Pre-processes filters in float space.
+        Gamma is now unified to occur in normalized space.
+        """
+        if batch_depth_numpy_raw.ndim == 4 and batch_depth_numpy_raw.shape[-1] == 3:
+            batch_depth_numpy = batch_depth_numpy_raw.mean(axis=-1)
+        else:
+            batch_depth_numpy = (
+                batch_depth_numpy_raw.squeeze(-1)
+                if batch_depth_numpy_raw.ndim == 4
+                else batch_depth_numpy_raw
+            )
+
+        batch_depth_numpy_float = batch_depth_numpy.astype(np.float32)
+
+        # Dev Tools: allow skipping ALL low-res preprocessing (gamma/dilate/blur)
+        if (
+            is_low_res_task
+            and getattr(self, "skip_lowres_preproc_var", None) is not None
+            and self.skip_lowres_preproc_var.get()
+        ):
+            return batch_depth_numpy_float
+
+        # Apply Filters BEFORE Gamma (Standard pipeline)
+        current_width = (
+            batch_depth_numpy_raw.shape[2]
+            if batch_depth_numpy_raw.ndim == 4
+            else batch_depth_numpy_raw.shape[1]
+        )
+        res_scale = math.sqrt(current_width / 960.0)
+
+        def map_val(v):
+            f_v = float(v)
+            # Backward compatibility: older configs stored erosion as 30..40 => -0..-10
+            if f_v > 30.0 and f_v <= 40.0:
+                return -(f_v - 30.0)
+            return f_v
+
+        render_dilate_x = map_val(depth_dilate_size_x) * res_scale
+        render_dilate_y = map_val(depth_dilate_size_y) * res_scale
+        render_blur_x = depth_blur_size_x * res_scale
+        render_blur_y = depth_blur_size_y * res_scale
+        render_dilate_left = float(depth_dilate_left) * res_scale
+        render_blur_left = float(depth_blur_left) * res_scale
+
+        if (
+            abs(render_dilate_left) > 1e-5
+            or render_blur_left > 0
+            or abs(render_dilate_x) > 1e-5
+            or abs(render_dilate_y) > 1e-5
+            or render_blur_x > 0
+            or render_blur_y > 0
+        ):
+            device = torch.device("cpu")
+            tensor_4d = (
+                torch.from_numpy(batch_depth_numpy_float).unsqueeze(1).to(device)
+            )
+            # Left-only pre-step (directional): applied before normal X/Y dilate/blur to preserve parity
+
+            # Dilate Left (directional) - optional
+            if abs(render_dilate_left) > 1e-5:
+                tensor_before = tensor_4d
+                tensor_4d = custom_dilate_left(
+                    tensor_before, float(render_dilate_left), False, max_raw_value
+                )
+
+            if render_blur_left > 0:
+                # Blur Left: blur *only* along strong left edges (dark->bright when moving left->right).
+                # This avoids blurring smooth gradients that typically don't create warp/splat jaggies.
+                effective_max_value = max(max_raw_value, 1e-5)
+                EDGE_STEP_8BIT = (
+                    3.0  # raise to blur fewer edges; lower to blur more edges
+                )
+                step_thresh = effective_max_value * (EDGE_STEP_8BIT / 255.0)
+
+                dx = tensor_4d[:, :, :, 1:] - tensor_4d[:, :, :, :-1]
+                edge_core = dx > step_thresh
+
+                edge_mask = torch.zeros_like(tensor_4d, dtype=torch.float32)
+                edge_mask[:, :, :, 1:] = edge_core.float()
+
+                # Expand into a small band around the edge (both sides) so it feels like a normal blur (no hard cut-off).
+                k_blur = int(round(render_blur_left))
+                if k_blur <= 0:
+                    k_blur = 1
+                if k_blur % 2 == 0:
+                    k_blur += 1
+
+                # Keep the band relatively tight around the detected edge so we don't soften large interior regions.
+                band_half = max(1, int(math.ceil(k_blur / 4.0)))
+                edge_band = (
+                    F.max_pool2d(
+                        edge_mask,
+                        kernel_size=(1, 2 * band_half + 1),
+                        stride=1,
+                        padding=(0, band_half),
+                    )
+                    > 0.5
+                ).float()
+
+                # Feather the band so the blend ramps on/off smoothly.
+                alpha = custom_blur(edge_band, 7, 1, False, 1.0)
+                alpha = torch.clamp(alpha, 0.0, 1.0)
+
+                # Two-pass blur for Blur Left:
+                # - Horizontal-only blur helps anti-alias along X (like your regular Blur X behavior),
+                # - Vertical-only blur helps smooth stair-steps along the edge.
+                # We blend horizontal/vertical Blur Left based on a compact UI selector:
+                #   0.0 = all horizontal, 1.0 = all vertical, 0.5 = 50/50.
+                try:
+                    mix_f = float(self.depth_blur_left_mix_var.get())
+                except Exception:
+                    mix_f = 0.5
+                mix_f = max(0.0, min(1.0, mix_f))
+
+                BLUR_LEFT_V_WEIGHT = mix_f
+                BLUR_LEFT_H_WEIGHT = 1.0 - mix_f
+
+                blurred_h = None
+                blurred_v = None
+                if BLUR_LEFT_H_WEIGHT > 1e-6:
+                    blurred_h = custom_blur(tensor_4d, k_blur, 1, False, max_raw_value)
+                if BLUR_LEFT_V_WEIGHT > 1e-6:
+                    blurred_v = custom_blur(tensor_4d, 1, k_blur, False, max_raw_value)
+
+                if blurred_h is not None and blurred_v is not None:
+                    wsum = BLUR_LEFT_H_WEIGHT + BLUR_LEFT_V_WEIGHT
+                    blurred = (
+                        blurred_h * BLUR_LEFT_H_WEIGHT + blurred_v * BLUR_LEFT_V_WEIGHT
+                    ) / max(wsum, 1e-6)
+                elif blurred_h is not None:
+                    blurred = blurred_h
+                elif blurred_v is not None:
+                    blurred = blurred_v
+                else:
+                    blurred = tensor_4d
+
+                tensor_4d = tensor_4d * (1.0 - alpha) + blurred * alpha
+            if abs(render_dilate_x) > 1e-5 or abs(render_dilate_y) > 1e-5:
+                tensor_4d = custom_dilate(
+                    tensor_4d,
+                    float(render_dilate_x),
+                    float(render_dilate_y),
+                    False,
+                    max_raw_value,
+                )
+            if render_blur_x > 0 or render_blur_y > 0:
+                tensor_4d = custom_blur(
+                    tensor_4d,
+                    float(render_blur_x),
+                    float(render_blur_y),
+                    False,
+                    max_raw_value,
+                )
+            batch_depth_numpy_float = tensor_4d.squeeze(1).cpu().numpy()
+            release_cuda_memory()
+
+        return batch_depth_numpy_float
+
+    def _process_single_video_tasks(
+        self,
+        video_path,
+        settings,
+        initial_overall_task_counter,
+        is_single_file_mode,
+        finished_source_folder=None,
+        finished_depth_folder=None,
+    ):
+        """
+        Handles the full processing lifecycle (sidecar, auto-conv, task loop, move-to-finished)
+        for a single video and its depth map.
+
+        Returns: (tasks_processed_count: int, any_task_completed_successfully: bool)
+        """
+        # Initialize task-local variables (some of these were local in the old _run_batch_process loop)
+        current_depth_dilate_size_x = 0
+        current_depth_dilate_size_y = 0
+        current_depth_blur_size_x = 0
+        current_depth_blur_size_y = 0
+        current_depth_dilate_left = 0.0
+        current_depth_blur_left = 0.0
+        current_depth_blur_left_mix = 0.0
+
+        video_name = os.path.splitext(os.path.basename(video_path))[0]
+        logger.info(f"==> Processing Video: {video_name}")
+        self.progress_queue.put(("update_info", {"filename": video_name}))
+
+        # Keep a local counter for tasks processed in this function
+        local_task_counter = initial_overall_task_counter
+
+        video_specific_settings = self._get_video_specific_settings(
+            video_path,
+            settings["input_depth_maps"],
+            settings["zero_disparity_anchor"],
+            settings["max_disp"],
+            is_single_file_mode,
+        )
+
+        processing_tasks = self._get_defined_tasks(settings)
+        has_low_res_task = any(t.get("is_low_res") for t in processing_tasks)
+        has_full_res_task = any(not t.get("is_low_res") for t in processing_tasks)
+        expected_task_count = len(processing_tasks)
+        processed_tasks_count = 0
+        any_task_completed_successfully_for_this_video = False
+
+        if video_specific_settings.get("error"):
+            logger.error(
+                f"Error getting video specific settings for {video_name}: {video_specific_settings['error']}. Skipping."
+            )
+            # Skip the expected task count in the progress bar
+            local_task_counter += expected_task_count
+            self.progress_queue.put(("processed", local_task_counter))
+            return expected_task_count, False
+
+        actual_depth_map_path = video_specific_settings["actual_depth_map_path"]
+        current_zero_disparity_anchor = video_specific_settings["convergence_plane"]
+        current_max_disparity_percentage = video_specific_settings[
+            "max_disparity_percentage"
+        ]
+        current_input_bias = video_specific_settings["input_bias"]
+        anchor_source = video_specific_settings["anchor_source"]
+        max_disp_source = video_specific_settings["max_disp_source"]
+        gamma_source = video_specific_settings["gamma_source"]
+        map_source = video_specific_settings.get("map_source", "N/A")
+        current_depth_gamma = video_specific_settings["depth_gamma"]
+        current_depth_dilate_size_x = video_specific_settings["depth_dilate_size_x"]
+        current_depth_dilate_size_y = video_specific_settings["depth_dilate_size_y"]
+        current_depth_blur_size_x = video_specific_settings["depth_blur_size_x"]
+        current_depth_blur_size_y = video_specific_settings["depth_blur_size_y"]
+        current_depth_dilate_left = float(
+            video_specific_settings.get(
+                "depth_dilate_left", settings.get("depth_dilate_left", 0.0)
+            )
+        )
+        current_depth_blur_left = float(
+            video_specific_settings.get(
+                "depth_blur_left", settings.get("depth_blur_left", 0.0)
+            )
+        )
+        current_depth_blur_left_mix = float(
+            video_specific_settings.get(
+                "depth_blur_left_mix",
+                getattr(self, "depth_blur_left_mix_var", None).get()
+                if getattr(self, "depth_blur_left_mix_var", None)
+                else 0.0,
+            )
+        )
+
+        if not processing_tasks:
+            logger.debug(
+                f"==> No processing tasks configured for {video_name}. Skipping."
+            )
+            return 0, False
+
+        # --- Auto-Convergence Logic (BEFORE initializing readers) ---
+        auto_conv_mode = settings["auto_convergence_mode"]
+
+        # --- NEW: Global Normalization Policy variables ---
+        enable_global_norm_policy = video_specific_settings["enable_global_norm"]
+        gn_source = video_specific_settings["gn_source"]
+
+        if (
+            video_specific_settings["sidecar_found"]
+            and self.enable_global_norm_var.get()
+        ):
+            # Policy: Sidecar exists AND GUI toggle is ON. Policy forces GN OFF.
+            if not self._gn_warning_shown:
+                messagebox.showwarning(
+                    "GN Policy Warning",
+                    f"Sidecar found for '{video_name}'.\n"
+                    f"Global Normalization is DISABLED for this clip, overriding the GUI setting.\n"
+                    f"Further warnings will be logged to console only.",
+                )
+                self._gn_warning_shown = (
+                    True  # Set flag to log to console only next time
+                )
+            else:
+                logger.warning(
+                    f"GN Policy: Sidecar found for {video_name}. GN forced OFF (console log only)."
+                )
+
+        # --- NEW LOGIC: Sidecar overrides Auto-Convergence ---
+        if anchor_source == "Sidecar" and auto_conv_mode != "Off":
+            logger.info(
+                f"Sidecar found for {video_name}. Convergence Point locked to Sidecar value ({current_zero_disparity_anchor:.4f}). Auto-Convergence SKIPPED."
+            )
+            auto_conv_mode = "Off"
+
+        if auto_conv_mode != "Off":
+            logger.info(
+                f"Auto-Convergence is ENABLED (Mode: {auto_conv_mode}). Running pre-pass..."
+            )
+
+            try:
+                anchor_float = float(current_zero_disparity_anchor)
+            except (ValueError, TypeError):
+                logger.error(
+                    f"Invalid convergence anchor value found: {current_zero_disparity_anchor}. Defaulting to 0.5."
+                )
+                anchor_float = 0.5
+
+            new_anchor_avg, new_anchor_peak = self._determine_auto_convergence(
+                video_path,
+                actual_depth_map_path,
+                settings["process_length"],
+                settings["full_res_batch_size"],
+                anchor_float,
+                gamma=settings.get("depth_gamma", 1.0),
+            )
+            if auto_conv_mode == "Average":
+                new_anchor_val = new_anchor_avg
+            elif auto_conv_mode == "Peak":
+                new_anchor_val = new_anchor_peak
+            elif auto_conv_mode == "Hybrid":
+                new_anchor_val = (new_anchor_avg + new_anchor_peak) / 2.0
+            else:
+                new_anchor_val = float(current_zero_disparity_anchor)
+
+            if new_anchor_val != current_zero_disparity_anchor:
+                current_zero_disparity_anchor = new_anchor_val
+                anchor_source = "Auto"
+
+            logger.info(
+                f"Using Convergence Point: {current_zero_disparity_anchor:.4f} (Source: {anchor_source})"
+            )
+
+        for task in processing_tasks:
+            if self.stop_event.is_set():
+                logger.info(
+                    f"==> Stopping {task['name']} processing for {video_name} due to user request"
+                )
+                # Increment the global counter for all remaining, skipped tasks
+                remaining_tasks_to_increment = (
+                    expected_task_count - processed_tasks_count
+                )
+                local_task_counter += remaining_tasks_to_increment
+                self.progress_queue.put(("processed", local_task_counter))
+                return (
+                    expected_task_count,
+                    any_task_completed_successfully_for_this_video,
+                )
+
+            logger.debug(f"\n==> Starting {task['name']} pass for {video_name}")
+            self.progress_queue.put(
+                ("status", f"Processing {task['name']} for {video_name}")
+            )
+
+            # Decide what to show in the Map field
+            if self.multi_map_var.get():
+                # Multi-Map mode
+                if actual_depth_map_path and map_source not in ("", "N/A"):
+                    map_folder = os.path.basename(
+                        os.path.dirname(actual_depth_map_path)
+                    ).strip()
+                    map_label = f"{map_folder} ({map_source})"
+                else:
+                    map_label = "N/A"
+            else:
+                # Normal mode
+                map_label = "Direct file" if is_single_file_mode else "Direct folder"
+
+            self.progress_queue.put(
+                (
+                    "update_info",
+                    {
+                        "task_name": task["name"],
+                        "convergence": f"{current_zero_disparity_anchor:.2f} ({anchor_source})",
+                        "disparity": f"{current_max_disparity_percentage:.1f}% ({max_disp_source})",
+                        "gamma": f"{current_depth_gamma:.2f} ({gamma_source})",
+                        "map": map_label,
+                    },
+                )
+            )
+
+            (
+                video_reader_input,
+                depth_reader_input,
+                processed_fps,
+                original_vid_h,
+                original_vid_w,
+                current_processed_height,
+                current_processed_width,
+                video_stream_info,
+                total_frames_input,
+                total_frames_depth,
+                actual_depth_height,
+                actual_depth_width,
+                depth_stream_info,
+            ) = self._initialize_video_and_depth_readers(
+                video_path,
+                actual_depth_map_path,
+                settings["process_length"],
+                task,
+                settings["match_depth_res"],
+            )
+
+            # Explicitly check for None for critical components before proceeding
+            if (
+                video_reader_input is None
+                or depth_reader_input is None
+                or video_stream_info is None
+            ):
+                logger.error(
+                    f"Skipping {task['name']} pass for {video_name} due to reader initialization error, frame count mismatch, or missing stream info."
+                )
+                local_task_counter += 1
+                processed_tasks_count += 1
+                self.progress_queue.put(("processed", local_task_counter))
+                release_cuda_memory()
+                continue
+
+            # --- MODIFIED: Use the policy to determine the mode ---
+            assume_raw_input_mode = (
+                not enable_global_norm_policy
+            )  # If GN is OFF, assume RAW Input
+            global_depth_min = 0.0
+            global_depth_max = 1.0
+
+            # --- UNCONDITIONAL Max Content Value Scan for RAW/Normalization Modes ---
+            max_content_value = 1.0
+            raw_depth_reader_temp = None
+            try:
+                depth_info_tmp = get_video_stream_info(actual_depth_map_path)
+
+                bit_depth_tmp = _infer_depth_bit_depth(depth_info_tmp)
+
+                pix_fmt_tmp = str((depth_info_tmp or {}).get("pix_fmt", ""))
+
+                # Use source/processing dimensions for the content scan (avoids NameError and keeps preview/render parity)
+                scan_w = original_vid_w or current_processed_width or actual_depth_width
+                scan_h = (
+                    original_vid_h or current_processed_height or actual_depth_height
+                )
+
+                if bit_depth_tmp > 8:
+                    raw_depth_reader_temp = FFmpegDepthPipeReader(
+                        actual_depth_map_path,
+                        out_w=scan_w,
+                        out_h=scan_h,
+                        bit_depth=bit_depth_tmp,
+                        num_frames=total_frames_depth,
+                        pix_fmt=pix_fmt_tmp,
+                    )
+
+                else:
+                    raw_depth_reader_temp = VideoReader(
+                        actual_depth_map_path, ctx=cpu(0), width=scan_w, height=scan_h
+                    )
+
+                if len(raw_depth_reader_temp) > 0:
+                    _, max_content_value = compute_global_depth_stats(
+                        depth_map_reader=raw_depth_reader_temp,
+                        total_frames=total_frames_depth,
+                        chunk_size=task["batch_size"],
+                    )
+                    logger.debug(f"Max content depth scanned: {max_content_value:.3f}.")
+                else:
+                    logger.error("RAW depth reader has no frames for content scan.")
+            except Exception as e:
+                logger.error(f"Failed to scan max content depth: {e}")
+            finally:
+                if raw_depth_reader_temp:
+                    if hasattr(raw_depth_reader_temp, "close"):
+                        try:
+                            raw_depth_reader_temp.close()
+                        except Exception:
+                            pass
+                    del raw_depth_reader_temp
+                    gc.collect()
+            # --- END UNCONDITIONAL SCAN ---
+
+            if not assume_raw_input_mode:
+                logger.info(
+                    "==> Global Depth Normalization selected. Starting global depth stats pre-pass with RAW reader."
+                )
+
+                raw_depth_reader_temp = None
+                try:
+                    depth_info_tmp = get_video_stream_info(actual_depth_map_path)
+
+                    bit_depth_tmp = _infer_depth_bit_depth(depth_info_tmp)
+
+                    pix_fmt_tmp = str((depth_info_tmp or {}).get("pix_fmt", ""))
+
+                    if bit_depth_tmp > 8:
+                        raw_depth_reader_temp = FFmpegDepthPipeReader(
+                            actual_depth_map_path,
+                            out_w=actual_depth_width,
+                            out_h=actual_depth_height,
+                            bit_depth=bit_depth_tmp,
+                            num_frames=total_frames_depth,
+                            pix_fmt=pix_fmt_tmp,
+                        )
+
+                    else:
+                        raw_depth_reader_temp = VideoReader(
+                            actual_depth_map_path,
+                            ctx=cpu(0),
+                            width=actual_depth_width,
+                            height=actual_depth_height,
+                        )
+
+                    if len(raw_depth_reader_temp) > 0:
+                        global_depth_min, global_depth_max = compute_global_depth_stats(
+                            depth_map_reader=raw_depth_reader_temp,
+                            total_frames=total_frames_depth,
+                            chunk_size=task["batch_size"],
+                        )
+                        logger.debug(
+                            "Successfully computed global stats from RAW reader."
+                        )
+                    else:
+                        logger.error("RAW depth reader has no frames.")
+                except Exception as e:
+                    logger.error(
+                        f"Failed to initialize/read RAW depth reader for global stats: {e}"
+                    )
+                    global_depth_min = 0.0
+                    global_depth_max = 1.0
+                finally:
+                    if raw_depth_reader_temp:
+                        if hasattr(raw_depth_reader_temp, "close"):
+                            try:
+                                raw_depth_reader_temp.close()
+                            except Exception:
+                                pass
+                        del raw_depth_reader_temp
+                        gc.collect()
+            else:
+                logger.debug(
+                    "==> No Normalization (Assume Raw 0-1 Input) selected. Skipping global stats pre-pass."
+                )
+
+                # --- RAW INPUT MODE SCALING ---
+                final_scaling_factor = 1.0
+
+                if max_content_value <= 256.0 and max_content_value > 1.0:
+                    final_scaling_factor = 255.0
+                    logger.debug(
+                        f"Content Max {max_content_value:.2f} <= 8-bit. SCALING BY 255.0."
+                    )
+                elif max_content_value > 256.0 and max_content_value <= 1024.0:
+                    final_scaling_factor = 1023.0
+                    logger.debug(
+                        f"Content Max {max_content_value:.2f} (9-10bit). SCALING BY 1023.0 (expected 10-bit max) for preview parity."
+                    )
+                else:
+                    final_scaling_factor = 1023.0
+                    logger.warning(
+                        f"Max content value is too high/low ({max_content_value:.2f}). Using fallback 1023.0."
+                    )
+
+                global_depth_max = final_scaling_factor
+                global_depth_min = 0.0
+
+                logger.debug(
+                    f"Raw Input Final Scaling Factor set to: {global_depth_max:.3f}"
+                )
+
+            if not (
+                actual_depth_height == current_processed_height
+                and actual_depth_width == current_processed_width
+            ):
+                # Low-res parity path intentionally loads depth at clip resolution so dilation/blur happens pre-resize.
+                # The depth frames are resized to match the (possibly low-res) video frames *after* preprocessing inside depthSplatting().
+                if task.get("is_low_res"):
+                    skip_lr_preproc = bool(
+                        getattr(self, "skip_lowres_preproc_var", None) is not None
+                        and self.skip_lowres_preproc_var.get()
+                    )
+                    if (not skip_lr_preproc) and (
+                        actual_depth_width == original_vid_w
+                        and actual_depth_height == original_vid_h
+                    ):
+                        logger.info(
+                            f"==> Low-Resolution pass: Upscaling depthmap to clip resolution ({actual_depth_width}x{actual_depth_height}) "
+                            f"before preprocessing for parity; it will be resized to ({current_processed_width}x{current_processed_height}) after preprocessing."
+                        )
+                    else:
+                        _bd_warn = (
+                            _infer_depth_bit_depth(depth_stream_info)
+                            if depth_stream_info
+                            else 8
+                        )
+                        if _bd_warn > 8:
+                            logger.warning(
+                                f"==> Warning: Depth map reader output resolution ({actual_depth_width}x{actual_depth_height}) does not match processed "
+                                f"video resolution ({current_processed_width}x{current_processed_height}) for {task['name']} pass. "
+                                f"This indicates an issue with `load_pre_rendered_depth`'s `width`/`height` parameters. Processing may proceed but results might be misaligned."
+                            )
+                else:
+                    _bd_warn = (
+                        _infer_depth_bit_depth(depth_stream_info)
+                        if depth_stream_info
+                        else 8
+                    )
+                    if _bd_warn > 8:
+                        logger.warning(
+                            f"==> Warning: Depth map reader output resolution ({actual_depth_width}x{actual_depth_height}) does not match processed "
+                            f"video resolution ({current_processed_width}x{current_processed_height}) for {task['name']} pass. "
+                            f"This indicates an issue with `load_pre_rendered_depth`'s `width`/`height` parameters. Processing may proceed but results might be misaligned."
+                        )
+
+            tv_disp_comp = 1.0
+            if assume_raw_input_mode:
+                try:
+                    if _infer_depth_bit_depth(depth_stream_info) > 8 and str((depth_stream_info or {}).get("color_range", "unknown")).lower() == "tv":
+                        tv_disp_comp = 1.0 / (DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM)
+                except Exception:
+                    tv_disp_comp = 1.0
+
+            actual_percentage_for_calculation = (current_max_disparity_percentage / 20.0) * tv_disp_comp
+            actual_max_disp_pixels = (
+                actual_percentage_for_calculation / 100.0
+            ) * current_processed_width
+            logger.debug(
+                f"==> Max Disparity Input: {current_max_disparity_percentage:.1f}% -> Calculated Max Disparity for splatting ({task['name']}): {actual_max_disp_pixels:.2f} pixels"
+            )
+
+            self.progress_queue.put(
+                (
+                    "update_info",
+                    {
+                        "disparity": f"{current_max_disparity_percentage:.1f}% ({actual_max_disp_pixels:.2f} pixels)"
+                    },
+                )
+            )
+
+            current_output_subdir = os.path.join(
+                settings["output_splatted"], task["output_subdir"]
+            )
+            os.makedirs(current_output_subdir, exist_ok=True)
+            output_video_path_base = os.path.join(
+                current_output_subdir, f"{video_name}.mp4"
+            )
+
+            # Prepare signature for render-time Total(D+P) capture (for CSV/sidecar exact max update)
+            try:
+                _track_true = (
+                    getattr(self, "track_dp_total_true_on_render_var", None) is not None
+                    and self.track_dp_total_true_on_render_var.get()
+                )
+                if _track_true and ((has_full_res_task and (not task.get("is_low_res"))) or ((not has_full_res_task) and task.get("is_low_res"))):
+                    self._dp_total_true_active_sig = self._dp_total_signature(
+                        actual_depth_map_path,
+                        float(current_zero_disparity_anchor),
+                        float(current_max_disparity_percentage),
+                        float(current_depth_gamma),
+                    )
+                    self._dp_total_true_active_val = None
+                else:
+                    self._dp_total_true_active_sig = None
+                    self._dp_total_true_active_val = None
+            except Exception:
+                self._dp_total_true_active_sig = None
+                self._dp_total_true_active_val = None
+            completed_splatting_task = self.depthSplatting(
+                input_video_reader=video_reader_input,
+                depth_map_reader=depth_reader_input,
+                total_frames_to_process=total_frames_input,
+                processed_fps=processed_fps,
+                output_video_path_base=output_video_path_base,
+                target_output_height=current_processed_height,
+                target_output_width=current_processed_width,
+                max_disp=actual_max_disp_pixels,
+                process_length=settings["process_length"],
+                batch_size=task["batch_size"],
+                dual_output=settings["dual_output"],
+                zero_disparity_anchor_val=current_zero_disparity_anchor,
+                video_stream_info=video_stream_info,
+                input_bias=current_input_bias,
+                assume_raw_input=assume_raw_input_mode,
+                global_depth_min=global_depth_min,
+                global_depth_max=global_depth_max,
+                depth_stream_info=depth_stream_info,
+                user_output_crf=(
+                    settings.get("output_crf_low")
+                    if task["is_low_res"]
+                    else settings.get("output_crf_full", settings.get("output_crf"))
+                ),
+                is_low_res_task=task["is_low_res"],
+                depth_gamma=current_depth_gamma,
+                depth_dilate_size_x=(
+                    0.0
+                    if (
+                        task["is_low_res"]
+                        and getattr(self, "skip_lowres_preproc_var", None) is not None
+                        and self.skip_lowres_preproc_var.get()
+                    )
+                    else current_depth_dilate_size_x
+                ),
+                depth_dilate_size_y=(
+                    0.0
+                    if (
+                        task["is_low_res"]
+                        and getattr(self, "skip_lowres_preproc_var", None) is not None
+                        and self.skip_lowres_preproc_var.get()
+                    )
+                    else current_depth_dilate_size_y
+                ),
+                depth_blur_size_x=(
+                    0.0
+                    if (
+                        task["is_low_res"]
+                        and getattr(self, "skip_lowres_preproc_var", None) is not None
+                        and self.skip_lowres_preproc_var.get()
+                    )
+                    else current_depth_blur_size_x
+                ),
+                depth_blur_size_y=(
+                    0.0
+                    if (
+                        task["is_low_res"]
+                        and getattr(self, "skip_lowres_preproc_var", None) is not None
+                        and self.skip_lowres_preproc_var.get()
+                    )
+                    else current_depth_blur_size_y
+                ),
+                depth_dilate_left=(
+                    0.0
+                    if (
+                        task["is_low_res"]
+                        and getattr(self, "skip_lowres_preproc_var", None) is not None
+                        and self.skip_lowres_preproc_var.get()
+                    )
+                    else float(current_depth_dilate_left)
+                ),
+                depth_blur_left=(
+                    0.0
+                    if (
+                        task["is_low_res"]
+                        and getattr(self, "skip_lowres_preproc_var", None) is not None
+                        and self.skip_lowres_preproc_var.get()
+                    )
+                    else float(current_depth_blur_left)
+                ),
+            )
+
+            if self.stop_event.is_set():
+                logger.info(
+                    f"==> Stopping {task['name']} pass for {video_name} due to user request"
+                )
+                break
+
+            if completed_splatting_task:
+                logger.debug(
+                    f"==> Splatted {task['name']} video saved for {video_name}."
+                )
+                any_task_completed_successfully_for_this_video = True
+
+                # Persist per-clip settings/metadata + (optional) render-time True Max into sidecar/CSV
+                try:
+                    # Capture any render-measured True Max Total(D+P)
+                    sig = getattr(self, "_dp_total_true_active_sig", None)
+                    dp_true = getattr(self, "_dp_total_true_active_val", None)
+                    if sig and dp_true is not None:
+                        self._dp_total_true_cache[sig] = float(dp_true)
+
+                    sidecar_ext = self.APP_CONFIG_DEFAULTS.get("SIDECAR_EXT", ".fssidecar")
+                    depth_bn_noext = os.path.splitext(os.path.basename(actual_depth_map_path))[0]
+                    sidecar_folder = self._get_sidecar_base_folder()
+                    json_sidecar_path = os.path.join(sidecar_folder, f"{depth_bn_noext}{sidecar_ext}")
+
+                    sidecar_data = {}
+                    try:
+                        sidecar_data = self.sidecar_manager.load_sidecar_data(json_sidecar_path) or {}
+                    except Exception:
+                        sidecar_data = {}
+
+                    # Always overwrite with the EFFECTIVE values used for THIS clip (avoid defaults leaking in)
+                    sidecar_data["convergence_plane"] = float(current_zero_disparity_anchor)
+                    sidecar_data["max_disparity"] = float(current_max_disparity_percentage)
+                    sidecar_data["gamma"] = float(f"{float(current_depth_gamma):.2f}")
+
+                    sidecar_data["depth_dilate_size_x"] = float(current_depth_dilate_size_x)
+                    sidecar_data["depth_dilate_size_y"] = float(current_depth_dilate_size_y)
+                    sidecar_data["depth_blur_size_x"] = float(current_depth_blur_size_x)
+                    sidecar_data["depth_blur_size_y"] = float(current_depth_blur_size_y)
+                    sidecar_data["depth_dilate_left"] = float(current_depth_dilate_left)
+                    sidecar_data["depth_blur_left"] = float(current_depth_blur_left)
+
+                    # Selected depth map (Multi-Map mode)
+                    try:
+                        if self.multi_map_var.get():
+                            base_depth = os.path.normpath(settings.get("input_depth_maps", "") or "")
+                            if base_depth:
+                                rel = os.path.relpath(os.path.normpath(actual_depth_map_path), base_depth)
+                                parts = rel.split(os.sep)
+                                if len(parts) > 1 and parts[0] not in (".", ""):
+                                    sidecar_data["selected_depth_map"] = parts[0]
+                    except Exception:
+                        pass
+
+                    # Auto Border: only compute/save when Auto is selected; otherwise preserve any existing per-clip borders.
+                    try:
+                        border_mode = self.border_mode_var.get()
+                    except Exception:
+                        border_mode = "Off"
+
+                    if border_mode in ("Auto Basic", "Auto Adv."):
+                        try:
+                            if border_mode == "Auto Basic":
+                                width = max(
+                                    0.0,
+                                    (1.0 - float(current_zero_disparity_anchor))
+                                    * 2.0
+                                    * (float(current_max_disparity_percentage) / 20.0) * tv_disp_comp,
+                                )
+                                sidecar_data["left_border"] = round(width, 3)
+                                sidecar_data["right_border"] = round(width, 3)
+                                sidecar_data["auto_border_L"] = sidecar_data["left_border"]
+                                sidecar_data["auto_border_R"] = sidecar_data["right_border"]
+                                sidecar_data["border_mode"] = border_mode
+                                logger.info(
+                                    f"Auto Border (Basic): left_border={sidecar_data['left_border']}, right_border={sidecar_data['right_border']}"
+                                )
+                            else:
+                                l_val, r_val = self._scan_borders_for_depth_path(
+                                    actual_depth_map_path,
+                                    float(current_zero_disparity_anchor),
+                                    float(current_max_disparity_percentage),
+                                    float(current_depth_gamma),
+                                )
+                                if l_val is not None and r_val is not None:
+                                    sidecar_data["left_border"] = round(float(l_val), 3)
+                                    sidecar_data["right_border"] = round(float(r_val), 3)
+                                    sidecar_data["auto_border_L"] = sidecar_data["left_border"]
+                                    sidecar_data["auto_border_R"] = sidecar_data["right_border"]
+                                    sidecar_data["border_mode"] = border_mode
+                                    logger.info(
+                                        f"Auto Border (Adv.): left_border={sidecar_data['left_border']}, right_border={sidecar_data['right_border']}"
+                                    )
+                        except Exception:
+                            pass
+
+                    # Persist render-measured True Max Total(D+P) into sidecar (and log the value) if available
+                    if dp_true is not None:
+                        try:
+                            sidecar_data["dp_total_max_true"] = round(float(dp_true), 3)
+                            logger.info(
+                                f"True Max Total(D+P): dp_total_max_true={sidecar_data['dp_total_max_true']:.3f}"
+                            )
+                        except Exception:
+                            pass
+
+                    # Save sidecar (create if missing)
+                    try:
+                        if not self.sidecar_manager.save_sidecar_data(json_sidecar_path, sidecar_data):
+                            logger.error(f"Failed to save sidecar: {os.path.basename(json_sidecar_path)}")
+                    except Exception:
+                        pass
+
+                    # Update CSV row (creates file on demand)
+                    self._auto_pass_csv_update_row_for_paths(
+                        source_video_path=video_path,
+                        depth_map_path=actual_depth_map_path,
+                        current_data=sidecar_data,
+                        dp_total_max_true=(float(dp_true) if dp_true is not None else None),
+                    )
+                except Exception:
+                    pass
+
+                if video_reader_input is not None:
+                    del video_reader_input
+                if depth_reader_input is not None:
+                    del depth_reader_input
+                torch.cuda.empty_cache()
+                gc.collect()
+                logger.debug(
+                    "Explicitly deleted VideoReader objects and forced garbage collection to release file handles."
+                )
+            else:
+                logger.info(
+                    f"==> Splatting task '{task['name']}' for '{video_name}' was skipped or failed. Files will NOT be moved."
+                )
+                if video_reader_input:
+                    del video_reader_input
+                if depth_reader_input:
+                    del depth_reader_input
+                torch.cuda.empty_cache()
+                gc.collect()
+
+            local_task_counter += 1
+            processed_tasks_count += 1
+            self.progress_queue.put(("processed", local_task_counter))
+            logger.debug(f"==> Completed {task['name']} pass for {video_name}.")
+
+        # After all tasks for the current video are processed or stopped
+        if self.stop_event.is_set():
+            return expected_task_count, any_task_completed_successfully_for_this_video
+
+        # Move to finished logic
+        move_enabled = settings[
+            "move_to_finished"
+        ]  # Use the setting from the dictionary
+
+        if is_single_file_mode:
+            # CRITICAL FIX: Get the finished folders directly from settings (set in start_single_processing)
+            single_finished_src = settings.get("single_finished_source_folder")
+            single_finished_depth = settings.get("single_finished_depth_folder")
+
+            # --- Check move_enabled setting ---
+            if (
+                single_finished_src
+                and single_finished_depth
+                and move_enabled
+                and any_task_completed_successfully_for_this_video
+            ):
+                self._move_processed_files(
+                    video_path,
+                    actual_depth_map_path,
+                    single_finished_src,
+                    single_finished_depth,
+                )
+            else:
+                logger.debug(
+                    f"Single file move skipped. Enabled={move_enabled}, Success={any_task_completed_successfully_for_this_video}, PathsValid={bool(single_finished_src)}"
+                )
+
+        elif (
+            any_task_completed_successfully_for_this_video
+            and finished_source_folder
+            and finished_depth_folder
+            and move_enabled
+        ):
+            # Batch mode move (uses the arguments passed from _run_batch_process)
+            self._move_processed_files(
+                video_path,
+                actual_depth_map_path,
+                finished_source_folder,
+                finished_depth_folder,
+            )
+
+        # Return the number of tasks actually processed for the global counter update
+        return processed_tasks_count, any_task_completed_successfully_for_this_video
+
+    def _preview_processing_callback(
+        self, source_frames: dict, params: dict
+    ) -> Optional[Image.Image]:
+        """
+        Callback for VideoPreviewer. Performs splatting on a single frame for preview.
+        """
+        # NOTE: Do not clear the 'Current Processing Information' panel on every preview render.
+        # Clearing here caused filename/task info to briefly appear and then reset to N/A each time the preview updates.
+
+        if not globals()["CUDA_AVAILABLE"]:
+            logger.error("Preview processing requires a CUDA-enabled GPU.")
+            return None
+
+        logger.debug("--- Starting Preview Processing Callback ---")
+
+        left_eye_tensor = source_frames.get("source_video")
+        depth_tensor_raw = source_frames.get("depth_map")
+
+        if left_eye_tensor is None or depth_tensor_raw is None:
+            logger.error("Preview failed: Missing source video or depth map tensor.")
+            return None
+
+        # --- Get latest settings and Preview Mode ---
+        params = self.get_current_preview_settings()
+        if not params:
+            logger.error("Preview failed: Could not get current preview settings.")
+            return None
+
+        preview_source = self.preview_source_var.get()
+        is_low_res_preview = preview_source in [
+            "Splat Result(Low)",
+            "Occlusion Mask(Low)",
+        ]
+
+        # Determine the target resolution for the preview tensor
+        W_orig = left_eye_tensor.shape[3]
+        H_orig = left_eye_tensor.shape[2]
+
+        # ----------------------------------------------------------------------
+        # NEW SIDECAR LOGIC FOR PREVIEW
+        # ----------------------------------------------------------------------
+        depth_map_path = None
+        if 0 <= self.previewer.current_video_index < len(self.previewer.video_list):
+            current_source_dict = self.previewer.video_list[
+                self.previewer.current_video_index
+            ]
+            depth_map_path = current_source_dict.get("depth_map")
+
+        gui_config = {
+            "convergence_plane": float(self.zero_disparity_anchor_var.get()),
+            "max_disparity": float(self.max_disp_var.get()),
+            "gamma": float(self.depth_gamma_var.get()),
+        }
+
+        merged_config = gui_config.copy()
+
+        # Set final parameters from the merged config
+        params["convergence_point"] = merged_config["convergence_plane"]
+        params["max_disp"] = merged_config["max_disparity"]
+        params["depth_gamma"] = merged_config["gamma"]
+
+        # ----------------------------------------------------------------------
+        # END NEW SIDECAR LOGIC FOR PREVIEW
+        # ----------------------------------------------------------------------
+
+        W_target, H_target = W_orig, H_orig
+
+        if is_low_res_preview:
+            try:
+                W_target_requested = int(self.pre_res_width_var.get())
+
+                if W_target_requested <= 0:
+                    W_target_requested = W_orig  # Fallback
+
+                # 1. Calculate aspect-ratio-correct height based on the requested width
+                aspect_ratio = W_orig / H_orig
+                H_target_calculated = int(round(W_target_requested / aspect_ratio))
+
+                # 2. Ensure both W and H are divisible by 2 for codec compatibility
+                W_target = (
+                    W_target_requested
+                    if W_target_requested % 2 == 0
+                    else W_target_requested + 1
+                )
+                H_target = (
+                    H_target_calculated
+                    if H_target_calculated % 2 == 0
+                    else H_target_calculated + 1
+                )
+
+                # 3. Handle potential extreme fallbacks
+                if W_target <= 0 or H_target <= 0:
+                    W_target, H_target = W_orig, H_orig
+                    logger.warning(
+                        "Low-Res preview: Calculated dimensions invalid, falling back to original."
+                    )
+                else:
+                    logger.debug(
+                        f"Low-Res preview: AR corrected target {W_target}x{H_target}. (Original W: {W_orig}, H: {H_orig})"
+                    )
+
+                # Resize Left Eye to aspect-ratio-correct low-res target for consistency
+                left_eye_tensor_resized = F.interpolate(
+                    left_eye_tensor.cuda(),
+                    size=(H_target, W_target),
+                    mode="bilinear",
+                    align_corners=False,
+                )
+            except Exception as e:
+                logger.error(
+                    f"Low-Res preview failed during AR calculation/resize: {e}. Falling back to original res.",
+                    exc_info=True,
+                )
+                W_target, H_target = W_orig, H_orig
+                left_eye_tensor_resized = left_eye_tensor.cuda()
+        else:
+            left_eye_tensor_resized = left_eye_tensor.cuda()  # Use original res
+
+        logger.debug(f"Preview Params: {params}")
+        logger.debug(
+            f"Target Resolution: {W_target}x{H_target} (Low-Res: {is_low_res_preview})"
+        )
+
+        # --- Process Depth Frame ---
+        depth_numpy_raw = depth_tensor_raw.squeeze(0).permute(1, 2, 0).cpu().numpy()
+        logger.debug(
+            f"Raw depth numpy shape: {depth_numpy_raw.shape}, range: [{depth_numpy_raw.min():.2f}, {depth_numpy_raw.max():.2f}]"
+        )
+
+        # Ensure depth pre-processing happens at the same resolution as the *render* pipeline.
+        # - Full-res preview: pre-proc at clip resolution (W_orig/H_orig).
+        # - Low-res preview: pre-proc at clip resolution, then downscale to low-res AFTER pre-proc.
+        #   This matches the low-res render ordering and avoids the "extra thick" low-res preview look.
+        W_preproc, H_preproc = (W_orig, H_orig)
+        if depth_numpy_raw.ndim == 2:
+            depth_numpy_raw = depth_numpy_raw[:, :, None]
+        if (
+            depth_numpy_raw.shape[0] != H_preproc
+            or depth_numpy_raw.shape[1] != W_preproc
+        ):
+            try:
+                had_singleton_channel = (
+                    depth_numpy_raw.ndim == 3 and depth_numpy_raw.shape[2] == 1
+                )
+                interp = (
+                    cv2.INTER_LINEAR
+                    if (
+                        W_preproc > depth_numpy_raw.shape[1]
+                        or H_preproc > depth_numpy_raw.shape[0]
+                    )
+                    else cv2.INTER_AREA
+                )
+                depth_numpy_raw = cv2.resize(
+                    depth_numpy_raw, (W_preproc, H_preproc), interpolation=interp
+                )
+                if had_singleton_channel and depth_numpy_raw.ndim == 2:
+                    depth_numpy_raw = depth_numpy_raw[:, :, None]
+                logger.debug(
+                    f"Preview depth resized for pre-proc: {depth_numpy_raw.shape}"
+                )
+            except Exception as e:
+                logger.error(
+                    f"Preview depth resize (pre-proc) failed: {e}. Continuing with raw depth resolution.",
+                    exc_info=True,
+                )
+
+        # 1. DETERMINE MAX CONTENT VALUE FOR THE FRAME (for AutoGain scaling)
+        # We need the max *raw* value of the depth frame content
+        max_raw_content_value = depth_numpy_raw.max()
+        if max_raw_content_value < 1.0:
+            max_raw_content_value = 1.0  # Fallback for already 0-1 normalized content
+
+        # --- NEW: Get Global Normalization Policy for Preview (Sidecar check) ---
+        enable_global_norm = params.get("enable_global_norm", False)
+
+        # Policy Check: Sidecar existence forces GN OFF
+        sidecar_exists = False
+        if depth_map_path:
+            sidecar_folder = self._get_sidecar_base_folder()
+            depth_map_basename = os.path.splitext(os.path.basename(depth_map_path))[0]
+            sidecar_ext = self.APP_CONFIG_DEFAULTS["SIDECAR_EXT"]
+            json_sidecar_path = os.path.join(
+                sidecar_folder, f"{depth_map_basename}{sidecar_ext}"
+            )
+            sidecar_exists = os.path.exists(json_sidecar_path)
+
+        if sidecar_exists:
+            # Policy: If sidecar exists, GN is forced OFF
+            enable_global_norm = False
+
+        # --- NEW: Determine Global Min/Max from cache if GN is ON ---
+        global_min, global_max = 0.0, 1.0
+
+        if enable_global_norm and depth_map_path:
+            if depth_map_path not in self._clip_norm_cache:
+                # --- CACHE MISS: Run the slow scan synchronously ---
+                logger.info(
+                    f"Preview GN: Cache miss for {os.path.basename(depth_map_path)}. Running clip-local scan..."
+                )
+                global_min, global_max = self._compute_clip_global_depth_stats(
+                    depth_map_path
+                )
+            else:
+                # --- CACHE HIT: Use cached values ---
+                global_min, global_max = self._clip_norm_cache[depth_map_path]
+                logger.debug(
+                    f"Preview GN: Cache hit for {os.path.basename(depth_map_path)}. Min/Max: {global_min:.3f}/{global_max:.3f}"
+                )
+
+        # --- END NEW CACHE LOGIC ---
+
+        # Determine the scaling factor (Only relevant for MANUAL/RAW mode)
+        final_scaling_factor = 1.0
+
+        if not enable_global_norm:  # MANUAL/RAW INPUT MODE
+            if max_raw_content_value <= 256.0 and max_raw_content_value > 1.0:
+                final_scaling_factor = 255.0
+            elif max_raw_content_value > 256.0 and max_raw_content_value <= 1024.0:
+                final_scaling_factor = max_raw_content_value
+            elif max_raw_content_value > 1024.0:
+                final_scaling_factor = 65535.0
+            else:
+                final_scaling_factor = 1.0
+        else:  # GLOBAL NORMALIZATION MODE
+            # Use the global max from the cache/scan as the "max value" for scaling (only to correctly apply pre-processing if needed)
+            final_scaling_factor = max(global_max, 1e-5)
+
+        logger.debug(
+            f"Preview: GN={enable_global_norm}. Final Scaling Factor for Pre-Proc: {final_scaling_factor:.3f}"
+        )
+
+        depth_numpy_processed = self._process_depth_batch(
+            batch_depth_numpy_raw=np.expand_dims(depth_numpy_raw, axis=0),
+            depth_stream_info=None,
+            depth_gamma=params["depth_gamma"],
+            depth_dilate_size_x=params["depth_dilate_size_x"],
+            depth_dilate_size_y=params["depth_dilate_size_y"],
+            depth_blur_size_x=params["depth_blur_size_x"],
+            depth_blur_size_y=params["depth_blur_size_y"],
+            depth_dilate_left=params.get("depth_dilate_left", 0.0),
+            depth_blur_left=params.get("depth_blur_left", 0.0),
+            is_low_res_task=is_low_res_preview,
+            max_raw_value=final_scaling_factor,
+            global_depth_min=0.0,
+            global_depth_max=1.0,
+            debug_task_name="Preview",
+        )
+        logger.debug(
+            f"Processed depth numpy shape: {depth_numpy_processed.shape}, range: [{depth_numpy_processed.min():.2f}, {depth_numpy_processed.max():.2f}]"
+        )
+
+        # Low-Res preview: downscale the *processed* depth AFTER dilation/blur (matches render ordering).
+        if is_low_res_preview and (
+            depth_numpy_processed.shape[1] != H_target
+            or depth_numpy_processed.shape[2] != W_target
+        ):
+            try:
+                _d = depth_numpy_processed.squeeze(0)
+                _interp = (
+                    cv2.INTER_AREA
+                    if (W_target < _d.shape[1] and H_target < _d.shape[0])
+                    else cv2.INTER_LINEAR
+                )
+                _d = cv2.resize(_d, (W_target, H_target), interpolation=_interp)
+                depth_numpy_processed = np.expand_dims(_d.astype(np.float32), axis=0)
+                logger.debug(
+                    f"Low-Res preview: resized processed depth to target {W_target}x{H_target}."
+                )
+            except Exception as e:
+                logger.error(
+                    f"Low-Res preview post-preproc resize failed: {e}. Continuing with processed depth resolution.",
+                    exc_info=True,
+                )
+
+        # 2. Normalize based on the 'enable_global_norm' policy
+        depth_normalized = depth_numpy_processed.squeeze(0)
+
+        # Match render behavior:
+        # - In GN mode: normalize using cached/scanned min/max
+        # - In Manual/RAW mode: scale into 0-1 using the detected scaling factor
+        if enable_global_norm:
+            min_val, max_val = global_min, global_max
+            depth_range = max_val - min_val
+            if depth_range > 1e-5:
+                depth_normalized = (depth_normalized - min_val) / depth_range
+            else:
+                depth_normalized = np.zeros_like(depth_normalized)
+        else:
+            # In manual mode, preview frames may arrive as uint8/uint16-like ranges.
+            # Scale to 0..1 (render path uses the clip's max content value for this).
+            if final_scaling_factor and final_scaling_factor > 1.0 + 1e-5:
+                depth_normalized = depth_normalized / float(final_scaling_factor)
+
+        depth_normalized = np.clip(depth_normalized, 0, 1)
+
+        # 3. Apply the SAME gamma math as the render path (so preview == output)
+        depth_gamma_val = float(params.get("depth_gamma", 1.0))
+        if round(depth_gamma_val, 2) != 1.0:
+            # Math: 1.0 - (1.0 - depth)^gamma
+            depth_normalized = 1.0 - np.power(
+                1.0 - np.clip(depth_normalized, 0, 1), depth_gamma_val
+            )
+            depth_normalized = np.clip(depth_normalized, 0, 1)
+
+        logger.debug(
+            f"Final normalized depth shape: {depth_normalized.shape}, range: [{depth_normalized.min():.2f}, {depth_normalized.max():.2f}]"
+        )
+
+        # --- Perform Splatting ---
+        stereo_projector = ForwardWarpStereo(occlu_map=True).cuda()
+        # Ensure depth map is resized to the target resolution (low-res or original)
+        disp_map_tensor = (
+            torch.from_numpy(depth_normalized).unsqueeze(0).unsqueeze(0).float().cuda()
+        )
+
+        # Resize Disparity Map to match the (potentially resized) Left Eye
+        if H_target != disp_map_tensor.shape[2] or W_target != disp_map_tensor.shape[3]:
+            logger.debug(f"Resizing depth map to match target {W_target}x{H_target}.")
+            disp_map_tensor = F.interpolate(
+                disp_map_tensor,
+                size=(H_target, W_target),
+                mode="bilinear",
+                align_corners=False,
+            )
+
+        disp_map_tensor = (disp_map_tensor - params["convergence_point"]) * 2.0
+
+        tv_disp_comp = 1.0
+        if not enable_global_norm:
+            try:
+                if getattr(self, "previewer", None) is not None:
+                    _bd = int(getattr(self.previewer, "_depth_bit_depth", 8) or 8)
+                    _dpath = getattr(self.previewer, "_depth_path", None)
+                    if _bd > 8 and _dpath:
+                        if not hasattr(self, "_depth_color_range_cache"):
+                            self._depth_color_range_cache = {}
+                        if _dpath not in self._depth_color_range_cache:
+                            _info = get_video_stream_info(_dpath)
+                            self._depth_color_range_cache[_dpath] = str((_info or {}).get("color_range", "unknown")).lower()
+                        if self._depth_color_range_cache.get(_dpath) == "tv":
+                            tv_disp_comp = 1.0 / (DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM)
+            except Exception:
+                tv_disp_comp = 1.0
+
+
+        # Calculate disparity in pixels based on the TARGET width (W_target)
+        actual_max_disp_pixels = (params["max_disp"] / 20.0 / 100.0) * W_target * tv_disp_comp
+        disp_map_tensor = disp_map_tensor * actual_max_disp_pixels
+
+        # Preview-only depth/pop separation metrics (percent of screen width)
+        # Uses a lightweight sampled scan of the current normalized depth map.
+        try:
+            if getattr(self, "previewer", None) is not None and hasattr(
+                self.previewer, "set_depth_pop_metrics"
+            ):
+                show_metrics = bool(
+                    self.depth_pop_enabled_var.get()
+                ) and preview_source in (
+                    "Anaglyph 3D",
+                    "Dubois Anaglyph",
+                    "Optimized Anaglyph",
+                )
+                if show_metrics:
+                    _stride = 8  # sample stride for speed (1/64 pixels)
+                    _sample = depth_normalized[::_stride, ::_stride]
+                    _valid = (_sample > 0.001) & (_sample < 0.999)
+                    if _valid.any():
+                        _ds = _sample[_valid]
+                        # Disparity percent of screen width (W cancels out):
+                        # disp_pct = (depth - conv) * 2 * (max_disp / 20)
+                        _disp_pct = (
+                            (_ds - params["convergence_point"])
+                            * 2.0
+                            * (params["max_disp"] / 20.0) * tv_disp_comp
+                        )
+                        _min_pct = float(_disp_pct.min())
+                        _max_pct = float(_disp_pct.max())
+                        _depth_pct = max(0.0, -_min_pct)  # screen-behind
+                        _pop_pct = float(
+                            _max_pct
+                        )  # screen-out (can be negative if all behind)
+                    else:
+                        _depth_pct, _pop_pct = 0.0, 0.0
+                    sig = None
+                    try:
+                        _dp_path = getattr(self.previewer, "_depth_path", "")
+                        sig = self._dp_total_signature(_dp_path, params.get("convergence_point", 0.0), params.get("max_disp", 0.0), params.get("depth_gamma", 1.0))
+                    except Exception:
+                        sig = None
+                    try:
+                        self.previewer.set_depth_pop_metrics(_depth_pct, _pop_pct, sig)
+                    except TypeError:
+                        self.previewer.set_depth_pop_metrics(_depth_pct, _pop_pct)
+                else:
+                    try:
+                        self.previewer.set_depth_pop_metrics(None, None, None)
+                    except TypeError:
+                        self.previewer.set_depth_pop_metrics(None, None)
+        except Exception:
+            pass
+
+        with torch.no_grad():
+            # Use the potentially resized Left Eye
+            right_eye_tensor_raw, occlusion_mask = stereo_projector(
+                left_eye_tensor_resized, disp_map_tensor
+            )
+
+            # Apply low-res specific post-processing
+            right_eye_tensor = right_eye_tensor_raw
+
+        # --- Apply black borders for Anaglyph and Wigglegram ---
+        if preview_source in [
+            "Anaglyph 3D",
+            "Dubois Anaglyph",
+            "Optimized Anaglyph",
+            "Wigglegram",
+        ]:
+            l_pct = params.get("left_border_pct", 0.0)
+            r_pct = params.get("right_border_pct", 0.0)
+
+            l_px = int(round(l_pct * W_target / 100.0))
+            r_px = int(round(r_pct * W_target / 100.0))
+
+            if l_px > 0:
+                # Left eye: Opaque black border on the left side
+                # left_eye_tensor_resized is (1, 3, H, W)
+                left_eye_tensor_resized[:, :, :, :l_px] = 0.0
+            if r_px > 0:
+                # Right eye: Opaque black border on the right side
+                # right_eye_tensor is (1, 3, H, W)
+                right_eye_tensor[:, :, :, -r_px:] = 0.0
+
+        if preview_source == "Splat Result" or preview_source == "Splat Result(Low)":
+            final_tensor = right_eye_tensor.cpu()
+        elif (
+            preview_source == "Occlusion Mask"
+            or preview_source == "Occlusion Mask(Low)"
+        ):
+            final_tensor = occlusion_mask.repeat(1, 3, 1, 1).cpu()
+
+        elif preview_source == "Depth Map":
+            # Direct grayscale view (visualization-only TV-range expansion for 10-bit depth, when tagged 'tv')
+            depth_vis = depth_normalized
+            try:
+                if (
+                    DEPTH_VIS_APPLY_TV_RANGE_EXPANSION_10BIT
+                    and getattr(self, "previewer", None) is not None
+                ):
+                    bd = int(getattr(self.previewer, "_depth_bit_depth", 8) or 8)
+                    dpath = getattr(self.previewer, "_depth_path", None)
+                    if bd >= 10 and isinstance(dpath, str) and dpath:
+                        if not hasattr(self, "_depth_color_range_cache"):
+                            self._depth_color_range_cache = {}
+                        rng = self._depth_color_range_cache.get(dpath)
+                        if rng is None:
+                            info = get_video_stream_info(dpath)
+                            rng = str(
+                                (info or {}).get("color_range")
+                                or (info or {}).get("range")
+                                or ""
+                            ).lower()
+                            self._depth_color_range_cache[dpath] = rng
+                        if rng == "tv":
+                            depth_vis = (depth_vis - DEPTH_VIS_TV10_BLACK_NORM) / (
+                                DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM
+                            )
+            except Exception:
+                pass
+            depth_vis_uint8 = (np.clip(depth_vis, 0, 1) * 255).astype(np.uint8)
+            depth_vis_3ch = np.stack([depth_vis_uint8] * 3, axis=-1)
+            final_tensor = (
+                torch.from_numpy(depth_vis_3ch).permute(2, 0, 1).unsqueeze(0).float()
+                / 255.0
+            )
+
+        elif preview_source == "Depth Map (Color)":
+            # Diagnostic color view (visualization-only TV-range expansion for 10-bit depth, when tagged 'tv')
+            depth_vis = depth_normalized
+            try:
+                if (
+                    DEPTH_VIS_APPLY_TV_RANGE_EXPANSION_10BIT
+                    and getattr(self, "previewer", None) is not None
+                ):
+                    bd = int(getattr(self.previewer, "_depth_bit_depth", 8) or 8)
+                    dpath = getattr(self.previewer, "_depth_path", None)
+                    if bd >= 10 and isinstance(dpath, str) and dpath:
+                        if not hasattr(self, "_depth_color_range_cache"):
+                            self._depth_color_range_cache = {}
+                        rng = self._depth_color_range_cache.get(dpath)
+                        if rng is None:
+                            info = get_video_stream_info(dpath)
+                            rng = str(
+                                (info or {}).get("color_range")
+                                or (info or {}).get("range")
+                                or ""
+                            ).lower()
+                            self._depth_color_range_cache[dpath] = rng
+                        if rng == "tv":
+                            depth_vis = (depth_vis - DEPTH_VIS_TV10_BLACK_NORM) / (
+                                DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM
+                            )
+            except Exception:
+                pass
+            depth_vis_uint8 = (np.clip(depth_vis, 0, 1) * 255).astype(np.uint8)
+            vis_color = cv2.applyColorMap(depth_vis_uint8, cv2.COLORMAP_VIRIDIS)
+            vis_rgb = cv2.cvtColor(vis_color, cv2.COLOR_BGR2RGB)
+            final_tensor = (
+                torch.from_numpy(vis_rgb).permute(2, 0, 1).unsqueeze(0).float() / 255.0
+            )
+
+        elif preview_source == "Original (Left Eye)":
+            # Use the resized or original left eye depending on the low-res flag
+            final_tensor = left_eye_tensor_resized.cpu()
+        elif preview_source == "Anaglyph 3D":
+            left_np_anaglyph = (
+                left_eye_tensor_resized.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255
+            ).astype(np.uint8)
+            right_np_anaglyph = (
+                right_eye_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255
+            ).astype(np.uint8)
+            left_gray_np = cv2.cvtColor(left_np_anaglyph, cv2.COLOR_RGB2GRAY)
+            anaglyph_np = right_np_anaglyph.copy()
+            anaglyph_np[:, :, 0] = left_gray_np
+            final_tensor = (
+                torch.from_numpy(anaglyph_np).permute(2, 0, 1).float() / 255.0
+            ).unsqueeze(0)
+        elif preview_source == "Dubois Anaglyph":
+            left_np_anaglyph = (
+                left_eye_tensor_resized.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255
+            ).astype(np.uint8)
+            right_np_anaglyph = (
+                right_eye_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255
+            ).astype(np.uint8)
+            anaglyph_np = apply_dubois_anaglyph(left_np_anaglyph, right_np_anaglyph)
+            final_tensor = (
+                torch.from_numpy(anaglyph_np).permute(2, 0, 1).float() / 255.0
+            ).unsqueeze(0)
+        elif preview_source == "Optimized Anaglyph":
+            left_np_anaglyph = (
+                left_eye_tensor_resized.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255
+            ).astype(np.uint8)
+            right_np_anaglyph = (
+                right_eye_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255
+            ).astype(np.uint8)
+            anaglyph_np = apply_optimized_anaglyph(left_np_anaglyph, right_np_anaglyph)
+            final_tensor = (
+                torch.from_numpy(anaglyph_np).permute(2, 0, 1).float() / 255.0
+            ).unsqueeze(0)
+        elif preview_source == "Wigglegram":
+            # Pass the resized left eye and the splatted right eye
+            self.previewer._start_wigglegram_animation(
+                left_eye_tensor_resized.cpu(), right_eye_tensor.cpu()
+            )
+            return None
+        else:
+            final_tensor = right_eye_tensor.cpu()
+
+        pil_img = Image.fromarray(
+            (final_tensor.squeeze(0).permute(1, 2, 0).numpy() * 255).astype(np.uint8)
+        )
+
+        del stereo_projector, disp_map_tensor, right_eye_tensor_raw, occlusion_mask
+        release_cuda_memory()
+        logger.debug("--- Finished Preview Processing Callback ---")
+        return pil_img
+
+    def reset_to_defaults(self):
+        """Resets all GUI parameters to their default hardcoded values."""
+        if not messagebox.askyesno(
+            "Reset Settings",
+            "Are you sure you want to reset all settings to their default values?",
+        ):
+            return
+
+        self.input_source_clips_var.set("./input_source_clips")
+        self.input_depth_maps_var.set("./input_depth_maps")
+        self.output_splatted_var.set("./output_splatted")
+        self.max_disp_var.set("20.0")
+        self.process_length_var.set("-1")
+        self.enable_full_res_var.set(True)
+        self.batch_size_var.set("10")
+        self.enable_low_res_var.set(False)
+        self.pre_res_width_var.set("1920")
+        self.pre_res_height_var.set("1080")
+        self.low_res_batch_size_var.set("50")
+        self.dual_output_var.set(False)
+        self.enable_global_norm_var.set(False)
+        self.zero_disparity_anchor_var.set("0.5")
+        self.enable_sidecar_gamma_var.set(True)
+        self.enable_sidecar_blur_dilate_var.set(True)
+
+        self.border_manual_var.set(False)
+        self.border_width_var.set("0.0")
+        self.border_bias_var.set("0.0")
+
+        self.output_crf_var.set("23")
+        self.output_crf_full_var.set("23")
+        self.output_crf_low_var.set("23")
+        self.move_to_finished_var.set(True)
+
+        # Ensure UI/Toggles match the new reset states
+        self.toggle_processing_settings_fields()
+        self._on_border_manual_toggle()
+        self.on_slider_release()
+        self._save_config()
+        self.clear_processing_info()
+        self.status_label.config(text="Settings reset to defaults.")
+
+    def restore_finished_files(self):
+        """Moves all files from 'finished' folders back to their original input folders."""
+        if not messagebox.askyesno(
+            "Restore Finished Files",
+            "Are you sure you want to move all files from 'finished' folders back to their input directories?",
+        ):
+            return
+
+        source_clip_dir = self.input_source_clips_var.get()
+        depth_map_dir = self.input_depth_maps_var.get()
+
+        is_source_dir = os.path.isdir(source_clip_dir)
+        is_depth_dir = os.path.isdir(depth_map_dir)
+
+        if not (is_source_dir and is_depth_dir):
+            messagebox.showerror(
+                "Restore Error",
+                "Restore 'finished' operation is only applicable when Input Source Clips and Input Depth Maps are set to directories (batch mode). Please ensure current settings reflect this.",
+            )
+            self.status_label.config(text="Restore finished: Not in batch mode.")
+            return
+
+        finished_source_folder = os.path.join(source_clip_dir, "finished")
+        finished_depth_folder = os.path.join(depth_map_dir, "finished")
+
+        restored_count = 0
+        errors_count = 0
+
+        if os.path.isdir(finished_source_folder):
+            logger.info(f"==> Restoring source clips from: {finished_source_folder}")
+            for filename in os.listdir(finished_source_folder):
+                src_path = os.path.join(finished_source_folder, filename)
+                dest_path = os.path.join(source_clip_dir, filename)
+                if os.path.isfile(src_path):
+                    try:
+                        shutil.move(src_path, dest_path)
+                        restored_count += 1
+                        logger.debug(f"Moved '{filename}' to '{source_clip_dir}'")
+                    except Exception as e:
+                        errors_count += 1
+                        logger.error(f"Error moving source clip '{filename}': {e}")
+        else:
+            logger.info(
+                f"==> Finished source folder not found: {finished_source_folder}"
+            )
+
+        if os.path.isdir(finished_depth_folder):
+            logger.info(
+                f"==> Restoring depth maps and sidecars from: {finished_depth_folder}"
+            )
+            for filename in os.listdir(finished_depth_folder):
+                src_path = os.path.join(finished_depth_folder, filename)
+                dest_path = os.path.join(depth_map_dir, filename)
+                if os.path.isfile(src_path):
+                    try:
+                        shutil.move(src_path, dest_path)
+                        restored_count += 1
+                        logger.debug(f"Moved '{filename}' to '{depth_map_dir}'")
+                    except Exception as e:
+                        errors_count += 1
+                        logger.error(
+                            f"Error moving depth map/sidecar '{filename}': {e}"
+                        )
+        else:
+            logger.info(f"==> Finished depth folder not found: {finished_depth_folder}")
+
+        if restored_count > 0 or errors_count > 0:
+            self.clear_processing_info()
+            self.status_label.config(
+                text=f"Restore complete: {restored_count} files moved, {errors_count} errors."
+            )
+            messagebox.showinfo(
+                "Restore Complete",
+                f"Finished files restoration attempted.\n{restored_count} files moved.\n{errors_count} errors occurred.",
+            )
+        else:
+            self.clear_processing_info()
+            self.status_label.config(text="No files found to restore.")
+            messagebox.showinfo(
+                "Restore Complete", "No files found in 'finished' folders to restore."
+            )
+
+    def _round_slider_variable_value(self, tk_var: tk.Variable, decimals: int):
+        """Rounds the float/string value of a tk.Variable and sets it back."""
+        try:
+            current_value = float(tk_var.get())
+            rounded_value = round(current_value, decimals)
+            if current_value != rounded_value:
+                tk_var.set(rounded_value)
+                logger.debug(
+                    f"Rounded {current_value} to {rounded_value} (decimals={decimals})"
+                )
+        except ValueError:
+            pass
+
+    def _run_batch_process(self, settings):
+        """
+        Batch processing entry point.
+
+        In multi-file mode:
+          - 'From' and 'To' are treated as 1-based indices into the *GUI list*
+            (self.previewer.video_list), i.e. the same numbers you see when
+            jumping between clips.
+        In single-file mode:
+          - The From/To fields are ignored and the single video is processed.
+        """
+        try:
+            # --- 1. Basic setup (folder paths, discovered videos, etc.) ---
+            setup_result = self._setup_batch_processing(settings)
+            if "error" in setup_result:
+                logger.error(setup_result["error"])
+                messagebox.showerror("Batch Processing Error", setup_result["error"])
+                return
+
+            input_videos = setup_result["input_videos"]
+            is_single_file_mode = setup_result["is_single_file_mode"]
+            finished_source_folder = setup_result["finished_source_folder"]
+            finished_depth_folder = setup_result["finished_depth_folder"]
+
+            if not input_videos:
+                logger.error("No input videos found for processing.")
+                messagebox.showerror(
+                    "Processing Error", "No input videos found for processing."
+                )
+                return
+
+            # --- 2. Apply From/To range on the *preview list* when available ---
+            # In single-file mode, we always process the one file and ignore From/To.
+            if (
+                not is_single_file_mode
+                and hasattr(self, "previewer")
+                and getattr(self.previewer, "video_list", None)
+            ):
+                # The previewer list is what you see in the GUI (1/XXXX, 2/XXXX, ...).
+                available_entries = self.previewer.video_list
+                total_videos = len(available_entries)
+
+                # Defaults: full range
+                start_index_0 = 0  # 0-based
+                end_index_0 = total_videos  # exclusive
+
+                # Parse "From" (1-based in UI)
+                from_str = self.process_from_var.get().strip()
+                if from_str:
+                    try:
+                        from_val = int(from_str)
+                        if from_val > 0:
+                            # convert to 0-based, clamp to bounds
+                            start_index_0 = max(0, min(total_videos, from_val - 1))
+                    except ValueError:
+                        logger.warning(f"Invalid 'From' value '{from_str}', ignoring.")
+
+                # Parse "To" (1-based in UI, inclusive)
+                to_str = self.process_to_var.get().strip()
+                if to_str:
+                    try:
+                        to_val = int(to_str)
+                        if to_val > 0:
+                            # convert to exclusive end index, clamp, and ensure at least 1 video
+                            end_index_0 = max(
+                                start_index_0 + 1, min(total_videos, to_val)
+                            )
+                    except ValueError:
+                        logger.warning(f"Invalid 'To' value '{to_str}', ignoring.")
+
+                # Log the range in GUI-style indices if we're not using the full list
+                if start_index_0 > 0 or end_index_0 < total_videos:
+                    logger.info(
+                        f"Processing range: videos {start_index_0 + 1} to {end_index_0} "
+                        f"(out of {total_videos} total)"
+                    )
+
+                # Slice the preview list and build the actual video path list
+                selected_entries = available_entries[start_index_0:end_index_0]
+                sliced_videos = [
+                    entry.get("source_video")
+                    for entry in selected_entries
+                    if entry.get("source_video")
+                ]
+
+                input_videos = sliced_videos
+
+            else:
+                # Multi-file mode with no previewer/video_list: treat From/To as simple
+                # 1-based indices over the discovered input_videos list (old behavior).
+                # In *single-file* mode, we intentionally ignore From/To and leave
+                # input_videos unchanged so the current preview clip always runs.
+                if not is_single_file_mode:
+                    total_videos = len(input_videos)
+                    start_index_0 = 0
+                    end_index_0 = total_videos
+
+                    from_str = self.process_from_var.get().strip()
+                    if from_str:
+                        try:
+                            from_val = int(from_str)
+                            if from_val > 0:
+                                start_index_0 = max(0, min(total_videos, from_val - 1))
+                        except ValueError:
+                            logger.warning(
+                                f"Invalid 'From' value '{from_str}', ignoring."
+                            )
+
+                    to_str = self.process_to_var.get().strip()
+                    if to_str:
+                        try:
+                            to_val = int(to_str)
+                            if to_val > 0:
+                                end_index_0 = max(
+                                    start_index_0 + 1, min(total_videos, to_val)
+                                )
+                        except ValueError:
+                            logger.warning(f"Invalid 'To' value '{to_str}', ignoring.")
+
+                    if start_index_0 > 0 or end_index_0 < total_videos:
+                        logger.info(
+                            f"Processing range: videos {start_index_0 + 1} to {end_index_0} "
+                            f"(out of {total_videos} total)"
+                        )
+                    input_videos = input_videos[start_index_0:end_index_0]
+                # else: single-file mode -> From/To boxes are ignored on purpose
+
+            # After applying the range, make sure we still have something to do
+            if not input_videos:
+                logger.error(
+                    "No input videos left to process after applying From/To range."
+                )
+                messagebox.showerror(
+                    "Processing Error",
+                    "No input videos left to process after applying the From/To range.",
+                )
+                return
+
+            # --- 3. Determine total tasks for the progress bar ---
+            processing_tasks = self._get_defined_tasks(settings)
+            if not processing_tasks:
+                logger.error(
+                    "No processing tasks defined. Please enable at least one output resolution."
+                )
+                messagebox.showerror(
+                    "Processing Error",
+                    "No processing tasks defined. Please enable at least one output resolution.",
+                )
+                return
+
+            tasks_per_video = len(processing_tasks)
+            total_tasks = len(input_videos) * tasks_per_video
+            logger.info(
+                f"Total tasks to process: {total_tasks} "
+                f"({len(input_videos)} videos √ó {tasks_per_video} tasks each)"
+            )
+            self.progress_queue.put(("total", total_tasks))
+
+            overall_task_counter = 0
+
+            # --- 4. Main processing loop ---
+            for idx, video_path in enumerate(input_videos):
+                if self.stop_event.is_set():
+                    logger.info("==> Stopping processing due to user request")
+                    break
+
+                # Delegates all per-video work to the helper
+                tasks_processed, _ = self._process_single_video_tasks(
+                    video_path=video_path,
+                    settings=settings,
+                    initial_overall_task_counter=overall_task_counter,
+                    is_single_file_mode=is_single_file_mode,
+                    finished_source_folder=finished_source_folder,
+                    finished_depth_folder=finished_depth_folder,
+                )
+
+                overall_task_counter += tasks_processed
+
+        except Exception as e:
+            logger.error(
+                f"An unexpected error occurred during batch processing: {e}",
+                exc_info=True,
+            )
+            self.progress_queue.put(("status", f"Error: {e}"))
+            error_message = str(e)
+            self.after(
+                0,
+                lambda msg=error_message: messagebox.showerror(
+                    "Processing Error",
+                    f"An unexpected error occurred during batch processing: {msg}",
+                ),
+            )
+        finally:
+            release_cuda_memory()
+            self.progress_queue.put("finished")
+            self.after(0, self.clear_processing_info)
+
+    def run_fusion_sidecar_generator(self):
+        """Initializes and runs the FusionSidecarGenerator tool."""
+
+        # Use an external thread to prevent the GUI from freezing during the file scan
+        def worker():
+            self.status_label.config(
+                text="Starting Fusion Export Sidecar Generation..."
+            )
+            generator = FusionSidecarGenerator(self, self.sidecar_manager)
+            generator.generate_sidecars()
+
+        threading.Thread(target=worker, daemon=True).start()
+
+    def run_fusion_sidecar_generator_custom(self):
+        """Initializes and runs the FusionSidecarGenerator tool for custom sidecar export."""
+
+        def worker():
+            self.status_label.config(
+                text="Starting Custom Fusion Export Sidecar Generation..."
+            )
+            generator = FusionSidecarGenerator(self, self.sidecar_manager)
+            generator.generate_custom_sidecars()
+
+        threading.Thread(target=worker, daemon=True).start()
+
+    def run_preview_auto_converge(self, force_run=False):
+        """
+        Starts the Auto-Convergence pre-pass on the current preview clip in a thread,
+        and updates the convergence slider/preview upon completion.
+        'force_run=True' is used when triggered by the combo box, as validation is needed.
+        """
+        if not hasattr(self, "previewer") or not self.previewer.source_readers:
+            if force_run:
+                messagebox.showwarning(
+                    "Auto-Converge Preview",
+                    "Please load a video in the Previewer first.",
+                )
+                self.auto_convergence_combo.set("Off")  # Reset combo on fail
+            return
+
+        current_index = self.previewer.current_video_index
+        if current_index == -1:
+            if force_run:
+                messagebox.showwarning(
+                    "Auto-Converge Preview",
+                    "No video is currently selected for processing.",
+                )
+                self.auto_convergence_combo.set("Off")  # Reset combo on fail
+            return
+
+        mode = self.auto_convergence_mode_var.get()
+        if mode == "Off":
+            if (
+                force_run
+            ):  # This should be caught by the cache check, but as a safeguard
+                return
+            messagebox.showwarning(
+                "Auto-Converge Preview",
+                "Auto-Convergence Mode must be set to 'Average', 'Peak', or 'Hybrid'.",
+            )
+            return
+
+        current_source_dict = self.previewer.video_list[current_index]
+        single_video_path = current_source_dict.get("source_video")
+        single_depth_path = current_source_dict.get("depth_map")
+
+        # --- NEW: Check if calculation is already done for a different mode/path ---
+        is_path_mismatch = single_depth_path != self._auto_conv_cached_path
+        is_cache_complete = (self._auto_conv_cache["Average"] is not None) or (
+            self._auto_conv_cache["Peak"] is not None
+        )
+
+        # If running from the combo box (force_run=True) AND the cache is incomplete
+        # BUT the path has changed, we must clear the cache and run.
+        if force_run and is_path_mismatch and is_cache_complete:
+            logger.info("New video detected. Clearing Auto-Converge cache.")
+            self._auto_conv_cache = {"Average": None, "Peak": None}
+            self._auto_conv_cached_path = None
+
+        # Validate paths
+        if (
+            not isinstance(single_video_path, str)
+            or not os.path.exists(single_video_path)
+            or not isinstance(single_depth_path, str)
+            or not os.path.exists(single_depth_path)
+        ):
+            messagebox.showerror(
+                "Auto-Converge Preview Error",
+                f"Invalid video or depth map path.\nVideo: {single_video_path}\nDepth: {single_depth_path}",
+            )
+            if force_run:
+                self.auto_convergence_combo.set("Off")
+            return
+
+        try:
+            current_anchor = float(self.zero_disparity_anchor_var.get())
+            process_length = int(self.process_length_var.get())
+            batch_size = int(self.batch_size_var.get())
+            gamma = self._safe_float(self.depth_gamma_var, 1.0)
+        except ValueError as e:
+            messagebox.showerror(
+                "Auto-Converge Preview Error",
+                f"Invalid input for slider or process length: {e}",
+            )
+            if force_run:
+                self.auto_convergence_combo.set("Off")
+            return
+
+        # Set running flag and disable inputs
+        self._is_auto_conv_running = True
+        self.btn_auto_converge_preview.config(state="disabled")
+        self.start_button.config(state="disabled")
+        self.start_single_button.config(state="disabled")
+        self.auto_convergence_combo.config(state="disabled")  # Disable combo during run
+
+        self.status_label.config(
+            text=f"Auto-Convergence pre-pass started ({mode} mode)..."
+        )
+
+        # Start the calculation in a new thread
+        # Start the calculation in a new thread
+        worker_args = (
+            single_video_path,
+            single_depth_path,
+            process_length,
+            batch_size,
+            current_anchor,
+            gamma,
+            mode,
+        )
+        self.auto_converge_thread = threading.Thread(
+            target=self._auto_converge_worker, args=worker_args
+        )
+        self.auto_converge_thread.start()
+
+    def _dp_total_signature(self, depth_path: str, conv: float, max_disp: float, gamma: float) -> str:
+        """Signature for caching Total(D+P) metrics.
+
+        IMPORTANT: Convergence changes should NOT invalidate the cached display/estimate.
+        Also tolerate older call sites that might accidentally swap (conv, max_disp).
+        """
+        try:
+            a = float(conv)
+            b = float(max_disp)
+
+            # If args look swapped (conv should be ~0..1, max_disp usually > 1), swap them.
+            if 0.0 <= a <= 1.0 and b > 1.0:
+                conv_val = a
+                max_disp_val = b
+            elif 0.0 <= b <= 1.0 and a > 1.0:
+                conv_val = b
+                max_disp_val = a
+            else:
+                conv_val = a
+                max_disp_val = b
+
+            # NOTE: conv_val intentionally excluded to prevent cache resets when convergence changes.
+            return f"{depth_path}|{float(max_disp_val):.4f}|{float(gamma):.4f}"
+        except Exception:
+            return str(depth_path)
+
+    def _estimate_dp_total_max_for_depth_video(
+        self,
+        depth_path: str,
+        convergence_point: float,
+        max_disp: float,
+        depth_gamma: float,
+        sample_frames: int = 10,
+        pixel_stride: int = 8,
+        total_frames_override: Optional[int] = None,
+        *,
+        params: Optional[dict] = None,
+    ) -> Optional[float]:
+        """
+        Estimates the maximum Total(D+P) for a clip by sampling frames.
+
+        IMPORTANT: This aims to match the preview/render math as closely as possible:
+          - Reads depth in RAW code values (10-bit stays 0..1023-ish, not RGB-expanded)
+          - Runs the same depth pre-processing (dilate/blur) used by preview/render
+          - Applies the same normalization policy and gamma curve:
+                depth = clip(depth, 0..1)
+                depth = 1 - (1 - depth) ** gamma
+        """
+        if not depth_path or not os.path.exists(depth_path):
+            return None
+
+        # Grab the same pre-proc knobs the preview pipeline uses (fallback to GUI vars if not provided).
+        p = params or {}
+
+        def _pfloat(key: str, default: float) -> float:
+            try:
+                v = p.get(key, default)
+                return float(v)
+            except Exception:
+                return float(default)
+
+        # NOTE: sidecars store these keys (depth_*) via _save_current_sidecar_data.
+        depth_dilate_size_x = _pfloat("depth_dilate_size_x", self._safe_float(self.depth_dilate_size_x_var))
+        depth_dilate_size_y = _pfloat("depth_dilate_size_y", self._safe_float(self.depth_dilate_size_y_var))
+        depth_blur_size_x = _pfloat("depth_blur_size_x", self._safe_float(self.depth_blur_size_x_var))
+        depth_blur_size_y = _pfloat("depth_blur_size_y", self._safe_float(self.depth_blur_size_y_var))
+        depth_dilate_left = _pfloat("depth_dilate_left", self._safe_float(self.depth_dilate_left_var))
+        depth_blur_left = _pfloat("depth_blur_left", self._safe_float(self.depth_blur_left_var))
+
+        # Gamma used for the estimator math (prefer explicit param dict if present).
+        try:
+            depth_gamma = float(p.get("depth_gamma", depth_gamma))
+        except Exception:
+            depth_gamma = float(depth_gamma)
+
+        # Build sample indices (evenly spaced, clamped)
+        total_frames = 0
+        try:
+            if total_frames_override is not None:
+                total_frames = int(total_frames_override)
+        except Exception:
+            total_frames = 0
+
+        if total_frames <= 0:
+            try:
+                tmp = VideoReader(depth_path, ctx=cpu(0))
+                total_frames = len(tmp)
+                del tmp
+            except Exception:
+                total_frames = 0
+
+        if total_frames <= 0:
+            return None
+
+        sample_frames = int(max(1, sample_frames))
+        if sample_frames >= total_frames:
+            indices = list(range(total_frames))
+        else:
+            indices = [int(round(i * (total_frames - 1) / (sample_frames - 1))) for i in range(sample_frames)]
+        # Ensure strictly increasing (helps the sequential reader)
+        indices = sorted(set(max(0, min(total_frames - 1, i)) for i in indices))
+
+        # Try to preserve RAW depth values (10-bit+) by using the same ffmpeg-backed reader used by render.
+        depth_stream_info = None
+        bit_depth = 8
+        pix_fmt = ""
+        try:
+            depth_stream_info = get_video_stream_info(depth_path)
+            bit_depth = _infer_depth_bit_depth(depth_stream_info)
+            pix_fmt = str((depth_stream_info or {}).get("pix_fmt", ""))
+        except Exception:
+            depth_stream_info = None
+            bit_depth = 8
+            pix_fmt = ""
+
+        # Determine an output size for sampling. If the previewer is active, match its current depth native size.
+        out_w = None
+        out_h = None
+        try:
+            if self.previewer is not None and getattr(self.previewer, "_depth_path", None) == depth_path:
+                out_w = int(getattr(self.previewer, "_depth_native_w", 0) or 0)
+                out_h = int(getattr(self.previewer, "_depth_native_h", 0) or 0)
+        except Exception:
+            out_w, out_h = None, None
+
+        # Fallback: take from ffprobe stream info
+        if not out_w or not out_h:
+            try:
+                out_w = int((depth_stream_info or {}).get("width", 0) or 0)
+                out_h = int((depth_stream_info or {}).get("height", 0) or 0)
+            except Exception:
+                out_w, out_h = 0, 0
+
+        if not out_w or not out_h:
+            return None
+
+        # We only need sampled frames; no need to match clip resolution here (keeps it fast).
+        # This still matches parity better than RGB-expanded reads.
+        try:
+            depth_reader, _, _, _, _ = load_pre_rendered_depth(
+                depth_map_path=depth_path,
+                process_length=-1,
+                target_height=out_h,
+                target_width=out_w,
+                match_resolution_to_target=False,
+            )
+        except Exception:
+            # Fallback to Decord if the ffmpeg-backed reader cannot be created.
+            try:
+                depth_reader = VideoReader(depth_path, ctx=cpu(0))
+            except Exception:
+                return None
+
+        max_total = None
+
+        # Optional: adopt the same "sidecar forces GN off" behavior used in preview.
+        enable_global_norm = bool(p.get("enable_global_norm", False))
+        try:
+            sidecar_folder = self._get_sidecar_base_folder()
+            depth_map_basename = os.path.splitext(os.path.basename(depth_path))[0]
+            sidecar_ext = self.APP_CONFIG_DEFAULTS.get("SIDECAR_EXT", ".fssidecar")
+            json_sidecar_path = os.path.join(sidecar_folder, f"{depth_map_basename}{sidecar_ext}")
+            if os.path.exists(json_sidecar_path):
+                enable_global_norm = False
+        except Exception:
+            pass
+
+        # Fetch cached global min/max if GN is enabled (rare for estimator because sidecar usually exists).
+        global_min, global_max = 0.0, 1.0
+        if enable_global_norm:
+            try:
+                c = self._clip_norm_cache.get(depth_path)
+                if c:
+                    global_min = float(c.get("min", 0.0))
+                    global_max = float(c.get("max", 1.0))
+            except Exception:
+                global_min, global_max = 0.0, 1.0
+
+        for idx in indices:
+            try:
+                # Read raw frame as (1,H,W,1)
+                if hasattr(depth_reader, "seek"):
+                    depth_reader.seek(int(idx))
+                frame_np = depth_reader.get_batch([int(idx)]).asnumpy()
+
+                # Ensure numeric + channel-last gray
+                if frame_np.ndim == 3:
+                    # (1,H,W) -> (1,H,W,1)
+                    frame_np = frame_np[..., None]
+                elif frame_np.ndim == 4 and frame_np.shape[-1] >= 1:
+                    # If somehow RGB, take the first channel (depth stored as gray replicated)
+                    if frame_np.shape[-1] != 1:
+                        frame_np = frame_np[..., :1]
+                else:
+                    continue
+
+                frame_raw = frame_np.astype(np.float32, copy=False)
+
+                # Mirror preview: determine per-frame max content value (used in processing + normalization)
+                max_raw_content_value = float(np.max(frame_raw))
+                if max_raw_content_value < 1.0:
+                    max_raw_content_value = 1.0
+
+                # Pre-process (dilate/blur, left-edge ops) exactly like preview
+                try:
+                    processed = self._process_depth_batch(
+                        batch_depth_numpy_raw=frame_raw,
+                        depth_stream_info=depth_stream_info,
+                        depth_gamma=depth_gamma,
+                        depth_dilate_size_x=depth_dilate_size_x,
+                        depth_dilate_size_y=depth_dilate_size_y,
+                        depth_blur_size_x=depth_blur_size_x,
+                        depth_blur_size_y=depth_blur_size_y,
+                        is_low_res_task=False,
+                        max_raw_value=max_raw_content_value,
+                        global_depth_min=global_min,
+                        global_depth_max=global_max,
+                        depth_dilate_left=depth_dilate_left,
+                        depth_blur_left=depth_blur_left,
+                        debug_batch_index=0,
+                        debug_frame_index=int(idx),
+                        debug_task_name="EstimateMaxTotal",
+                    )
+                except Exception:
+                    processed = frame_raw  # fallback
+
+                # _process_depth_batch already returns normalized + gamma-applied depth in 0..1 (preview/render parity).
+                # Only fall back to manual scaling+gamma if the processor failed and returned raw code values.
+                try:
+                    if hasattr(processed, "ndim") and processed.ndim == 4:
+                        depth_norm = processed[0, ..., 0]
+                    elif hasattr(processed, "ndim") and processed.ndim == 3:
+                        depth_norm = processed[0, ...]
+                    else:
+                        depth_norm = processed
+                except Exception:
+                    depth_norm = processed[0, ..., 0]
+
+                try:
+                    maxv = float(np.max(depth_norm))
+                except Exception:
+                    maxv = 1.0
+
+                # If we're still in raw code space (e.g., 10-bit TV-range values ~64..940), scale using fixed ranges (NOT observed max).
+                if maxv > 1.5:
+                    if maxv <= 256.0:
+                        depth_norm = depth_norm / 255.0
+                    elif maxv <= 1024.0:
+                        depth_norm = depth_norm / 1023.0
+                    elif maxv <= 4096.0:
+                        depth_norm = depth_norm / 4095.0
+                    elif maxv <= 65536.0:
+                        depth_norm = depth_norm / 65535.0
+                    else:
+                        depth_norm = depth_norm / float(maxv)
+                    depth_norm = np.clip(depth_norm, 0.0, 1.0)
+                    if depth_gamma and abs(depth_gamma - 1.0) > 1e-6:
+                        inv = 1.0 - depth_norm
+                        inv = np.clip(inv, 0.0, 1.0)
+                        depth_norm = 1.0 - np.power(inv, float(depth_gamma))
+
+                depth_norm = np.clip(depth_norm, 0.0, 1.0)
+
+                # Compute Total(D+P) for this frame (match preview: stride sample + ignore holes)
+                ds = depth_norm[::max(1, int(pixel_stride)), ::max(1, int(pixel_stride))].astype(np.float32, copy=False)
+                valid = (ds > 0.001) & (ds < 0.999)
+                if not np.any(valid):
+                    continue
+                dmin = float(np.min(ds[valid]))
+                dmax = float(np.max(ds[valid]))
+
+                tv_disp_comp = 1.0
+                if not enable_global_norm:
+                    try:
+                        if _infer_depth_bit_depth(depth_stream_info) > 8 and str((depth_stream_info or {}).get("color_range", "unknown")).lower() == "tv":
+                            tv_disp_comp = 1.0 / (DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM)
+                    except Exception:
+                        tv_disp_comp = 1.0
+
+                scale = 2.0 * (float(max_disp) / 20.0) * tv_disp_comp
+                min_pct = (dmin - float(convergence_point)) * scale
+                max_pct = (dmax - float(convergence_point)) * scale
+
+                depth_pct = abs(min_pct) if min_pct < 0 else 0.0
+                pop_pct = max_pct if max_pct > 0 else 0.0
+                total = float(depth_pct + pop_pct)
+
+                if max_total is None or total > max_total:
+                    max_total = total
+
+            except Exception:
+                continue
+
+        try:
+            if hasattr(depth_reader, "close"):
+                depth_reader.close()
+        except Exception:
+            pass
+
+        return max_total
+
+
+    def run_estimate_dp_total_max(self):
+        """Compute a quick sampled estimate of this clip's Max Total(D+P) and seed the overlay."""
+        try:
+            if getattr(self, "previewer", None) is None:
+                return
+            depth_path = getattr(self.previewer, "_depth_path", None)
+            if not depth_path:
+                return
+
+            try:
+                logger.info(f"Estimate Max Total(D+P): sampling clip depth map: {os.path.basename(depth_path)}")
+            except Exception:
+                pass
+
+            params = None
+            try:
+                # Use the same parameters the preview processing uses (includes sidecar overrides)
+                if getattr(self.previewer, "get_params_callback", None):
+                    params = self.previewer.get_params_callback()
+            except Exception:
+                params = None
+
+            if isinstance(params, dict):
+                conv = float(params.get("zero_disparity_anchor", self.zero_disparity_anchor_var.get()))
+                max_disp = float(params.get("max_disp", self.max_disp_var.get()))
+                gamma = float(params.get("depth_gamma", self.depth_gamma_var.get()))
+            else:
+                conv = float(self.zero_disparity_anchor_var.get())
+                max_disp = float(self.max_disp_var.get())
+                gamma = float(self.depth_gamma_var.get())
+
+            total_frames_override = None
+            try:
+                # Reuse already-known frame count from the active preview (avoids re-probing/decord hangs)
+                if getattr(self, "previewer", None) is not None and hasattr(self.previewer, "frame_scrubber"):
+                    total_frames_override = int(self.previewer.frame_scrubber.cget("to")) + 1
+            except Exception:
+                total_frames_override = None
+
+            sig = self._dp_total_signature(depth_path, conv, max_disp, gamma)
+
+            try:
+                if hasattr(self, "btn_est_dp_total"):
+                    self.btn_est_dp_total.config(state="disabled")
+            except Exception:
+                pass
+
+            est_start_ts = time.time()
+
+            def _worker():
+                try:
+                    est = self._estimate_dp_total_max_for_depth_video(
+                    depth_path, conv, max_disp, gamma, sample_frames=10, pixel_stride=8, total_frames_override=total_frames_override, params=params
+                )
+                except Exception as e:
+                    try:
+                        logger.exception(f"Estimate Max Total(D+P) failed: {e}")
+                    except Exception:
+                        pass
+                    est = None
+
+                def _apply():
+                    try:
+                        if est is not None:
+                            try:
+                                logger.info(f"Estimate Max Total(D+P): {float(est):.2f}%")
+                            except Exception:
+                                pass
+                            self._dp_total_est_cache[sig] = float(est)
+                            if getattr(self, "previewer", None) is not None and hasattr(self.previewer, "set_depth_pop_max_estimate"):
+                                ui_sig = None
+                            try:
+                                ui_sig = getattr(self.previewer, "_dp_signature", None)
+                            except Exception:
+                                ui_sig = None
+                            if not ui_sig:
+                                ui_sig = sig
+                            self.previewer.set_depth_pop_max_estimate(float(est), ui_sig)
+                    finally:
+                        try:
+                            if est is None:
+                                elapsed = None
+                                try:
+                                    elapsed = time.time() - est_start_ts
+                                except Exception:
+                                    pass
+                                if elapsed is not None:
+                                    logger.info(f"Estimate Max Total(D+P): (no result)  (took {elapsed:.2f}s)")
+                                else:
+                                    logger.info("Estimate Max Total(D+P): (no result)")
+                        except Exception:
+                            pass
+
+                        try:
+                            if hasattr(self, "btn_est_dp_total"):
+                                self.btn_est_dp_total.config(state="normal")
+                        except Exception:
+                            pass
+
+                self.after(0, _apply)
+
+            threading.Thread(target=_worker, daemon=True).start()
+        except Exception:
+            try:
+                if hasattr(self, "btn_est_dp_total"):
+                    self.btn_est_dp_total.config(state="normal")
+            except Exception:
+                pass
+
+    def _get_cached_dp_total_est_for_current(self) -> Optional[float]:
+        try:
+            if getattr(self, "previewer", None) is None:
+                return None
+            depth_path = getattr(self.previewer, "_depth_path", None)
+            if not depth_path:
+                return None
+            conv = float(self.zero_disparity_anchor_var.get())
+            max_disp = float(self.max_disp_var.get())
+            gamma = float(self.depth_gamma_var.get())
+            sig = self._dp_total_signature(depth_path, conv, max_disp, gamma)
+            v = self._dp_total_est_cache.get(sig, None)
+            return float(v) if v is not None else None
+        except Exception:
+            return None
+
+    def _auto_pass_csv_get_path(self) -> str:
+        try:
+            return os.path.join(self._get_sidecar_base_folder(), "auto_pass_export.csv")
+        except Exception:
+            return "auto_pass_export.csv"
+
+    def _auto_pass_csv_load_cache(self, csv_path: str) -> None:
+        """Load CSV rows into memory (keyed by source_video basename)."""
+        try:
+            self._auto_pass_csv_path = csv_path
+            rows = {}
+            if os.path.exists(csv_path):
+                with open(csv_path, "r", newline="", encoding="utf-8") as f:
+                    r = csv.DictReader(f)
+                    for row in r:
+                        # Prefer source_video as the stable key; fall back to depth_map for older exports.
+                        key = str(row.get("source_video", "")).strip()
+                        if not key:
+                            key = str(row.get("depth_map", "")).strip()
+                        if key:
+                            rows[key] = dict(row)
+            self._auto_pass_csv_cache = rows
+        except Exception:
+            self._auto_pass_csv_cache = {}
+
+    def _auto_pass_csv_flush_cache(self, fieldnames: list) -> None:
+        try:
+            if not self._auto_pass_csv_path or self._auto_pass_csv_cache is None:
+                return
+
+            # Always write the CSV (for Resolve / general interoperability)
+            with open(self._auto_pass_csv_path, "w", newline="", encoding="utf-8") as f:
+                w = csv.DictWriter(f, fieldnames=fieldnames)
+                w.writeheader()
+                for key in sorted(self._auto_pass_csv_cache.keys()):
+                    w.writerow(self._auto_pass_csv_cache[key])
+
+            # TSV export removed (CSV import works fine in LibreOffice after clicking OK).
+        except Exception:
+            pass
+
+    def _auto_pass_csv_update_row_from_current(self) -> None:
+        """If auto_pass_export.csv exists, update this clip's row when sidecar changes."""
+        try:
+            csv_path = self._auto_pass_csv_get_path()
+            if not os.path.exists(csv_path):
+                return
+
+            if self._auto_pass_csv_cache is None or self._auto_pass_csv_path != csv_path:
+                self._auto_pass_csv_load_cache(csv_path)
+
+            if getattr(self, "previewer", None) is None:
+                return
+            idx = getattr(self.previewer, "current_video_index", -1)
+            if idx is None or idx < 0:
+                return
+            src = self.previewer.video_list[idx].get("source_video", "")
+            depth = self.previewer.video_list[idx].get("depth_map", "")
+            depth_bn = os.path.basename(depth) if depth else ""
+            src_bn = os.path.basename(src) if src else ""
+
+            # Frame token: last underscore + digits at end of source basename (before extension).
+            frame_num = ""
+            try:
+                stem = os.path.splitext(src_bn)[0]
+                mfr = re.search(r"_([0-9]+)$", stem)
+                frame_num = mfr.group(1) if mfr else ""
+            except Exception:
+                frame_num = ""
+            if not frame_num:
+                # Fallback: try depth basename pattern "..._<digits>_depth"
+                try:
+                    stem = os.path.splitext(depth_bn)[0]
+                    if stem.endswith("_depth"):
+                        stem = stem[:-6]
+                    mfr = re.search(r"_([0-9]+)$", stem)
+                    frame_num = mfr.group(1) if mfr else ""
+                except Exception:
+                    frame_num = ""
+
+            res = self._get_current_sidecar_paths_and_data()
+            if not res:
+                return
+            _, _, current_data = res
+
+            dp_est = self._get_cached_dp_total_est_for_current()
+            if dp_est is None:
+                try:
+                    dp_est = current_data.get("dp_total_max_est", None)
+                except Exception:
+                    dp_est = None
+
+            # Best-effort: measured (render-time) max Total(D+P), if available
+            dp_true = None
+            try:
+                sig = self._dp_total_signature(
+                    depth,
+                    float(current_data.get("convergence_plane", 0.5)),
+                    float(current_data.get("max_disparity", 0.0)),
+                    float(current_data.get("gamma", 1.0)),
+                )
+                dp_true = self._dp_total_true_cache.get(sig, None)
+            except Exception:
+                dp_true = None
+            if dp_true is None:
+                try:
+                    dp_true = current_data.get("dp_total_max_true", None)
+                except Exception:
+                    dp_true = None
+
+            row = {
+                "frame": frame_num,
+                "source_video": src_bn,
+                "selected_depth_map": str(current_data.get("selected_depth_map", "")),
+                "convergence_plane": round(float(current_data.get("convergence_plane", 0.5)), 6),
+                "left_border": round(float(current_data.get("left_border", 0.0)), 3),
+                "right_border": round(float(current_data.get("right_border", 0.0)), 3),
+                "border_mode": str(current_data.get("border_mode", "")),
+                "set_disparity": round(float(current_data.get("max_disparity", 0.0)), 3),
+                "true_max_disp": round(float(dp_true), 3) if dp_true is not None else "",
+                "est_max_disp": round(float(dp_est), 3) if dp_est is not None else "",
+                "gamma": round(float(current_data.get("gamma", 1.0)), 3),
+            }
+
+            # Key by source basename (stable + avoids collisions across multi-map folders).
+            self._auto_pass_csv_cache[src_bn] = row
+
+            fieldnames = [
+                "frame",
+                "source_video",
+                "selected_depth_map",
+                "convergence_plane",
+                "left_border",
+                "right_border",
+                "border_mode",
+                "set_disparity",
+                "true_max_disp",
+                "est_max_disp",
+                "gamma",
+            ]
+            self._auto_pass_csv_flush_cache(fieldnames)
+        except Exception:
+            pass
+
+    def _auto_pass_csv_update_row_for_paths(
+        self,
+        source_video_path: str,
+        depth_map_path: str,
+        current_data: dict,
+        dp_total_max_true: Optional[float] = None,
+    ) -> None:
+        """Best-effort: update/merge a single row in the AUTO-PASS CSV (if it exists).
+
+        This is used by non-preview code paths (e.g. render) where we already know the
+        source/depth paths and a dict of the most relevant settings.
+        """
+        try:
+            csv_path = self._auto_pass_csv_get_path()
+            if not csv_path:
+                return
+            # Ensure destination folder exists (CSV may be created on-demand)
+            try:
+                os.makedirs(os.path.dirname(csv_path), exist_ok=True)
+            except Exception:
+                pass
+        except Exception:
+            return
+
+        try:
+            self._auto_pass_csv_load_cache(csv_path)
+        except Exception:
+            return
+
+        try:
+            depth_bn = os.path.basename(depth_map_path)
+            src_bn = os.path.basename(source_video_path)
+
+            # Frame token: last underscore + digits at end of source basename (before extension).
+            frame_num = ""
+            try:
+                stem = os.path.splitext(src_bn)[0]
+                mfr = re.search(r"_([0-9]+)$", stem)
+                frame_num = mfr.group(1) if mfr else ""
+            except Exception:
+                frame_num = ""
+            if not frame_num:
+                # Fallback: try depth basename pattern "..._<digits>_depth"
+                try:
+                    stem = os.path.splitext(depth_bn)[0]
+                    if stem.endswith("_depth"):
+                        stem = stem[:-6]
+                    mfr = re.search(r"_([0-9]+)$", stem)
+                    frame_num = mfr.group(1) if mfr else ""
+                except Exception:
+                    frame_num = ""
+
+            # Preserve anything already present for this clip unless we are explicitly overwriting it.
+            existing = dict(self._auto_pass_csv_cache.get(src_bn, {}))
+            row = dict(existing)
+
+            row.update(
+                {
+                    "frame": frame_num,
+                    "source_video": src_bn,
+                    "selected_depth_map": str(current_data.get("selected_depth_map", "")),
+                    "convergence_plane": round(float(current_data.get("convergence_plane", 0.5)), 6),
+                    "left_border": round(float(current_data.get("left_border", 0.0)), 3),
+                    "right_border": round(float(current_data.get("right_border", 0.0)), 3),
+                    "border_mode": str(current_data.get("border_mode", "")),
+                    "set_disparity": round(
+                        float(current_data.get("max_disparity", current_data.get("set_disparity", 0.0))),
+                        3,
+                    ),
+                    "gamma": round(float(current_data.get("gamma", 1.0)), 3),
+                }
+            )
+
+            # Optional columns (keep internal names, change only CSV keys)
+            try:
+                if current_data.get("dp_total_max_est", None) not in ("", None):
+                    row["est_max_disp"] = round(float(current_data.get("dp_total_max_est")), 3)
+            except Exception:
+                pass
+
+            if dp_total_max_true is not None:
+                try:
+                    row["true_max_disp"] = round(float(dp_total_max_true), 3)
+                except Exception:
+                    row["true_max_disp"] = dp_total_max_true
+
+            self._auto_pass_csv_cache[src_bn] = row
+
+            fieldnames = [
+                "frame",
+                "source_video",
+                "selected_depth_map",
+                "convergence_plane",
+                "left_border",
+                "right_border",
+                "border_mode",
+                "set_disparity",
+                "true_max_disp",
+                "est_max_disp",
+                "gamma",
+            ]
+            self._auto_pass_csv_flush_cache(fieldnames=fieldnames)
+        except Exception:
+            return
+
+    def _save_current_settings_and_notify(self):
+        """Saves current GUI settings to default config file and notifies the user."""
+        config_filename = self.APP_CONFIG_DEFAULTS["DEFAULT_CONFIG_FILENAME"]
+        try:
+            self._save_config()
+            # --- MODIFIED: Use the new dictionary constant in messages ---
+            self.status_label.config(text=f"Settings saved to {config_filename}.")
+            messagebox.showinfo(
+                "Settings Saved",
+                f"Current settings successfully saved to {config_filename}.",
+            )
+            # --- END MODIFIED ---
+        except Exception as e:
+            self.status_label.config(text="Settings save failed.")
+            # --- MODIFIED: Use the new dictionary constant in messages ---
+            messagebox.showerror(
+                "Save Error", f"Failed to save settings to {config_filename}:\n{e}"
+            )
+
+
+    def run_auto_pass(self) -> None:
+        """Run AUTO-PASS over the preview list (or From/To range) without rendering."""
+        if not getattr(self, "previewer", None) or not getattr(self.previewer, "video_list", None):
+            messagebox.showwarning("AUTO-PASS", "Load/Refresh the Preview list first.")
+            return
+
+        available_entries = self.previewer.video_list
+        total_videos = len(available_entries)
+        if total_videos == 0:
+            messagebox.showwarning("AUTO-PASS", "No clips found in the preview list.")
+            return
+
+        # Range selection (1-based in UI, 0-based internally)
+        start_index_0 = 0
+        end_index_0 = total_videos
+
+        try:
+            from_str = self.process_from_var.get().strip()
+            to_str = self.process_to_var.get().strip()
+
+            from_ui = int(from_str) if from_str else 1
+            to_ui = int(to_str) if to_str else total_videos
+
+            from_ui = max(1, min(from_ui, total_videos))
+            to_ui = max(1, min(to_ui, total_videos))
+
+            start_index_0 = from_ui - 1
+            end_index_0 = to_ui
+            if start_index_0 >= end_index_0:
+                raise ValueError("From must be <= To.")
+
+        except Exception:
+            messagebox.showerror(
+                "Invalid Range",
+                f"Please enter a valid From/To range between 1 and {total_videos}.",
+            )
+            return
+
+        # Snapshot the current GUI settings ONCE (thread-safe)
+        try:
+            base_sidecar_data = {
+                "convergence_plane": self._safe_float(self.zero_disparity_anchor_var, 0.5),
+                "max_disparity": self._safe_float(self.max_disp_var, 20.0),
+                "gamma": float(f"{self._safe_float(self.depth_gamma_var, 1.0):.2f}"),
+                "depth_dilate_size_x": self._safe_float(self.depth_dilate_size_x_var),
+                "depth_dilate_size_y": self._safe_float(self.depth_dilate_size_y_var),
+                "depth_blur_size_x": self._safe_float(self.depth_blur_size_x_var),
+                "depth_blur_size_y": self._safe_float(self.depth_blur_size_y_var),
+                "depth_dilate_left": self._safe_float(self.depth_dilate_left_var),
+                "depth_blur_left": int(round(self._safe_float(self.depth_blur_left_var))),
+                "depth_blur_left_mix": self._safe_float(self.depth_blur_left_mix_var, 0.5),
+                "selected_depth_map": self.selected_depth_map_var.get(),
+            }
+
+            auto_conv_mode = self.auto_convergence_mode_var.get()
+            border_mode = self.border_mode_var.get()
+            process_length = int(self.process_length_var.get())
+            batch_size = int(self.batch_size_var.get())
+
+            border_w = self._safe_float(self.border_width_var)
+            border_b = self._safe_float(self.border_bias_var)
+
+        except Exception as e:
+            messagebox.showerror("AUTO-PASS", f"Could not read current settings: {e}")
+            return
+
+        # Disable interactive controls while AUTO-PASS runs
+        self.stop_event.clear()
+        try:
+            self.stop_button.config(state="normal")
+        except Exception:
+            pass
+        try:
+            self.start_button.config(state="disabled")
+            self.start_single_button.config(state="disabled")
+        except Exception:
+            pass
+        try:
+            self.update_sidecar_button.config(state="disabled")
+        except Exception:
+            pass
+        try:
+            self.btn_auto_converge_preview.config(state="disabled")
+            self.btn_auto_pass.config(state="disabled")
+        except Exception:
+            pass
+
+        self.status_label.config(
+            text=f"AUTO-PASS running‚Ä¶ ({start_index_0 + 1}‚Äì{end_index_0} of {total_videos})"
+        )
+
+        worker_args = (
+            available_entries,
+            start_index_0,
+            end_index_0,
+            base_sidecar_data,
+            auto_conv_mode,
+            border_mode,
+            process_length,
+            batch_size,
+            border_w,
+            border_b,
+        )
+
+        t = threading.Thread(target=self._auto_pass_worker, args=worker_args, daemon=True)
+        t.start()
+
+    def _auto_pass_worker(
+        self,
+        available_entries,
+        start_index_0,
+        end_index_0,
+        base_sidecar_data,
+        auto_conv_mode,
+        border_mode,
+        process_length,
+        batch_size,
+        border_w,
+        border_b,
+    ) -> None:
+        total = max(0, end_index_0 - start_index_0)
+        completed = 0
+
+
+        # AUTO-PASS export (CSV) - optional helper output for timeline/Resolve workflows
+        rows_for_csv = []
+        csv_out_path = None
+        try:
+            csv_out_path = os.path.join(self._get_sidecar_base_folder(), "auto_pass_export.csv")
+        except Exception:
+            csv_out_path = "auto_pass_export.csv"
+        # Pre-compute manual borders from the current Width/Bias (used when mode is Off/Manual)
+        try:
+            w = float(border_w)
+            b = float(border_b)
+        except Exception:
+            w = 0.0
+            b = 0.0
+
+        if b >= 0:
+            manual_right = w
+            manual_left = w * (1.0 - b)
+        else:
+            manual_left = w
+            manual_right = w * (1.0 + b)
+
+        manual_left = min(5.0, max(0.0, manual_left))
+        manual_right = min(5.0, max(0.0, manual_right))
+
+        sidecar_ext = self.APP_CONFIG_DEFAULTS.get("SIDECAR_EXT", ".fssidecar")
+        sidecar_base_folder = self._get_sidecar_base_folder()
+        os.makedirs(sidecar_base_folder, exist_ok=True)
+
+        gamma = float(base_sidecar_data.get("gamma", 1.0))
+        max_disp = float(base_sidecar_data.get("max_disparity", 20.0))
+        fallback_anchor = float(base_sidecar_data.get("convergence_plane", 0.5))
+
+        for idx in range(start_index_0, end_index_0):
+            if self.stop_event.is_set():
+                break
+
+            entry = available_entries[idx]
+            rgb_path = entry.get("source_video")
+            depth_path = entry.get("depth_map")
+            if not rgb_path or not depth_path:
+                continue
+
+            depth_basename = os.path.splitext(os.path.basename(depth_path))[0]
+            json_sidecar_path = os.path.join(sidecar_base_folder, f"{depth_basename}{sidecar_ext}")
+
+            current_data = self.sidecar_manager.load_sidecar_data(json_sidecar_path)
+
+            # Apply current GUI settings snapshot (preserves overlap/bias from existing sidecar)
+            current_data.update(base_sidecar_data)
+
+            # 1) AUTO-CONVERGE (optional)
+            conv_val = fallback_anchor
+            if auto_conv_mode and auto_conv_mode != "Off":
+                avg_val, peak_val = self._determine_auto_convergence(
+                    rgb_path,
+                    depth_path,
+                    process_length,
+                    batch_size,
+                    fallback_anchor,
+                    gamma=gamma,
+                )
+                if auto_conv_mode == "Average":
+                    conv_val = avg_val
+                elif auto_conv_mode == "Peak":
+                    conv_val = peak_val
+                elif auto_conv_mode == "Hybrid":
+                    conv_val = 0.5 * (avg_val + peak_val)
+                else:
+                    conv_val = avg_val
+
+            current_data["convergence_plane"] = float(conv_val)
+
+            # 2) AUTO-BORDER (optional) ‚Äì runs AFTER convergence (conv_val affects borders)
+            if border_mode == "Auto Basic":
+                tv_disp_comp = 1.0
+                try:
+                    _info = get_video_stream_info(depth_path)
+                    if _infer_depth_bit_depth(_info) > 8 and str((_info or {}).get("color_range", "unknown")).lower() == "tv":
+                        tv_disp_comp = 1.0 / (DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM)
+                except Exception:
+                    tv_disp_comp = 1.0
+
+                width = max(0.0, (1.0 - conv_val) * 2.0 * (max_disp / 20.0) * tv_disp_comp)
+                width = min(5.0, width)
+                current_data["left_border"] = round(float(width), 3)
+                current_data["right_border"] = round(float(width), 3)
+                # Freeze borders into manual left/right so it's easy to tweak later
+                current_data["border_mode"] = "Manual"
+
+            elif border_mode == "Auto Adv.":
+                scan = self._scan_borders_for_depth_path(depth_path, float(conv_val), max_disp, gamma)
+                if scan:
+                    l_val, r_val = scan
+                else:
+                    l_val, r_val = 0.0, 0.0
+                current_data["left_border"] = float(l_val)
+                current_data["right_border"] = float(r_val)
+                current_data["border_mode"] = "Manual"
+
+            else:
+                # Off or Manual ‚Äì do not touch borders (preserve existing per-clip values)
+                pass
+
+            # Save sidecar
+            # Optional: estimate per-clip Max Total(D+P) (sampled) for CSV + later review
+            try:
+                dp_est = self._estimate_dp_total_max_for_depth_video(
+                    depth_path,
+                    float(current_data.get("convergence_plane", conv_val)),
+                    float(current_data.get("max_disparity", max_disp)),
+                    float(current_data.get("gamma", gamma)),
+                    sample_frames=10,
+                    pixel_stride=8,
+                    params=current_data,
+                )
+                if dp_est is not None:
+                    current_data["dp_total_max_est"] = round(float(dp_est), 3)
+            except Exception:
+                pass
+
+            self.sidecar_manager.save_sidecar_data(json_sidecar_path, current_data)
+
+
+            # Collect row for optional CSV export
+            try:
+                depth_bn = os.path.basename(depth_path)
+                src_bn = os.path.basename(rgb_path)
+                # Frame token: last underscore + digits at end of source basename (before extension).
+                frame_num = ""
+                try:
+                    stem = os.path.splitext(src_bn)[0]
+                    mfr = re.search(r"_([0-9]+)$", stem)
+                    frame_num = mfr.group(1) if mfr else ""
+                except Exception:
+                    frame_num = ""
+                if not frame_num:
+                    try:
+                        stem = os.path.splitext(depth_bn)[0]
+                        if stem.endswith("_depth"):
+                            stem = stem[:-6]
+                        mfr = re.search(r"_([0-9]+)$", stem)
+                        frame_num = mfr.group(1) if mfr else ""
+                    except Exception:
+                        frame_num = ""
+                rows_for_csv.append({
+                    "frame": frame_num,
+                    "source_video": src_bn,
+                    "selected_depth_map": str(current_data.get("selected_depth_map", "")),
+                    "convergence_plane": round(float(current_data.get("convergence_plane", 0.5)), 6),
+                    "left_border": round(float(current_data.get("left_border", 0.0)), 3),
+                    "right_border": round(float(current_data.get("right_border", 0.0)), 3),
+                    "border_mode": str(current_data.get("border_mode", "")),
+                    "set_disparity": round(float(current_data.get("max_disparity", max_disp)), 3),
+                    "true_max_disp": current_data.get("dp_total_max_true", ""),
+                    "est_max_disp": current_data.get("dp_total_max_est", ""),
+                    "gamma": round(float(current_data.get("gamma", gamma)), 3),
+                })
+            except Exception:
+                pass
+            completed += 1
+            self.after(
+                0,
+                lambda c=completed, t=total: self.status_label.config(
+                    text=f"AUTO-PASS‚Ä¶ {c}/{t}"
+                ),
+            )
+        # Write optional CSV export (best-effort; never blocks completion)
+        try:
+            if rows_for_csv and csv_out_path:
+                fieldnames = [
+                    "frame",
+                    "source_video",
+                    "selected_depth_map",
+                    "convergence_plane",
+                    "left_border",
+                    "right_border",
+                    "border_mode",
+                    "set_disparity",
+                    "true_max_disp",
+                    "est_max_disp",
+                    "gamma",
+                ]
+
+                # Merge/update existing CSV instead of overwriting (source_video basename is the key)
+                existing_rows = {}
+                try:
+                    if os.path.exists(csv_out_path):
+                        with open(csv_out_path, "r", newline="", encoding="utf-8") as rf:
+                            rcsv = csv.DictReader(rf)
+                            for rrow in rcsv:
+                                k = str(rrow.get("source_video", "")).strip()
+                                if k:
+                                    existing_rows[k] = dict(rrow)
+                except Exception:
+                    existing_rows = {}
+
+                for row in rows_for_csv:
+                    try:
+                        k = str(row.get("source_video", "")).strip()
+                        if k:
+                            existing_rows[k] = row
+                    except Exception:
+                        pass
+
+                with open(csv_out_path, "w", newline="", encoding="utf-8") as f:
+                    wcsv = csv.DictWriter(f, fieldnames=fieldnames)
+                    wcsv.writeheader()
+                    for k in sorted(existing_rows.keys()):
+                        wcsv.writerow(existing_rows[k])
+        except Exception:
+            pass
+
+        was_stopped = self.stop_event.is_set()
+        self.after(0, lambda: self._complete_auto_pass(completed, total, was_stopped))
+
+    def _complete_auto_pass(self, completed: int, total: int, was_stopped: bool) -> None:
+        try:
+            self.stop_button.config(state="disabled")
+        except Exception:
+            pass
+
+        # Re-enable controls
+        try:
+            self.start_button.config(state="normal")
+            self.start_single_button.config(state="normal")
+        except Exception:
+            pass
+        try:
+            self.btn_auto_converge_preview.config(state="normal")
+            self.btn_auto_pass.config(state="normal")
+        except Exception:
+            pass
+
+        self.stop_event.clear()
+        self._toggle_sidecar_update_button_state()
+
+        if was_stopped:
+            self.status_label.config(text=f"AUTO-PASS stopped ({completed}/{total}).")
+        else:
+            self.status_label.config(text=f"AUTO-PASS complete ({completed}/{total}).")
+
+        # Refresh current clip UI (convergence/border) so the results are immediately visible
+        try:
+            if getattr(self, "previewer", None) and 0 <= self.previewer.current_video_index < len(self.previewer.video_list):
+                _depth_map = self.previewer.video_list[self.previewer.current_video_index].get("depth_map")
+                if _depth_map:
+                    self.update_gui_from_sidecar(_depth_map)
+                    try:
+                        self.previewer.update_preview()
+                    except Exception:
+                        pass
+        except Exception:
+            pass
+
+    def _save_config(self):
+        """Saves current GUI settings to the default file."""
+        config = self._get_current_config()
+        config_filename = self.APP_CONFIG_DEFAULTS["DEFAULT_CONFIG_FILENAME"]
+        with open(config_filename, "w") as f:
+            json.dump(config, f, indent=4)
+
+    def _save_current_sidecar_data(
+        self,
+        is_auto_save: bool = False,
+        force_auto_L: Optional[float] = None,
+        force_auto_R: Optional[float] = None,
+    ) -> bool:
+        """
+        Core method to prepare data and save the sidecar file.
+
+        Args:
+            is_auto_save (bool): If True, logs are DEBUG/INFO, otherwise ERROR.
+            force_auto_L (float): Explicit left auto-border value to save.
+            force_auto_R (float): Explicit right auto-border value to save.
+
+        Returns:
+            bool: True on success, False on failure.
+        """
+        result = self._get_current_sidecar_paths_and_data()
+        if result is None:
+            if not is_auto_save:
+                messagebox.showwarning(
+                    "Sidecar Save", "Please load a video in the Previewer first."
+                )
+            return False
+
+        json_sidecar_path, depth_map_path, current_data = result
+
+        # 1. Get current GUI values (the data to override/save)
+        try:
+            gui_save_data = {
+                "convergence_plane": self._safe_float(
+                    self.zero_disparity_anchor_var, 0.5
+                ),
+                "max_disparity": self._safe_float(self.max_disp_var, 20.0),
+                "gamma": float(f"{self._safe_float(self.depth_gamma_var, 1.0):.2f}"),
+                "depth_dilate_size_x": self._safe_float(self.depth_dilate_size_x_var),
+                "depth_dilate_size_y": self._safe_float(self.depth_dilate_size_y_var),
+                "depth_blur_size_x": self._safe_float(self.depth_blur_size_x_var),
+                "depth_blur_size_y": self._safe_float(self.depth_blur_size_y_var),
+                "depth_dilate_left": self._safe_float(self.depth_dilate_left_var),
+                "depth_blur_left": int(
+                    round(self._safe_float(self.depth_blur_left_var))
+                ),
+                "depth_blur_left_mix": self._safe_float(
+                    self.depth_blur_left_mix_var, 0.5
+                ),
+                "selected_depth_map": self.selected_depth_map_var.get(),
+            }
+
+            # Convert Border Width/Bias to Left/Right for storage
+            w = self._safe_float(self.border_width_var)
+            b = self._safe_float(self.border_bias_var)
+            if b <= 0:
+                left_b = w
+                right_b = w * (1.0 + b)
+            else:
+                right_b = w
+                left_b = w * (1.0 - b)
+
+            # Auto Adv values: only save if we are in that mode OR if they were forced by a scan
+            final_auto_L = (
+                force_auto_L
+                if force_auto_L is not None
+                else self._safe_float(self.auto_border_L_var)
+            )
+            final_auto_R = (
+                force_auto_R
+                if force_auto_R is not None
+                else self._safe_float(self.auto_border_R_var)
+            )
+
+            # If not in Auto Adv and not forced, we might want to keep the sidecar's existing values
+            # instead of overwriting with GUI 0.0s. This prevents accidental clearing.
+            mode = self.border_mode_var.get()
+            if mode != "Auto Adv." and force_auto_L is None:
+                final_auto_L = current_data.get("auto_border_L", 0.0)
+                final_auto_R = current_data.get("auto_border_R", 0.0)
+
+            if final_auto_L is None:
+                final_auto_L = 0.0
+            if final_auto_R is None:
+                final_auto_R = 0.0
+
+            if mode == "Off":
+                # Preserve any existing per-clip borders; do not overwrite or clear.
+                pass
+            else:
+                gui_save_data.update(
+                    {
+                        "left_border": round(left_b, 3),
+                        "right_border": round(right_b, 3),
+                        "border_mode": mode,
+                        "auto_border_L": round(final_auto_L, 3),
+                        "auto_border_R": round(final_auto_R, 3),
+                    }
+                )
+        except ValueError:
+            logger.error("Sidecar Save: Invalid input value in GUI. Skipping save.")
+            if not is_auto_save:
+                messagebox.showerror(
+                    "Sidecar Error", "Invalid input value in GUI. Skipping save."
+                )
+            return False
+
+        # 2. Merge GUI values into current data (preserving overlap/bias)
+        current_data.update(gui_save_data)
+
+        # 3. Write the updated data back to the file using the manager
+        if self.sidecar_manager.save_sidecar_data(json_sidecar_path, current_data):
+            action = (
+                "Auto-Saved"
+                if is_auto_save
+                else ("Updated" if os.path.exists(json_sidecar_path) else "Created")
+            )
+
+            logger.info(f"{action} sidecar: {os.path.basename(json_sidecar_path)}")
+            self.status_label.config(text=f"{action} sidecar.")
+
+            # Update button text in case a file was just created
+            self._update_sidecar_button_text()
+
+            return True
+        else:
+            logger.error(
+                f"Sidecar Save: Failed to write sidecar file '{os.path.basename(json_sidecar_path)}'."
+            )
+            if not is_auto_save:
+                messagebox.showerror(
+                    "Sidecar Error",
+                    f"Failed to write sidecar file '{os.path.basename(json_sidecar_path)}'. Check logs.",
+                )
+            return False
+
+    def _save_debug_image(
+        self,
+        data: np.ndarray,
+        filename_tag: str,
+        batch_index: int,
+        frame_index: int,
+        task_name: str,
+    ):
+        """Saves a normalized (0-1) NumPy array as a grayscale PNG to a debug folder."""
+        if not self._debug_logging_enabled:
+            return
+
+        debug_dir = os.path.join(
+            os.path.dirname(self.input_source_clips_var.get()),
+            "splat_debug",
+            task_name,
+            "images",
+        )
+        os.makedirs(debug_dir, exist_ok=True)
+
+        # Create a filename that includes frame index, batch index, and tag
+        filename = os.path.join(
+            debug_dir, f"{frame_index:05d}_B{batch_index:02d}_{filename_tag}.png"
+        )
+
+        try:
+            # 1. Normalize data to 0-255 uint8 range for PIL
+            # If data is BxHxW, take the first frame (index 0)
+            if data.ndim == 3:
+                frame_np = data[0]
+            elif data.ndim == 4:
+                frame_np = data[0].squeeze()  # Assuming Bx1xHxW or similar
+            else:
+                frame_np = data  # Assume HxW
+
+            # 2. Ensure data is float 0-1 (if not already) and clip
+            if frame_np.dtype != np.float32:
+                # Assume raw values (e.g., 0-255) and normalize for visualization
+                frame_np = (
+                    frame_np.astype(np.float32) / frame_np.max()
+                    if frame_np.max() > 0
+                    else frame_np
+                )
+
+            frame_uint8 = (np.clip(frame_np, 0.0, 1.0) * 255).astype(np.uint8)
+
+            # 3. Save as Grayscale PNG
+            img = Image.fromarray(frame_uint8, mode="L")
+            img.save(filename)
+
+            logger.debug(
+                f"Saved debug image {filename_tag} (Shape: {frame_uint8.shape}) to {os.path.basename(debug_dir)}"
+            )
+        except Exception as e:
+            logger.error(f"Failed to save debug image {filename_tag}: {e}")
+
+    def _save_debug_numpy(
+        self,
+        data: np.ndarray,
+        filename_tag: str,
+        batch_index: int,
+        frame_index: int,
+        task_name: str,
+    ):
+        """Saves a NumPy array to a debug folder if debug logging is enabled."""
+        if not self._debug_logging_enabled:
+            return
+
+        output_path = self.output_splatted_var.get()
+        debug_root = os.path.join(os.path.dirname(output_path), "splat_debug")
+
+        # 1. Save NPZ (Existing Logic)
+        debug_dir_npz = os.path.join(debug_root, task_name)
+        os.makedirs(debug_dir_npz, exist_ok=True)
+        filename_npz = os.path.join(
+            debug_dir_npz, f"{frame_index:05d}_B{batch_index:02d}_{filename_tag}.npz"
+        )
+        logger.debug(f"Save path {filename_tag}")
+
+        try:
+            np.savez_compressed(filename_npz, data=data)
+            logger.debug(
+                f"Saved debug array {filename_tag} (Shape: {data.shape}) to {os.path.basename(debug_dir_npz)}"
+            )
+        except Exception as e:
+            logger.error(f"Failed to save debug array {filename_tag}: {e}")
+
+        # 2. Save PNG Image (New Logic)
+        self._save_debug_image(data, filename_tag, batch_index, frame_index, task_name)
+
+    def save_settings(self):
+        """Saves current GUI settings to a user-selected JSON file."""
+        filename = filedialog.asksaveasfilename(
+            defaultextension=".json",
+            filetypes=[("JSON files", "*.json")],
+            title="Save Settings to File",
+        )
+        if not filename:
+            return
+
+        try:
+            config_to_save = self._get_current_config()
+            with open(filename, "w") as f:
+                json.dump(config_to_save, f, indent=4)
+
+            messagebox.showinfo(
+                "Settings Saved",
+                f"Successfully saved settings to:\n{os.path.basename(filename)}",
+            )
+            self.status_label.config(text="Settings saved.")
+
+        except Exception as e:
+            messagebox.showerror(
+                "Save Error",
+                f"Failed to save settings to {os.path.basename(filename)}:\n{e}",
+            )
+            self.status_label.config(text="Settings save failed.")
+
+    def _set_input_state(self, state):
+        """Sets the state of all input widgets to 'normal' or 'disabled'."""
+
+        # Helper to set the state of all children in a frame
+        def set_frame_children_state(frame, state, exclude_frames=False):
+            """Recursively sets the state of all configurable widgets within a frame."""
+            for child in frame.winfo_children():
+                child_type = child.winfo_class()
+
+                # Check if the child is a Frame/LabelFrame that we need to recurse into
+                if (
+                    isinstance(child, (ttk.Frame, tk.Frame, ttk.LabelFrame))
+                    and not exclude_frames
+                ):
+                    set_frame_children_state(child, state, exclude_frames)
+
+                # Check for widgets that accept the 'state' configuration
+                if child_type in ("TEntry", "TButton", "TCheckbutton", "TCombobox"):
+                    try:
+                        # Use a keyword argument to pass the state
+                        child.config(state=state)
+                    except tk.TclError:
+                        # Some buttons/labels might throw an error if they don't support 'state' directly,
+                        # but Entries, Buttons, and Checkbuttons should be fine.
+                        pass
+
+                # Special handling for labels whose colors might need adjusting if they are linked to entry/button states
+                # (Not needed for simple ttk styles, but left for reference)
+
+        # --- 1. Top-level Frames ---
+
+        # Folder Frame (Input/Output Paths)
+        set_frame_children_state(self.folder_frame, state)
+
+        # Output Settings Frame (Max Disp, CRF, etc.)
+        set_frame_children_state(self.output_settings_frame, state)
+
+        # --- 2. Depth/Resolution Frames (Containers) ---
+
+        # Process Resolution Frame (Left Side)
+        set_frame_children_state(self.preprocessing_frame, state)
+
+        # Depth Map Pre-processing Container (Right Side)
+        set_frame_children_state(self.depth_settings_container, state)
+
+        # --- CRITICAL FIX: Explicitly re-enable slider widgets if state is 'normal' ---
+        if state == "normal" and hasattr(self, "widgets_to_disable"):
+            for widget in self.widgets_to_disable:
+                # ttk.Scale can use 'normal' or 'disabled'
+                widget.config(state="normal")
+
+        if hasattr(self, "update_sidecar_button"):
+            if state == "disabled":
+                self.update_sidecar_button.config(state="disabled")
+            else:  # state == 'normal'
+                # When batch is done, re-apply the sidecar override logic immediately
+                self._toggle_sidecar_update_button_state()
+
+        # 3. Re-apply the specific field enable/disable logic
+        # This is CRITICAL. If we set state='normal' for everything,
+        # toggle_processing_settings_fields will correctly re-disable the Low Res W/H fields
+        # if the "Enable Low Resolution" checkbox is unchecked.
+        if hasattr(self, "previewer"):
+            self.previewer.set_ui_processing_state(state == "disabled")
+
+        if state == "normal":
+            self.toggle_processing_settings_fields()
+
+    def _set_saved_geometry(self: "SplatterGUI"):
+        """Applies the saved window width and position, with dynamic height."""
+        # Ensure the window is visible and all widgets are laid out for accurate height calculation
+        self.update_idletasks()
+
+        # 1. Use the saved/default width and height, with fallbacks
+        current_width = self.window_width
+        saved_height = self.window_height
+
+        # Recalculate height only if we are using the fallback default, otherwise respect saved size
+        if saved_height == 750:
+            calculated_height = self.winfo_reqheight()
+            if calculated_height < 100:
+                calculated_height = 750
+            current_height = calculated_height
+        else:
+            current_height = saved_height
+
+        # Fallback if saved width is invalid or too small
+        if current_width < 200:  # Minimum sensible width
+            current_width = 620  # Use default width
+
+        # 2. Construct the geometry string
+        geometry_string = f"{current_width}x{current_height}"
+        if self.window_x is not None and self.window_y is not None:
+            geometry_string += f"+{self.window_x}+{self.window_y}"
+        else:
+            # If no saved position, let Tkinter center it initially or place it at default
+            pass  # No position appended, Tkinter will handle default placement
+
+        # 3. Apply the geometry
+        self.geometry(geometry_string)
+        logger.debug(f"Applied saved geometry: {geometry_string}")
+
+        # Store the actual width that was applied (which is current_width) for save_config
+        self.window_width = current_width  # Update instance variable for save_config
+
+    def _setup_batch_processing(self, settings):
+        """
+        Handles input path validation, mode determination (single file vs batch),
+        and creates necessary 'finished' folders.
+
+        Returns a dict:
+
+            {
+                "input_videos": [...],
+                "is_single_file_mode": bool,
+                "finished_source_folder": str or None,
+                "finished_depth_folder": str or None,
+            }
+
+        or, on error:
+
+            { "error": "message" }
+        """
+        input_source_clips_path = settings["input_source_clips"]
+        input_depth_maps_path = settings["input_depth_maps"]
+        output_splatted = settings["output_splatted"]
+
+        is_source_file = os.path.isfile(input_source_clips_path)
+        is_source_dir = os.path.isdir(input_source_clips_path)
+        is_depth_file = os.path.isfile(input_depth_maps_path)
+        is_depth_dir = os.path.isdir(input_depth_maps_path)
+
+        input_videos = []
+        finished_source_folder = None
+        finished_depth_folder = None
+        is_single_file_mode = False
+
+        if is_source_file and is_depth_file:
+            # Single-file mode
+            is_single_file_mode = True
+            logger.debug(
+                "==> Running in single file mode. Files will not be moved to "
+                "'finished' folders (unless specifically enabled in Single Process mode)."
+            )
+            input_videos.append(input_source_clips_path)
+            os.makedirs(output_splatted, exist_ok=True)
+
+        elif is_source_dir and is_depth_dir:
+            # Batch (folder) mode
+            logger.debug("==> Running in batch (folder) mode.")
+
+            if settings["move_to_finished"]:
+                finished_source_folder = os.path.join(
+                    input_source_clips_path, "finished"
+                )
+                finished_depth_folder = os.path.join(input_depth_maps_path, "finished")
+                os.makedirs(finished_source_folder, exist_ok=True)
+                os.makedirs(finished_depth_folder, exist_ok=True)
+                logger.debug("Finished folders enabled for batch mode.")
+            else:
+                logger.debug(
+                    "Finished folders DISABLED by user setting. "
+                    "Files will remain in input folders."
+                )
+
+            os.makedirs(output_splatted, exist_ok=True)
+
+            video_extensions = ("*.mp4", "*.avi", "*.mov", "*.mkv")
+            for ext in video_extensions:
+                input_videos.extend(
+                    glob.glob(os.path.join(input_source_clips_path, ext))
+                )
+            input_videos = sorted(input_videos)
+
+        else:
+            msg = (
+                "==> Error: Input Source Clips and Input Depth Maps must both be "
+                "either files or directories. Skipping processing."
+            )
+            logger.error(msg)
+            return {"error": msg}
+
+        if not input_videos:
+            msg = f"No video files found in {input_source_clips_path}"
+            logger.error(msg)
+            return {"error": msg}
+
+        return {
+            "input_videos": input_videos,
+            "is_single_file_mode": is_single_file_mode,
+            "finished_source_folder": finished_source_folder,
+            "finished_depth_folder": finished_depth_folder,
+        }
+
+    def show_about(self):
+        """Displays the 'About' message box."""
+        message = (
+            f"Stereocrafter Splatting (Batch) - {GUI_VERSION}\n"
+            "A tool for generating right-eye stereo views from source video and depth maps.\n"
+            "Based on Decord, PyTorch, and OpenCV.\n"
+            "\n(C) 2024 Some Rights Reserved"
+        )
+        tk.messagebox.showinfo("About Stereocrafter Splatting", message)
+
+    def show_user_guide(self):
+        """Reads and displays the user guide from a markdown file in a new window."""
+        # Use a path relative to the script's directory for better reliability
+        guide_path = os.path.join(
+            os.path.dirname(os.path.abspath(__file__)), "assets", "merger_gui_guide.md"
+        )
+        try:
+            with open(guide_path, "r", encoding="utf-8") as f:
+                guide_content = f.read()
+        except FileNotFoundError:
+            messagebox.showerror(
+                "File Not Found",
+                f"The user guide file could not be found at:\n{guide_path}",
+            )
+            return
+        except Exception as e:
+            messagebox.showerror(
+                "Error", f"An error occurred while reading the user guide:\n{e}"
+            )
+            return
+
+        # Determine colors based on current theme
+        if self.dark_mode_var.get():
+            bg_color, fg_color = "#2b2b2b", "white"
+        else:
+            # Use a standard light bg for text that's slightly different from the main window
+            bg_color, fg_color = "#fdfdfd", "black"
+
+        # Create a new Toplevel window
+        guide_window = tk.Toplevel(self)
+        guide_window.title("SplatterGUI - User Guide")  # Corrected title
+        guide_window.geometry("600x700")
+        guide_window.transient(self)  # Keep it on top of the main window
+        guide_window.grab_set()  # Modal behavior
+        guide_window.configure(bg=bg_color)
+
+        text_frame = ttk.Frame(guide_window, padding="10")
+        text_frame.configure(style="TFrame")  # Ensure it follows the theme
+        text_frame.pack(expand=True, fill="both")
+
+        # Apply theme colors to the Text widget
+        text_widget = tk.Text(
+            text_frame,
+            wrap=tk.WORD,
+            relief="flat",
+            borderwidth=0,
+            padx=5,
+            pady=1,
+            font=("Segoe UI", 9),
+            bg=bg_color,
+            fg=fg_color,
+            insertbackground=fg_color,
+        )
+        text_widget.insert(tk.END, guide_content)
+        text_widget.config(state=tk.DISABLED)  # Make it read-only
+
+        scrollbar = ttk.Scrollbar(
+            text_frame, orient=tk.VERTICAL, command=text_widget.yview
+        )
+        text_widget["yscrollcommand"] = scrollbar.set
+
+        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
+        text_widget.pack(side=tk.LEFT, expand=True, fill="both")
+
+        button_frame = ttk.Frame(guide_window, padding=(0, 0, 0, 10))
+        button_frame.pack()
+        ok_button = ttk.Button(button_frame, text="Close", command=guide_window.destroy)
+        ok_button.pack(pady=2)
+
+    def start_processing(self):
+        """Starts the video processing in a separate thread."""
+        self.stop_event.clear()
+        self.start_button.config(state="disabled")
+        self.start_single_button.config(state="disabled")
+        self.stop_button.config(state="normal")
+        self.status_label.config(text="Starting processing...")
+        # --- Disable all inputs at start ---
+        self._set_input_state("disabled")
+
+        # --- CRITICAL FIX: Explicitly disable slider widgets ---
+        if hasattr(self, "widgets_to_disable"):
+            for widget in self.widgets_to_disable:
+                widget.config(state="disabled")
+
+        # --- NEW: Disable previewer widgets ---
+        if hasattr(self, "previewer"):
+            self.previewer.set_ui_processing_state(True)
+            self.previewer.cleanup()  # Release any loaded preview videos
+
+        # Input validation for all fields
+        try:
+            max_disp_val = float(self.max_disp_var.get())
+            if max_disp_val <= 0:
+                raise ValueError("Max Disparity must be positive.")
+
+            anchor_val = float(self.zero_disparity_anchor_var.get())
+            if not (0.0 <= anchor_val <= 1.0):
+                raise ValueError("Zero Disparity Anchor must be between 0.0 and 1.0.")
+
+            if self.enable_full_res_var.get():
+                full_res_batch_size_val = int(self.batch_size_var.get())
+                if full_res_batch_size_val <= 0:
+                    raise ValueError("Full Resolution Batch Size must be positive.")
+
+            if self.enable_low_res_var.get():
+                pre_res_w = int(self.pre_res_width_var.get())
+                pre_res_h = int(self.pre_res_height_var.get())
+                if pre_res_w <= 0 or pre_res_h <= 0:
+                    raise ValueError(
+                        "Low-Resolution Width and Height must be positive."
+                    )
+                low_res_batch_size_val = int(self.low_res_batch_size_var.get())
+                if low_res_batch_size_val <= 0:
+                    raise ValueError("Low-Resolution Batch Size must be positive.")
+
+            if not (self.enable_full_res_var.get() or self.enable_low_res_var.get()):
+                raise ValueError(
+                    "At least one resolution (Full or Low) must be enabled to start processing."
+                )
+
+            # --- NEW: Depth Pre-processing Validation ---
+            depth_gamma_val = float(self.depth_gamma_var.get())
+            if depth_gamma_val <= 0:
+                raise ValueError("Depth Gamma must be positive.")
+
+            # Validate Dilate X/Y
+            depth_dilate_size_x_val = float(self.depth_dilate_size_x_var.get())
+            depth_dilate_size_y_val = float(self.depth_dilate_size_y_var.get())
+            if (
+                depth_dilate_size_x_val < -10.0
+                or depth_dilate_size_x_val > 30.0
+                or depth_dilate_size_y_val < -10.0
+                or depth_dilate_size_y_val > 30.0
+            ):
+                raise ValueError("Depth Dilate Sizes (X/Y) must be between -10 and 30.")
+
+            # Validate Blur X/Y
+            depth_blur_size_x_val = int(float(self.depth_blur_size_x_var.get()))
+            depth_blur_size_y_val = int(float(self.depth_blur_size_y_var.get()))
+            if depth_blur_size_x_val < 0 or depth_blur_size_y_val < 0:
+                raise ValueError("Depth Blur Sizes (X/Y) must be non-negative.")
+            # Validate Dilate/Blur Left
+            depth_dilate_left_val = float(self.depth_dilate_left_var.get())
+            depth_blur_left_val = int(float(self.depth_blur_left_var.get()))
+            if depth_dilate_left_val < 0.0 or depth_dilate_left_val > 20.0:
+                raise ValueError("Dilate Left must be between 0 and 20.")
+            if depth_blur_left_val < 0 or depth_blur_left_val > 20:
+                raise ValueError("Blur Left must be between 0 and 20.")
+
+        except ValueError as e:
+            self.status_label.config(text=f"Error: {e}")
+            self.start_button.config(state="normal")
+            self.stop_button.config(state="disabled")
+            return
+
+        settings = {
+            "input_source_clips": self.input_source_clips_var.get(),
+            "input_depth_maps": self.input_depth_maps_var.get(),
+            "output_splatted": self.output_splatted_var.get(),
+            "max_disp": float(self.max_disp_var.get()),
+            "process_length": int(self.process_length_var.get()),
+            "enable_full_resolution": self.enable_full_res_var.get(),
+            "full_res_batch_size": int(self.batch_size_var.get()),
+            "enable_low_resolution": self.enable_low_res_var.get(),
+            "low_res_width": int(self.pre_res_width_var.get()),
+            "low_res_height": int(self.pre_res_height_var.get()),
+            "low_res_batch_size": int(self.low_res_batch_size_var.get()),
+            "dual_output": self.dual_output_var.get(),
+            "zero_disparity_anchor": float(self.zero_disparity_anchor_var.get()),
+            "enable_global_norm": self.enable_global_norm_var.get(),  # Renamed
+            "match_depth_res": True,
+            "move_to_finished": self.move_to_finished_var.get(),
+            "output_crf": int(self.output_crf_full_var.get()),  # legacy
+            "output_crf_full": int(self.output_crf_full_var.get()),
+            "output_crf_low": int(self.output_crf_low_var.get()),
+            # --- Depth Pre-processing & Auto-Convergence Settings ---
+            "depth_gamma": depth_gamma_val,
+            "depth_dilate_size_x": depth_dilate_size_x_val,
+            "depth_dilate_size_y": depth_dilate_size_y_val,
+            "depth_blur_size_x": depth_blur_size_x_val,
+            "depth_blur_size_y": depth_blur_size_y_val,
+            "depth_dilate_left": float(self.depth_dilate_left_var.get()),
+            "depth_blur_left": int(round(float(self.depth_blur_left_var.get()))),
+            "auto_convergence_mode": self.auto_convergence_mode_var.get(),
+            "enable_sidecar_gamma": self.enable_sidecar_gamma_var.get(),
+            "enable_sidecar_blur_dilate": self.enable_sidecar_blur_dilate_var.get(),
+        }
+        self.processing_thread = threading.Thread(
+            target=self._run_batch_process, args=(settings,)
+        )
+        self.processing_thread.start()
+        self.check_queue()
+
+    def start_single_processing(self):
+        """
+        Starts processing for the single video currently loaded in the previewer.
+        It runs the batch logic in single-file mode.
+        """
+
+        # --- CRITICAL FIX: Explicitly disable slider widgets ---
+        if hasattr(self, "widgets_to_disable"):
+            for widget in self.widgets_to_disable:
+                widget.config(state="disabled")
+        # --- END CRITICAL FIX ---
+
+        if not hasattr(self, "previewer") or not self.previewer.source_readers:
+            messagebox.showwarning(
+                "Process Single Clip", "Please load a video in the Previewer first."
+            )
+            return
+
+        current_index = self.previewer.current_video_index
+        if current_index == -1:
+            messagebox.showwarning(
+                "Process Single Clip", "No video is currently selected for processing."
+            )
+            return
+
+        # 1. Get the current single file paths
+        current_source_dict = self.previewer.video_list[current_index]
+        single_video_path = current_source_dict.get("source_video")
+        single_depth_path = current_source_dict.get("depth_map")
+        # In Multi-Map mode, ensure Single/Test processing uses the clip's sidecar-selected map file.
+        if self.multi_map_var.get() and single_video_path:
+            try:
+                _mm_sidecar_map = self._get_sidecar_selected_map_for_video(
+                    single_video_path
+                )
+                if _mm_sidecar_map:
+                    _mm_base = self.input_depth_maps_var.get()
+                    _mm_video_name = os.path.splitext(
+                        os.path.basename(single_video_path)
+                    )[0]
+                    _mm_folder = os.path.join(_mm_base, _mm_sidecar_map)
+                    _mm_mp4 = os.path.join(_mm_folder, f"{_mm_video_name}_depth.mp4")
+                    _mm_npz = os.path.join(_mm_folder, f"{_mm_video_name}_depth.npz")
+                    if os.path.exists(_mm_mp4):
+                        single_depth_path = _mm_mp4
+                    elif os.path.exists(_mm_npz):
+                        single_depth_path = _mm_npz
+            except Exception as _e:
+                logger.debug(
+                    f"[MM] Failed resolving sidecar-selected map for Single/Test: {_e}"
+                )
+
+        if not single_video_path or not single_depth_path:
+            messagebox.showerror(
+                "Process Single Clip Error",
+                "Could not get both video and depth map paths from previewer.",
+            )
+            return
+
+        # 2. Perform validation checks (copied from start_processing)
+        try:
+            # Full Resolution/Low Resolution checks
+            if not (self.enable_full_res_var.get() or self.enable_low_res_var.get()):
+                raise ValueError(
+                    "At least one resolution (Full or Low) must be enabled to start processing."
+                )
+
+            # Simplified validation for speed/simplicity (relying on start_processing for full checks)
+            float(self.max_disp_var.get())
+
+        except ValueError as e:
+            self.status_label.config(text=f"Error: {e}")
+            messagebox.showerror("Validation Error", str(e))
+            return
+
+        # --- MODIFIED: Delayed cleanup for diagnostic parity ---
+        is_test_active = self.enable_full_res_var.get() or self.enable_low_res_var.get()
+
+        # --- MODIFIED: Auto-Switch Preview for Tests ---
+        if self.map_test_var.get() or self.splat_test_var.get():
+            # If a test is active, sync GUI controls from sidecar once before capture/run (keeps parity without freezing live slider changes).
+            try:
+                self.update_gui_from_sidecar(single_depth_path)
+                # Sidecar restore may switch the map for this clip; re-read the effective depth path.
+                _cur_entry = self.previewer.video_list[current_index]
+                single_depth_path = _cur_entry.get("depth_map") or single_depth_path
+            except Exception:
+                pass
+
+            # Auto-switch preview mode for tests:
+            # - Map Test: always show Depth Map
+            # - Splat Test: if Full is enabled (even with Low), show Full splat preview; if only Low is enabled, show Low splat preview
+            if self.map_test_var.get():
+                target_mode = "Depth Map"
+            else:
+                full_enabled = (
+                    bool(getattr(self, "enable_full_res_var", None).get())
+                    if getattr(self, "enable_full_res_var", None)
+                    else False
+                )
+                low_enabled = (
+                    bool(getattr(self, "enable_low_res_var", None).get())
+                    if getattr(self, "enable_low_res_var", None)
+                    else False
+                )
+                if (
+                    full_enabled
+                    or (full_enabled and low_enabled)
+                    or (not low_enabled and full_enabled)
+                ):
+                    target_mode = "Splat Result"
+                elif low_enabled and not full_enabled:
+                    target_mode = "Splat Result(Low)"
+                else:
+                    # Fallback (shouldn't happen in normal usage)
+                    target_mode = "Splat Result"
+            self.preview_source_var.set(target_mode)
+            self.preview_size_var.set("100%")  # Ensure 1:1 scale for capture parity
+            logger.info(
+                f"Test Active: Auto-switching preview to {target_mode} at 100% scale."
+            )
+            self.previewer.update_preview()
+            # Give the previewer a moment to actually render the new mode
+            self.update()
+        else:
+            # Standard cleanup only if NOT in test mode
+            if hasattr(self, "previewer"):
+                self.previewer.cleanup()
+        # --- END AUTO-SWITCH ---
+
+        # 3. Compile settings dictionary
+        # We explicitly set the input paths to the single files, which forces batch logic
+        # to execute in single-file mode (checking os.path.isfile).
+
+        # --- NEW: Determine Finished Folders for Single Process (only if enabled) ---
+        single_finished_source_folder = None
+        single_finished_depth_folder = None
+
+        # --- Check the new GUI variable ---
+        if self.move_to_finished_var.get():
+            # We assume the finished folder is in the same directory as the original input file/depth map
+            single_finished_source_folder = os.path.join(
+                os.path.dirname(single_video_path), "finished"
+            )
+            single_finished_depth_folder = os.path.join(
+                os.path.dirname(single_depth_path), "finished"
+            )
+            os.makedirs(single_finished_source_folder, exist_ok=True)
+            os.makedirs(single_finished_depth_folder, exist_ok=True)
+            logger.debug(
+                f"Single Process: Finished folders set to: {single_finished_source_folder}"
+            )
+
+        settings = {
+            # --- OVERRIDDEN INPUTS FOR SINGLE MODE ---
+            "input_source_clips": single_video_path,
+            "input_depth_maps": single_depth_path,
+            "output_splatted": self.output_splatted_var.get(),  # Use the batch output folder
+            # --- END OVERRIDE ---
+            "max_disp": float(self.max_disp_var.get()),
+            "process_length": int(self.process_length_var.get()),
+            "enable_full_resolution": self.enable_full_res_var.get(),
+            "full_res_batch_size": int(self.batch_size_var.get()),
+            "enable_low_resolution": self.enable_low_res_var.get(),
+            "low_res_width": int(self.pre_res_width_var.get()),
+            "low_res_height": int(self.pre_res_height_var.get()),
+            "low_res_batch_size": int(self.low_res_batch_size_var.get()),
+            "dual_output": self.dual_output_var.get(),
+            "zero_disparity_anchor": float(self.zero_disparity_anchor_var.get()),
+            "enable_global_norm": self.enable_global_norm_var.get(),  # Renamed
+            "match_depth_res": True,
+            "output_crf": int(self.output_crf_full_var.get()),  # legacy
+            "output_crf_full": int(self.output_crf_full_var.get()),
+            "output_crf_low": int(self.output_crf_low_var.get()),
+            # --- Depth Pre-processing Settings ---
+            "depth_gamma": float(self.depth_gamma_var.get()),
+            "depth_dilate_size_x": int(float(self.depth_dilate_size_x_var.get())),
+            "depth_dilate_size_y": int(float(self.depth_dilate_size_y_var.get())),
+            "depth_blur_size_x": int(float(self.depth_blur_size_x_var.get())),
+            "depth_blur_size_y": int(float(self.depth_blur_size_y_var.get())),
+            "depth_dilate_left": float(self.depth_dilate_left_var.get()),
+            "depth_blur_left": int(round(float(self.depth_blur_left_var.get()))),
+            "depth_blur_left_mix": float(self.depth_blur_left_mix_var.get()),
+            "auto_convergence_mode": self.auto_convergence_mode_var.get(),
+            "enable_sidecar_gamma": self.enable_sidecar_gamma_var.get(),
+            "enable_sidecar_blur_dilate": self.enable_sidecar_blur_dilate_var.get(),
+            "single_finished_source_folder": single_finished_source_folder,
+            "single_finished_depth_folder": single_finished_depth_folder,
+            "move_to_finished": self.move_to_finished_var.get(),
+        }
+
+        # 4. Start the processing thread
+        self.stop_event.clear()
+        self.start_button.config(state="disabled")
+        self.start_single_button.config(state="disabled")  # Disable single button too
+        self.stop_button.config(state="normal")
+        self.status_label.config(
+            text=f"Starting single-clip processing for: {os.path.basename(single_video_path)}"
+        )
+        self._set_input_state("disabled")  # Disable all inputs
+
+        self.processing_thread = threading.Thread(
+            target=self._run_batch_process, args=(settings,)
+        )
+        self.processing_thread.start()
+        self.check_queue()
+
+    def stop_processing(self):
+        """Sets the stop event to gracefully halt processing."""
+        self.stop_event.set()
+        self.status_label.config(text="Stopping...")
+        self.stop_button.config(state="disabled")
+        self.start_single_button.config(state="normal")
+        # --- Re-enable previewer widgets on stop ---
+        if hasattr(self, "previewer"):
+            self.previewer.set_ui_processing_state(False)
+
+    def _toggle_debug_logging(self):
+        """Toggles debug logging and updates shared logger."""
+        self._debug_logging_enabled = (
+            self.debug_logging_var.get()
+        )  # Get checkbutton state
+
+        if self._debug_logging_enabled:
+            new_level = logging.DEBUG
+            level_str = "DEBUG"
+        else:
+            new_level = logging.INFO
+            level_str = "INFO"
+
+        # Call the utility function to change the root logger level
+        set_util_logger_level(new_level)
+
+        logger.info(f"Setting application logging level to: {level_str}")
+
+    def toggle_processing_settings_fields(self):
+        """Enables/disables resolution input fields and the START button based on checkbox states."""
+        # Full Resolution controls
+        if self.enable_full_res_var.get():
+            self.entry_full_res_batch_size.config(state="normal")
+            self.lbl_full_res_batch_size.config(state="normal")
+        else:
+            self.entry_full_res_batch_size.config(state="disabled")
+            self.lbl_full_res_batch_size.config(state="disabled")
+
+        # Low Resolution controls
+        if self.enable_low_res_var.get():
+            self.pre_res_width_label.config(state="normal")
+            self.pre_res_width_entry.config(state="normal")
+            self.pre_res_height_label.config(state="normal")
+            self.pre_res_height_entry.config(state="normal")
+            self.lbl_low_res_batch_size.config(state="normal")
+            self.entry_low_res_batch_size.config(state="normal")
+        else:
+            self.pre_res_width_label.config(state="disabled")
+            self.pre_res_width_entry.config(state="disabled")
+            self.pre_res_height_label.config(state="disabled")
+            self.pre_res_height_entry.config(state="disabled")
+            self.lbl_low_res_batch_size.config(state="disabled")
+            self.entry_low_res_batch_size.config(state="disabled")
+
+        # START button enable/disable logic: Must have at least one resolution enabled
+        if self.enable_full_res_var.get() or self.enable_low_res_var.get():
+            self.start_button.config(state="normal")
+        else:
+            self.start_button.config(state="disabled")
+
+    def _apply_preview_overlay_toggles(self):
+        """Apply preview-only overlay toggles to the previewer (Crosshair + D/P)."""
+        if not getattr(self, "previewer", None):
+            return
+
+        try:
+            self.previewer.set_crosshair_settings(
+                self.crosshair_enabled_var.get(),
+                self.crosshair_white_var.get(),
+                self.crosshair_multi_var.get(),
+            )
+        except Exception:
+            pass
+
+        try:
+            getattr(self.previewer, "set_depth_pop_enabled", lambda *_: None)(
+                self.depth_pop_enabled_var.get()
+            )
+        except Exception:
+            pass
+
+    def _toggle_sidecar_update_button_state(self):
+        """
+        Controls the Update Sidecar button state based on the Override Sidecar checkbox.
+        """
+
+        # Check if batch processing is currently active (easiest way is to check the stop button's state)
+        is_batch_processing_active = self.stop_button.cget("state") == "normal"
+
+        # Check if a video is currently loaded in the previewer
+        is_video_loaded = (
+            hasattr(self, "previewer") and self.previewer.current_video_index != -1
+        )
+
+        # If batch is active, the button MUST be disabled, regardless of override state.
+        if is_batch_processing_active:
+            self.update_sidecar_button.config(state="disabled")
+            return
+
+        # If a video is loaded and batch is NOT active, ENABLE the button.
+        if is_video_loaded:
+            self.update_sidecar_button.config(state="normal")
+        else:
+            self.update_sidecar_button.config(state="disabled")
+
+    def _update_clip_state_and_text(self):
+        """Combines state and text updates for the Sidecar button, run after a new video loads."""
+
+        # 1. Update the button text (Create vs Update)
+        if hasattr(self, "_update_sidecar_button_text"):
+            self._update_sidecar_button_text()
+
+        # 2. Update the button state (Normal vs Disabled by Override)
+        if hasattr(self, "_toggle_sidecar_update_button_state"):
+            self._toggle_sidecar_update_button_state()
+
+        # 3. Update the info panel to reflect the currently loaded preview clip
+        self._update_processing_info_for_preview_clip()
+
+    def _update_processing_info_for_preview_clip(self):
+        """Updates the 'Current Processing Information' panel to reflect the currently loaded preview clip."""
+        try:
+            if not getattr(self, "previewer", None):
+                return
+
+            idx = getattr(self.previewer, "current_video_index", -1)
+            video_list = getattr(self.previewer, "video_list", []) or []
+            if not (0 <= idx < len(video_list)):
+                return
+
+            source_dict = video_list[idx] if isinstance(video_list[idx], dict) else {}
+
+            # Filename (the main thing you care about in practice)
+            src_path = source_dict.get("source_video")
+            if src_path:
+                new_name = os.path.basename(src_path)
+                if self.processing_filename_var.get() != new_name:
+                    self.processing_filename_var.set(new_name)
+            # Map display during preview:
+            # - Normal mode: keep blank (map is already implied by the UI and filename prefix).
+            # - Multi-Map mode: show the currently selected map label (same text as the selector),
+            #   because the radio dot can be hard to see (e.g., with 3D glasses).
+            try:
+                if self.multi_map_var.get():
+                    sel_map = (self.selected_depth_map_var.get() or "").strip()
+                    if sel_map and self.processing_map_var.get() != sel_map:
+                        self.processing_map_var.set(sel_map)
+                else:
+                    if self.processing_map_var.get() != "":
+                        self.processing_map_var.set("")
+            except Exception:
+                # Fail closed: don't spam logs; leave whatever is currently displayed.
+                pass
+
+            # Task name: keep stable during preview
+            new_task = "Preview"
+            if self.processing_task_name_var.get() != new_task:
+                self.processing_task_name_var.set(new_task)
+
+            # Frames display: keep stable during preview; show total frames only
+            try:
+                to_val = self.previewer.frame_scrubber.cget("to")
+                total_frames = int(float(to_val)) + 1 if to_val is not None else None
+
+                if total_frames is not None:
+                    new_frames = f"{total_frames}"
+                    if self.processing_frames_var.get() != new_frames:
+                        self.processing_frames_var.set(new_frames)
+            except Exception:
+                pass
+
+        except Exception as e:
+            logger.debug(f"Preview info update skipped due to error: {e}")
+
+    def update_gui_from_sidecar(self, depth_map_path: str):
+        """
+        Reads the sidecar config for the given depth map path and updates the
+        Convergence, Max Disparity, and Gamma sliders.
+        """
+        # Clear suppression flag when opening a NEW video
+        # (Allow sidecar to load for the first time on new video)
+        # Get current source video to track video changes (not depth map changes)
+        current_source_video = None
+        if (
+            hasattr(self, "previewer")
+            and self.previewer
+            and 0 <= self.previewer.current_video_index < len(self.previewer.video_list)
+        ):
+            current_source_video = self.previewer.video_list[
+                self.previewer.current_video_index
+            ].get("source_video")
+
+        # Clear suppression flag when opening a NEW video (not when changing maps)
+        if current_source_video and current_source_video != getattr(
+            self, "_last_loaded_source_video", None
+        ):
+            self._suppress_sidecar_map_update = False
+            self._last_loaded_source_video = (
+                current_source_video  # Track source video, not depth map
+            )
+        if not self.update_slider_from_sidecar_var.get():
+            logger.debug(
+                "update_gui_from_sidecar: Feature is toggled OFF. Skipping update."
+            )
+            return
+
+        if not depth_map_path:
+            return
+
+        # 1. Determine sidecar path
+        depth_map_basename = os.path.splitext(os.path.basename(depth_map_path))[0]
+        sidecar_ext = self.APP_CONFIG_DEFAULTS["SIDECAR_EXT"]
+        # Use base folder for sidecars when Multi-Map is enabled
+        sidecar_folder = self._get_sidecar_base_folder()
+        json_sidecar_path = os.path.join(
+            sidecar_folder, f"{depth_map_basename}{sidecar_ext}"
+        )
+        logger.info(f"Looking for sidecar at: {json_sidecar_path}")
+
+        if not os.path.exists(json_sidecar_path):
+            logger.debug(
+                f"update_gui_from_sidecar: No sidecar found at {json_sidecar_path}. Calling _on_map_selection_changed to sync preview."
+            )
+            # FIXED: When no sidecar, update previewer with currently-selected map
+            self._on_map_selection_changed(from_sidecar=False)
+            return
+
+        # 2. Load merged config (Sidecar values merged with defaults)
+        # We use merge to ensure we get a complete dictionary even if keys are missing
+        sidecar_config = self.sidecar_manager.load_sidecar_data(json_sidecar_path)
+
+        logger.debug(
+            f"Updating sliders from sidecar: {os.path.basename(json_sidecar_path)}"
+        )
+
+        # 3. Update Sliders Programmatically (Requires programmatic setter/updater)
+
+        # Convergence
+        conv_val = sidecar_config.get(
+            "convergence_plane", self.zero_disparity_anchor_var.get()
+        )
+        self.zero_disparity_anchor_var.set(conv_val)
+        if self.set_convergence_value_programmatically:
+            self.set_convergence_value_programmatically(conv_val)
+
+        # Max Disparity (Simple set)
+        disp_val = sidecar_config.get("max_disparity", self.max_disp_var.get())
+        self.max_disp_var.set(disp_val)
+
+        # Gamma (Simple set)
+        gamma_val = sidecar_config.get("gamma", self.depth_gamma_var.get())
+        self.depth_gamma_var.set(gamma_val)
+
+        # Seed preview overlay "Max Total" from sidecar (most-accurate available):
+        # prefer dp_total_max_true (render-measured) else dp_total_max_est (sampled).
+        try:
+            dp_seed = sidecar_config.get("dp_total_max_true", None)
+            if dp_seed is None:
+                dp_seed = sidecar_config.get("dp_total_max_est", None)
+            if dp_seed is not None and getattr(self, "previewer", None) is not None and hasattr(self.previewer, "set_depth_pop_max_estimate"):
+                sig = self._dp_total_signature(depth_map_path, conv_val, disp_val, gamma_val)
+                self.previewer.set_depth_pop_max_estimate(float(dp_seed), sig)
+        except Exception:
+            pass
+
+
+        # Dilate X
+        dilate_x_val = sidecar_config.get(
+            "depth_dilate_size_x", self.depth_dilate_size_x_var.get()
+        )
+        self.depth_dilate_size_x_var.set(dilate_x_val)
+
+        # Dilate Y
+        dilate_y_val = sidecar_config.get(
+            "depth_dilate_size_y", self.depth_dilate_size_y_var.get()
+        )
+        self.depth_dilate_size_y_var.set(dilate_y_val)
+
+        # Blur X
+        blur_x_val = sidecar_config.get(
+            "depth_blur_size_x", self.depth_blur_size_x_var.get()
+        )
+        self.depth_blur_size_x_var.set(blur_x_val)
+        # Blur Y
+        blur_y_val = sidecar_config.get(
+            "depth_blur_size_y", self.depth_blur_size_y_var.get()
+        )
+        self.depth_blur_size_y_var.set(blur_y_val)
+
+        # Dilate Left
+        dilate_left_val = sidecar_config.get(
+            "depth_dilate_left", self.depth_dilate_left_var.get()
+        )
+        self.depth_dilate_left_var.set(dilate_left_val)
+
+        # Blur Left (stored as integer steps; accept older float values)
+        blur_left_val = sidecar_config.get(
+            "depth_blur_left", self.depth_blur_left_var.get()
+        )
+        try:
+            self.depth_blur_left_var.set(int(round(float(blur_left_val))))
+        except Exception:
+            self.depth_blur_left_var.set(0)
+
+        # Blur Left H‚ÜîV balance (defaults to 0.5 for older sidecars)
+        mix_val = sidecar_config.get(
+            "depth_blur_left_mix", self.depth_blur_left_mix_var.get()
+        )
+        try:
+            mix_f = float(mix_val)
+            if mix_f < 0.0:
+                mix_f = 0.0
+            if mix_f > 1.0:
+                mix_f = 1.0
+            # store as one decimal string to match UI selector
+            self.depth_blur_left_mix_var.set(f"{mix_f:.1f}")
+        except Exception:
+            self.depth_blur_left_mix_var.set("0.5")
+
+        # Selected Depth Map (for Multi-Map mode)
+        if self.multi_map_var.get():
+            selected_map_val = sidecar_config.get("selected_depth_map", "")
+            if selected_map_val:
+                # Only log when we actually switch maps (reduces redundant messages).
+                _current_sel = self.selected_depth_map_var.get()
+                if selected_map_val != _current_sel:
+                    logger.info(
+                        f"Restoring depth map selection from sidecar: {selected_map_val}"
+                    )
+                    self.selected_depth_map_var.set(selected_map_val)
+
+                # Ensure the current preview entry uses the sidecar-selected map (even if unchanged).
+                try:
+                    self._on_map_selection_changed(from_sidecar=True)
+                except Exception as e:
+                    logger.error(
+                        f"Failed to apply sidecar map '{selected_map_val}': {e}"
+                    )
+
+                # Do not allow sidecar to override manual click after this point
+                self._suppress_sidecar_map_update = True
+
+        # --- Border Settings ---
+        self.auto_border_L_var.set(str(sidecar_config.get("auto_border_L", 0.0)))
+        self.auto_border_R_var.set(str(sidecar_config.get("auto_border_R", 0.0)))
+
+        mode = sidecar_config.get("border_mode")
+        if mode is None:
+            # Migration from older sidecars
+            if "manual_border" in sidecar_config:
+                is_manual = sidecar_config.get("manual_border", False)
+                mode = "Manual" if is_manual else "Auto Basic"
+            else:
+                # Fresh clip or non-configured sidecar: keep current mode
+                mode = self.border_mode_var.get()
+
+        self.border_mode_var.set(mode)
+
+        left_b = sidecar_config.get("left_border", 0.0)
+        right_b = sidecar_config.get("right_border", 0.0)
+
+        # Convert back to width/bias for the sliders
+        w = max(left_b, right_b)
+        if w > 0:
+            if left_b > right_b:
+                b = (right_b / left_b) - 1.0
+            elif right_b > left_b:
+                b = 1.0 - (left_b / right_b)
+            else:
+                b = 0.0
+        else:
+            b = 0.0
+
+        self.border_width_var.set(f"{w:.2f}")
+        self.border_bias_var.set(f"{b:.2f}")
+
+        if self.set_border_width_programmatically:
+            self.set_border_width_programmatically(w)
+        if self.set_border_bias_programmatically:
+            self.set_border_bias_programmatically(b)
+
+        # Ensure UI state matches restored mode
+        self._on_border_mode_change()
+
+        # --- FIX: Refresh slider labels after restoring sidecar values ---
+        if hasattr(self, "slider_label_updaters"):
+            for updater in self.slider_label_updaters:
+                updater()
+
+        # --- Fix: resync processing queue depth map paths after refresh ---
+        if hasattr(self.previewer, "video_list") and hasattr(
+            self, "resolution_output_list"
+        ):
+            for i, video_entry in enumerate(self.previewer.video_list):
+                if i < len(self.resolution_output_list):
+                    self.resolution_output_list[i].depth_map = video_entry.get(
+                        "depth_map", None
+                    )
+        # 4. Refresh preview to show the new values
+        self.on_slider_release(None)
+
+    def _update_sidecar_button_text(self):
+        """Checks if a sidecar exists for the current preview video and updates the button text."""
+        is_sidecar_present = False
+
+        if 0 <= self.previewer.current_video_index < len(self.previewer.video_list):
+            current_source_dict = self.previewer.video_list[
+                self.previewer.current_video_index
+            ]
+            depth_map_path = current_source_dict.get("depth_map")
+
+            if depth_map_path:
+                depth_map_basename = os.path.splitext(os.path.basename(depth_map_path))[
+                    0
+                ]
+                sidecar_ext = self.APP_CONFIG_DEFAULTS["SIDECAR_EXT"]
+                sidecar_folder = (
+                    self._get_sidecar_base_folder()
+                )  # Use proper folder for multi-map mode
+                json_sidecar_path = os.path.join(
+                    sidecar_folder, f"{depth_map_basename}{sidecar_ext}"
+                )
+                is_sidecar_present = os.path.exists(json_sidecar_path)
+
+        button_text = "Update Sidecar" if is_sidecar_present else "Create Sidecar"
+        self.update_sidecar_button.config(text=button_text)
+
+    def update_sidecar_file(self):
+        """
+        Saves the current GUI values to the sidecar file after checking for user confirmation.
+        """
+        # 1. Get current sidecar path and data (needed for overwrite check)
+        result = self._get_current_sidecar_paths_and_data()
+        if result is None:
+            messagebox.showwarning(
+                "Sidecar Action", "Please load a video in the Previewer first."
+            )
+            return
+
+        json_sidecar_path, _, _ = result
+        is_sidecar_present = os.path.exists(json_sidecar_path)
+
+        # 2. Conditional Confirmation Dialog
+        if is_sidecar_present:
+            title = "Overwrite Sidecar File?"
+            message = (
+                f"This will overwrite parameters (Convergence, Disparity, Gamma, Borders) "
+                f"in the existing sidecar file:\n\n{os.path.basename(json_sidecar_path)}\n\n"
+                f"Do you want to continue?"
+            )
+            if not messagebox.askyesno(title, message):
+                self.status_label.config(text="Sidecar update cancelled.")
+                return
+
+        # 3. Call the core saving function
+        if self._save_current_sidecar_data(is_auto_save=False):
+            # Immediately refresh the preview to show the *effect* of the newly saved sidecar
+            self.on_slider_release(None)
+            # If an AUTO-PASS CSV exists, keep its row in sync with sidecar edits
+            try:
+                self._auto_pass_csv_update_row_from_current()
+            except Exception:
+                pass
+
+
+def compute_global_depth_stats(
+    depth_map_reader: VideoReader, total_frames: int, chunk_size: int = 100
+) -> Tuple[float, float]:
+    """
+    Computes the global min and max depth values from a depth video by reading it in chunks.
+    Assumes raw pixel values that need to be scaled (e.g., from 0-255 or 0-1023 range).
+    """
+    logger.info(
+        f"==> Starting global depth stats pre-pass for {total_frames} frames..."
+    )
+    global_min, global_max = np.inf, -np.inf
+
+    for i in range(0, total_frames, chunk_size):
+        current_indices = list(range(i, min(i + chunk_size, total_frames)))
+        if not current_indices:
+            break
+
+        chunk_numpy_raw = depth_map_reader.get_batch(current_indices).asnumpy()
+
+        # Handle RGB vs Grayscale depth maps
+        if chunk_numpy_raw.ndim == 4:
+            if chunk_numpy_raw.shape[-1] == 3:  # RGB
+                chunk_numpy = chunk_numpy_raw.mean(axis=-1)
+            else:  # Grayscale with channel dim
+                chunk_numpy = chunk_numpy_raw.squeeze(-1)
+        else:
+            chunk_numpy = chunk_numpy_raw
+
+        chunk_min = chunk_numpy.min()
+        chunk_max = chunk_numpy.max()
+
+        if chunk_min < global_min:
+            global_min = chunk_min
+        if chunk_max > global_max:
+            global_max = chunk_max
+
+        # draw_progress_bar(i + len(current_indices), total_frames, prefix="  Depth Stats:", suffix="Complete")
+
+    logger.info(
+        f"==> Global depth stats computed: min_raw={global_min:.3f}, max_raw={global_max:.3f}"
+    )
+    return float(global_min), float(global_max)
+
+
+def read_video_frames(
+    video_path: str,
+    process_length: int,
+    set_pre_res: bool,
+    pre_res_width: int,
+    pre_res_height: int,
+    dataset: str = "open",
+) -> Tuple[VideoReader, float, int, int, int, int, Optional[dict], int]:
+    """
+    Initializes a VideoReader for chunked reading.
+    Returns: (video_reader, fps, original_height, original_width, actual_processed_height, actual_processed_width, video_stream_info, total_frames_to_process)
+    """
+    if dataset == "open":
+        logger.debug(f"==> Initializing VideoReader for: {video_path}")
+        vid_info_only = VideoReader(
+            video_path, ctx=cpu(0)
+        )  # Use separate reader for info
+        original_height, original_width = vid_info_only.get_batch([0]).shape[1:3]
+        total_frames_original = len(vid_info_only)
+        logger.debug(
+            f"==> Original video shape: {total_frames_original} frames, {original_height}x{original_width} per frame"
+        )
+
+        height_for_reader = original_height
+        width_for_reader = original_width
+
+        if set_pre_res and pre_res_width > 0 and pre_res_height > 0:
+            height_for_reader = pre_res_height
+            width_for_reader = pre_res_width
+            logger.debug(
+                f"==> Pre-processing resolution set to: {width_for_reader}x{height_for_reader}"
+            )
+        else:
+            logger.debug(
+                f"==> Using original video resolution for reading: {width_for_reader}x{height_for_reader}"
+            )
+
+    else:
+        raise NotImplementedError(f"Dataset '{dataset}' not supported.")
+
+    # decord automatically resizes if width/height are passed to VideoReader
+    video_reader = VideoReader(
+        video_path, ctx=cpu(0), width=width_for_reader, height=height_for_reader
+    )
+
+    # Verify the actual shape after Decord processing, using the first frame
+    first_frame_shape = video_reader.get_batch([0]).shape
+    actual_processed_height, actual_processed_width = first_frame_shape[1:3]
+
+    fps = video_reader.get_avg_fps()  # Use actual FPS from the reader
+
+    total_frames_available = len(video_reader)
+    total_frames_to_process = total_frames_available  # Use available frames directly
+    if process_length != -1 and process_length < total_frames_available:
+        total_frames_to_process = process_length
+
+    logger.debug(
+        f"==> VideoReader initialized. Final processing dimensions: {actual_processed_width}x{actual_processed_height}. Total frames for processing: {total_frames_to_process}"
+    )
+
+    video_stream_info = get_video_stream_info(
+        video_path
+    )  # Get stream info for FFmpeg later
+
+    return (
+        video_reader,
+        fps,
+        original_height,
+        original_width,
+        actual_processed_height,
+        actual_processed_width,
+        video_stream_info,
+        total_frames_to_process,
+    )
+
+
+# -----------------------------
+# 10-bit+ depth decode helpers
+# -----------------------------
+class _NumpyBatch:
+    """Minimal wrapper to match Decord's get_batch(...).asnumpy() API."""
+
+    def __init__(self, arr: np.ndarray):
+        self._arr = arr
+
+    def asnumpy(self) -> np.ndarray:
+        return self._arr
+
+
+def _infer_depth_bit_depth(depth_stream_info: Optional[dict]) -> int:
+    """Best-effort bit-depth inference from ffprobe info."""
+    if not depth_stream_info:
+        return 8
+    pix_fmt = str(depth_stream_info.get("pix_fmt", "")).lower()
+    profile = str(depth_stream_info.get("profile", "")).lower()
+
+    # Common patterns: yuv420p10le, yuv444p12le, gray16le, etc.
+    m = re.search(r"(?:p|gray)(\d+)", pix_fmt)
+    if m:
+        try:
+            return int(m.group(1))
+        except Exception:
+            pass
+    if "main10" in profile or "10" in profile:
+        return 10
+    return 8
+
+
+def _build_depth_vf(pix_fmt: str, out_w: int, out_h: int) -> str:
+    pix_fmt = (pix_fmt or "").lower()
+    # If it's already gray*, don't try to extract planes.
+    if pix_fmt.startswith("gray"):
+        return f"scale={out_w}:{out_h}:flags=bilinear,format=gray16le"
+    # Otherwise, extract luma plane (Y) and convert to gray16.
+    return f"extractplanes=y,scale={out_w}:{out_h}:flags=bilinear,format=gray16le"
+
+
+class FFmpegDepthPipeReader:
+    """
+    Sequential ffmpeg-backed depth reader that preserves 10-bit+ values.
+    Implements a small subset of Decord's VideoReader API used by the splatter:
+      - __len__()
+      - seek(idx)
+      - get_batch([idx0, idx1, ...]).asnumpy()
+    NOTE: This reader is optimized for sequential access (render path).
+    """
+
+    def __init__(
+        self,
+        path: str,
+        out_w: int,
+        out_h: int,
+        bit_depth: int,
+        num_frames: int,
+        pix_fmt: str = "",
+    ):
+        self.path = path
+        self.out_w = int(out_w)
+        self.out_h = int(out_h)
+        self.bit_depth = int(bit_depth) if bit_depth else 16
+        self._num_frames = int(num_frames) if num_frames is not None else 0
+        self._pix_fmt = pix_fmt or ""
+        self._proc: Optional[subprocess.Popen] = None
+        self._next_index = 0
+        self._frame_bytes = self.out_w * self.out_h * 2  # gray16le
+        self._msb_shift: Optional[int] = None
+        self._use_16_to_n_scale: bool = False
+        self._start_process()
+
+    def __len__(self) -> int:
+        return self._num_frames
+
+    def _start_process(self):
+        self.close()
+        vf = _build_depth_vf(self._pix_fmt, self.out_w, self.out_h)
+        cmd = [
+            "ffmpeg",
+            "-hide_banner",
+            "-loglevel",
+            "error",
+            "-nostdin",
+            "-i",
+            self.path,
+            "-an",
+            "-sn",
+            "-dn",
+            "-vframes",
+            str(self._num_frames)
+            if self._num_frames and self._num_frames > 0
+            else "999999999",
+            "-vf",
+            vf,
+            "-f",
+            "rawvideo",
+            "pipe:1",
+        ]
+        self._proc = subprocess.Popen(
+            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
+        )
+        self._next_index = 0
+        self._msb_shift = None
+
+    def close(self):
+        try:
+            if self._proc is not None:
+                if self._proc.stdout:
+                    try:
+                        self._proc.stdout.close()
+                    except Exception:
+                        pass
+                if self._proc.stderr:
+                    try:
+                        self._proc.stderr.close()
+                    except Exception:
+                        pass
+                try:
+                    self._proc.terminate()
+                except Exception:
+                    pass
+        finally:
+            self._proc = None
+
+    def __del__(self):
+        self.close()
+
+    def seek(self, idx: int):
+        """Best-effort seek. Efficient only if seeking forward sequentially."""
+        idx = int(idx)
+        if idx == self._next_index:
+            return
+        if idx < self._next_index:
+            # Restart and fast-forward by discarding frames (rare in render path).
+            self._start_process()
+        # Discard until we reach idx
+        to_skip = idx - self._next_index
+        if to_skip <= 0:
+            self._next_index = idx
+            return
+        if self._proc is None or self._proc.stdout is None:
+            self._start_process()
+        discard_bytes = to_skip * self._frame_bytes
+        _ = self._proc.stdout.read(discard_bytes)
+        self._next_index = idx
+
+    def _read_exact(self, nbytes: int) -> bytes:
+        if self._proc is None or self._proc.stdout is None:
+            self._start_process()
+        buf = b""
+        while len(buf) < nbytes:
+            chunk = self._proc.stdout.read(nbytes - len(buf))
+            if not chunk:
+                break
+            buf += chunk
+        return buf
+
+    def _maybe_apply_shift(self, arr_u16: np.ndarray) -> np.ndarray:
+        """Normalize decoded gray16 values back into the native bit-depth range (e.g., 0..1023 for 10-bit).
+
+        ffmpeg's format conversion to gray16le may either:
+          1) left-shift the original N-bit values into the MSBs (exact multiples of 2^(16-N)), or
+          2) scale values to the full 16-bit range (0..65535).
+
+        We detect the likely behavior once and then apply either a right-shift or a scale-down so the
+        output values land back in 0..(2^bit_depth-1).
+        """
+        if self._msb_shift is None:
+            expected_max = (
+                (1 << self.bit_depth) - 1 if 0 < self.bit_depth < 16 else None
+            )
+            if expected_max is None:
+                self._msb_shift = 0
+                self._use_16_to_n_scale = False
+            else:
+                # Sample to avoid full-frame scans on large batches
+                flat = arr_u16.reshape(-1)
+                step = max(1, flat.size // 50000)
+                sample = flat[::step]
+                max_val = int(sample.max(initial=0))
+
+                if max_val <= expected_max:
+                    self._msb_shift = 0
+                    self._use_16_to_n_scale = False
+                else:
+                    shift = 16 - self.bit_depth
+                    if shift <= 0:
+                        self._msb_shift = 0
+                        self._use_16_to_n_scale = False
+                    else:
+                        low_mask = (1 << shift) - 1
+                        # If the sample's low bits are consistently zero, it's very likely MSB-aligned (pure left shift).
+                        low_bits_max = int((sample & low_mask).max(initial=0))
+                        if low_bits_max == 0:
+                            self._msb_shift = shift
+                            self._use_16_to_n_scale = False
+                        else:
+                            # Otherwise assume scaled-to-16bit; scale back down into expected range.
+                            self._msb_shift = 0
+                            self._use_16_to_n_scale = True
+
+        expected_max = (1 << self.bit_depth) - 1 if 0 < self.bit_depth < 16 else None
+        if expected_max is None:
+            return arr_u16
+
+        if self._msb_shift and self._msb_shift > 0:
+            return (arr_u16 >> self._msb_shift).astype(np.uint16)
+
+        if self._use_16_to_n_scale:
+            # Map 0..65535 -> 0..expected_max with rounding.
+            arr32 = arr_u16.astype(np.uint32)
+            return ((arr32 * expected_max + 32767) // 65535).astype(np.uint16)
+
+        return arr_u16
+
+    def get_batch(self, indices):
+        indices = list(indices)
+        if not indices:
+            return _NumpyBatch(
+                np.zeros((0, self.out_h, self.out_w, 1), dtype=np.uint16)
+            )
+
+        # Render path calls are contiguous and increasing. Enforce best-effort alignment.
+        first = int(indices[0])
+        if first != self._next_index:
+            self.seek(first)
+
+        n = len(indices)
+        expected = self._frame_bytes * n
+        buf = self._read_exact(expected)
+        if len(buf) != expected:
+            raise EOFError(
+                f"FFmpegDepthPipeReader: expected {expected} bytes, got {len(buf)}"
+            )
+
+        arr = np.frombuffer(buf, dtype=np.uint16).reshape(n, self.out_h, self.out_w, 1)
+        arr = self._maybe_apply_shift(arr)
+        self._next_index = first + n
+        return _NumpyBatch(arr.copy())
+
+
+def load_pre_rendered_depth(
+    depth_map_path: str,
+    process_length: int,
+    target_height: int,
+    target_width: int,
+    match_resolution_to_target: bool,
+) -> Tuple[Any, int, int, int, Optional[dict]]:
+    """
+    Initializes a reader for chunked depth map reading.
+    Preserves 10-bit+ depth maps by using an ffmpeg-backed reader when needed.
+    No normalization or autogain is applied here.
+    Returns:
+        (depth_reader, total_depth_frames_to_process, actual_depth_height, actual_depth_width, depth_stream_info)
+    """
+    logger.debug(f"==> Initializing depth reader from: {depth_map_path}")
+
+    depth_stream_info = get_video_stream_info(depth_map_path)
+    bit_depth = _infer_depth_bit_depth(depth_stream_info)
+    pix_fmt = str((depth_stream_info or {}).get("pix_fmt", ""))
+
+    logger.info(
+        f"==> Depth map stream: pix_fmt='{pix_fmt}', profile='{str((depth_stream_info or {}).get('profile', ''))}', inferred_bit_depth={bit_depth}"
+    )
+
+    if depth_map_path.lower().endswith((".mp4", ".avi", ".mov", ".mkv")):
+        # Determine total frames available (for mismatch checks) without relying on the ffmpeg pipe.
+        total_depth_frames_available = 0
+        try:
+            # Prefer ffprobe-derived frame count when available; Decord can be slow on some files.
+            nb = None
+            if isinstance(depth_stream_info, dict):
+                nb = depth_stream_info.get("nb_frames")
+                if nb in (None, "", "N/A"):
+                    nb = depth_stream_info.get("num_frames") or depth_stream_info.get("nb_read_frames")
+            if nb not in (None, "", "N/A"):
+                total_depth_frames_available = int(float(nb))
+            else:
+                _tmp = VideoReader(depth_map_path, ctx=cpu(0))
+                total_depth_frames_available = len(_tmp)
+                del _tmp
+        except Exception as e:
+            logger.warning(
+                f"Could not determine depth frame count for '{depth_map_path}': {e}"
+            )
+
+        total_depth_frames_to_process = total_depth_frames_available
+        if process_length != -1 and process_length < total_depth_frames_available:
+            total_depth_frames_to_process = process_length
+
+        # Choose reader implementation
+        if bit_depth > 8:
+            depth_reader = FFmpegDepthPipeReader(
+                depth_map_path,
+                out_w=target_width,
+                out_h=target_height,
+                bit_depth=bit_depth,
+                num_frames=total_depth_frames_available,
+                pix_fmt=pix_fmt,
+            )
+        else:
+            # 8-bit: decode at native res with Decord, then (optionally) resize with OpenCV for parity.
+            depth_reader = VideoReader(
+                depth_map_path, ctx=cpu(0)
+            )  # decode at native res; resize with OpenCV later for parity
+
+            # --- NEW: When match_resolution_to_target=True, wrap the reader so it *outputs* frames at (target_width,target_height)
+            # using OpenCV resizing (NOT Decord scaling). This keeps 8-bit preview/render parity and also enables the low-res
+            # pipeline to preprocess at clip resolution (depth_target_w/h are set to original clip res for low-res tasks).
+            class _NumpyBatch:
+                def __init__(self, arr):
+                    self._arr = arr
+
+                def asnumpy(self):
+                    return self._arr
+
+            class _ResizingDepthReader:
+                def __init__(self, inner_reader, out_w, out_h):
+                    self._inner = inner_reader
+                    self._out_w = int(out_w)
+                    self._out_h = int(out_h)
+
+                def __len__(self):
+                    return len(self._inner)
+
+                def seek(self, *args, **kwargs):
+                    return self._inner.seek(*args, **kwargs)
+
+                def get_batch(self, indices):
+                    arr = self._inner.get_batch(indices).asnumpy()
+                    # arr is typically (N,H,W,C) from Decord
+                    in_h = int(arr.shape[1])
+                    in_w = int(arr.shape[2])
+                    if in_w == self._out_w and in_h == self._out_h:
+                        return _NumpyBatch(arr)
+
+                    interp = (
+                        cv2.INTER_LINEAR
+                        if (self._out_w > in_w or self._out_h > in_h)
+                        else cv2.INTER_AREA
+                    )
+
+                    if arr.ndim == 4:
+                        out = np.empty(
+                            (arr.shape[0], self._out_h, self._out_w, arr.shape[3]),
+                            dtype=arr.dtype,
+                        )
+                        for i in range(arr.shape[0]):
+                            out[i] = cv2.resize(
+                                arr[i], (self._out_w, self._out_h), interpolation=interp
+                            )
+                    else:
+                        out = np.empty(
+                            (arr.shape[0], self._out_h, self._out_w), dtype=arr.dtype
+                        )
+                        for i in range(arr.shape[0]):
+                            out[i] = cv2.resize(
+                                arr[i], (self._out_w, self._out_h), interpolation=interp
+                            )
+
+                    return _NumpyBatch(out)
+
+            first_depth_frame_shape = depth_reader.get_batch([0]).asnumpy().shape
+            actual_depth_height, actual_depth_width = first_depth_frame_shape[1:3]
+
+            if match_resolution_to_target and (
+                actual_depth_width != target_width
+                or actual_depth_height != target_height
+            ):
+                depth_reader = _ResizingDepthReader(
+                    depth_reader, out_w=target_width, out_h=target_height
+                )
+                actual_depth_height, actual_depth_width = (
+                    int(target_height),
+                    int(target_width),
+                )
+
+        first_depth_frame_shape = depth_reader.get_batch([0]).asnumpy().shape
+        actual_depth_height, actual_depth_width = first_depth_frame_shape[1:3]
+
+        logger.debug(
+            f"==> Depth reader ready. Final depth resolution: {actual_depth_width}x{actual_depth_height}. "
+            f"Frames available: {total_depth_frames_available}. Frames to process: {total_depth_frames_to_process}. "
+            f"bit_depth={bit_depth}, pix_fmt='{pix_fmt}'."
+        )
+
+        return (
+            depth_reader,
+            total_depth_frames_to_process,
+            actual_depth_height,
+            actual_depth_width,
+            depth_stream_info,
+        )
+
+    elif depth_map_path.lower().endswith(".npz"):
+        logger.error(
+            "NPZ support is temporarily disabled with disk chunking refactor. Please convert NPZ to MP4 depth video."
+        )
+        raise NotImplementedError(
+            "NPZ depth map loading is not yet supported with disk chunking."
+        )
+    else:
+        raise ValueError(
+            f"Unsupported depth map format: {os.path.basename(depth_map_path)}. Only MP4 are supported with disk chunking."
+        )
+
+
+if __name__ == "__main__":
+    CUDA_AVAILABLE = check_cuda_availability()  # Sets the global flag
+
+    app = SplatterGUI()
+    app.mainloop()
\ No newline at end of file
diff --git a/splatting_gui.py b/splatting_gui.py
index 2aa7ac4..08693ce 100644
--- a/splatting_gui.py
+++ b/splatting_gui.py
@@ -86,443 +86,28 @@ except:
     logger.info("Forward Warp Pytorch is active.")
 from dependency.video_previewer import VideoPreviewer
 
-GUI_VERSION = "26-01-30.0"
-
-
-class FusionSidecarGenerator:
-    """Handles parsing Fusion Export files, matching them to depth maps,
-    and generating/saving FSSIDECAR files using carry-forward logic."""
-
-    FUSION_PARAMETER_CONFIG = {
-        # Key: {Label, Type, Default, FusionKey(fsexport), SidecarKey(fssidecar), Decimals}
-        "convergence": {
-            "label": "Convergence Plane",
-            "type": float,
-            "default": 0.5,
-            "fusion_key": "Convergence",
-            "sidecar_key": "convergence_plane",
-            "decimals": 3,
-        },
-        "max_disparity": {
-            "label": "Max Disparity",
-            "type": float,
-            "default": 35.0,
-            "fusion_key": "MaxDisparity",
-            "sidecar_key": "max_disparity",
-            "decimals": 1,
-        },
-        "gamma": {
-            "label": "Gamma Correction",
-            "type": float,
-            "default": 1.0,
-            "fusion_key": "FrontGamma",
-            "sidecar_key": "gamma",
-            "decimals": 2,
-        },
-        # These keys exist in the sidecar manager but are usually set in the source tool
-        # We include them here for completeness if Fusion ever exported them
-        "frame_overlap": {
-            "label": "Frame Overlap",
-            "type": float,
-            "default": 3,
-            "fusion_key": "Overlap",
-            "sidecar_key": "frame_overlap",
-            "decimals": 0,
-        },
-        "input_bias": {
-            "label": "Input Bias",
-            "type": float,
-            "default": 0.0,
-            "fusion_key": "Bias",
-            "sidecar_key": "input_bias",
-            "decimals": 2,
-        },
-        "left_border": {
-            "label": "Left Border",
-            "type": float,
-            "default": 0.0,
-            "fusion_key": "LeftBorder",
-            "sidecar_key": "left_border",
-            "decimals": 3,
-        },
-        "right_border": {
-            "label": "Right Border",
-            "type": float,
-            "default": 0.0,
-            "fusion_key": "RightBorder",
-            "sidecar_key": "right_border",
-            "decimals": 3,
-        },
-        "manual_border": {
-            "label": "Border Mode",
-            "type": str,
-            "default": "Off",
-            "fusion_key": "BorderMode",
-            "sidecar_key": "border_mode",
-            "decimals": 0,
-        },
-        "auto_border_l": {
-            "label": "Auto Border L",
-            "type": float,
-            "default": 0.0,
-            "fusion_key": "AutoBorderL",
-            "sidecar_key": "auto_border_L",
-            "decimals": 3,
-        },
-        "auto_border_r": {
-            "label": "Auto Border R",
-            "type": float,
-            "default": 0.0,
-            "fusion_key": "AutoBorderR",
-            "sidecar_key": "auto_border_R",
-            "decimals": 3,
-        },
-    }
-
-    def __init__(self, master_gui, sidecar_manager):
-        self.master_gui = master_gui
-        self.sidecar_manager = sidecar_manager
-        self.logger = logging.getLogger(__name__)
-
-    def _get_video_frame_count(self, file_path):
-        """Safely gets the frame count of a video file using moviepy."""
-        try:
-            clip = VideoFileClip(file_path)
-            fps = clip.fps
-            duration = clip.duration
-            if fps is None or duration is None:
-                # If moviepy failed to get reliable info, fall back
-                fps = 24
-                if duration is None:
-                    return 0
-
-            frames = math.ceil(duration * fps)
-            clip.close()
-            return frames
-        except Exception as e:
-            self.logger.warning(
-                f"Error getting frame count for {os.path.basename(file_path)}: {e}"
-            )
-            return 0
-
-    def _load_and_validate_fsexport(self, file_path):
-        """Loads, parses, and validates marker data from a Fusion Export file."""
-        try:
-            with open(file_path, "r") as f:
-                export_data = json.load(f)
-        except json.JSONDecodeError as e:
-            messagebox.showerror(
-                "File Error",
-                f"Failed to parse JSON in {os.path.basename(file_path)}: {e}",
-            )
-            return None
-        except Exception as e:
-            messagebox.showerror(
-                "File Error", f"Failed to read {os.path.basename(file_path)}: {e}"
-            )
-            return None
-
-        markers = export_data.get("markers", [])
-        if not markers:
-            messagebox.showwarning(
-                "Data Warning", "No 'markers' found in the export file."
-            )
-            return None
-
-        # Sort markers by frame number (critical for carry-forward logic)
-        markers.sort(key=lambda m: m["frame"])
-        self.logger.info(
-            f"Loaded {len(markers)} markers from {os.path.basename(file_path)}."
-        )
-        return markers
-
-    def _scan_target_videos(self, folder):
-        """Scans the target folder for video files and computes their frame counts."""
-        video_extensions = ("*.mp4", "*.avi", "*.mov", "*.mkv")
-        found_files_paths = []
-        for ext in video_extensions:
-            found_files_paths.extend(glob.glob(os.path.join(folder, ext)))
-        sorted_files_paths = sorted(found_files_paths)
-
-        if not sorted_files_paths:
-            messagebox.showwarning(
-                "No Files", f"No video depth map files found in: {folder}"
-            )
-            return None
-
-        target_video_data = []
-        cumulative_frames = 0
-
-        for full_path in sorted_files_paths:
-            total_frames = self._get_video_frame_count(full_path)
-
-            if total_frames == 0:
-                self.logger.warning(
-                    f"Skipping {os.path.basename(full_path)} due to zero frame count."
-                )
-                continue
-
-            target_video_data.append(
-                {
-                    "full_path": full_path,
-                    "basename": os.path.basename(full_path),
-                    "total_frames": total_frames,
-                    "timeline_start_frame": cumulative_frames,
-                    "timeline_end_frame": cumulative_frames + total_frames - 1,
-                }
-            )
-            cumulative_frames += total_frames
-
-        self.logger.info(
-            f"Scanned {len(target_video_data)} video files. Total timeline frames: {cumulative_frames}."
-        )
-        return target_video_data
-
-    def generate_sidecars(self):
-        """Main entry point for the Fusion Export to Sidecar generation workflow."""
-
-        # 1. Select Fusion Export File
-        export_file_path = filedialog.askopenfilename(
-            defaultextension=".fsexport",
-            filetypes=[
-                ("Fusion Export Files", "*.fsexport.txt;*.fsexport"),
-                ("All Files", "*.*"),
-            ],
-            title="Select Fusion Export (.fsexport) File",
-        )
-        if not export_file_path:
-            self.master_gui.status_label.config(
-                text="Fusion export selection cancelled."
-            )
-            return
-
-        markers = self._load_and_validate_fsexport(export_file_path)
-        if markers is None:
-            self.master_gui.status_label.config(text="Fusion export loading failed.")
-            return
-
-        # 2. Select Target Depth Map Folder
-        target_folder = filedialog.askdirectory(title="Select Target Depth Map Folder")
-        if not target_folder:
-            self.master_gui.status_label.config(
-                text="Depth map folder selection cancelled."
-            )
-            return
-
-        target_videos = self._scan_target_videos(target_folder)
-        if target_videos is None or not target_videos:
-            self.master_gui.status_label.config(text="No valid depth map videos found.")
-            return
-
-        # 3. Apply Parameters (Carry-Forward Logic)
-        applied_count = 0
-
-        # Initialize last known values with the config defaults
-        last_param_vals = {}
-        for key, config in self.FUSION_PARAMETER_CONFIG.items():
-            last_param_vals[key] = config["default"]
-
-        for file_data in target_videos:
-            file_start_frame = file_data["timeline_start_frame"]
-
-            # Find the most relevant marker (latest marker frame <= file_start_frame)
-            relevant_marker = None
-            for marker in markers:
-                if marker["frame"] <= file_start_frame:
-                    relevant_marker = marker
-                else:
-                    break
-
-            current_param_vals = last_param_vals.copy()
-
-            if relevant_marker and relevant_marker.get("values"):
-                marker_values = relevant_marker["values"]
-                updated_from_marker = False
-
-                for key, config in self.FUSION_PARAMETER_CONFIG.items():
-                    fusion_key = config["fusion_key"]
-                    default_val = config["default"]
-
-                    if fusion_key in marker_values:
-                        # Attempt to cast the value from the marker to the expected type
-                        val = marker_values.get(fusion_key, default_val)
-                        try:
-                            current_param_vals[key] = config["type"](val)
-                            updated_from_marker = True
-                        except (ValueError, TypeError):
-                            self.logger.warning(
-                                f"Marker value for '{fusion_key}' is invalid ({val}). Using previous/default value."
-                            )
+GUI_VERSION = "26-02-02.Big Refactoring"
 
-                if updated_from_marker:
-                    applied_count += 1
-
-            # 4. Save Sidecar JSON
-            sidecar_data = {}
-            for key, config in self.FUSION_PARAMETER_CONFIG.items():
-                value = current_param_vals[key]
-                if config["type"] is bool:
-                    sidecar_data[config["sidecar_key"]] = bool(value)
-                else:
-                    # Round to configured decimals for clean sidecar output
-                    sidecar_data[config["sidecar_key"]] = round(
-                        float(value), config["decimals"]
-                    )
-
-            base_name_without_ext = os.path.splitext(file_data["full_path"])[0]
-            json_filename = (
-                base_name_without_ext + ".fssidecar"
-            )  # Target sidecar extension
-
-            if not self.sidecar_manager.save_sidecar_data(json_filename, sidecar_data):
-                self.logger.error(
-                    f"Failed to save sidecar for {file_data['basename']}."
-                )
-
-            # Update last values for carry-forward to the next file
-            last_param_vals = current_param_vals.copy()
-
-        # 5. Final Status
-        if applied_count == 0:
-            self.master_gui.status_label.config(
-                text="Finished: No parameters were applied from the export file."
-            )
-        else:
-            self.master_gui.status_label.config(
-                text=f"Finished: Applied markers to {applied_count} files, generated {len(target_videos)} FSSIDECARs."
-            )
-        messagebox.showinfo(
-            "Sidecar Generation Complete",
-            f"Successfully processed {os.path.basename(export_file_path)} and generated {len(target_videos)} FSSIDECAR files.",
-        )
-
-    def generate_custom_sidecars(self):
-        """Generates sidecars with a custom name without requiring existing video files."""
-
-        # 1. Select Fusion Export File
-        export_file_path = filedialog.askopenfilename(
-            defaultextension=".fsexport",
-            filetypes=[
-                ("Fusion Export Files", "*.fsexport.txt;*.fsexport"),
-                ("All Files", "*.*"),
-            ],
-            title="Select Fusion Export (.fsexport) File",
-        )
-        if not export_file_path:
-            self.master_gui.status_label.config(
-                text="Fusion export selection cancelled."
-            )
-            return
-
-        markers = self._load_and_validate_fsexport(export_file_path)
-        if markers is None:
-            self.master_gui.status_label.config(text="Fusion export loading failed.")
-            return
-
-        # 2. Select Output Sidecar Path and Name
-        custom_save_path = filedialog.asksaveasfilename(
-            defaultextension=".fssidecar",
-            filetypes=[("Sidecar Files", "*.fssidecar")],
-            title="Save Sidecar As",
-            initialfile=os.path.splitext(os.path.basename(export_file_path))[0],
-        )
-        if not custom_save_path:
-            self.master_gui.status_label.config(text="Custom sidecar export cancelled.")
-            return
-
-        # 3. Process markers
-        applied_count = 0
-        last_param_vals = {
-            key: config["default"]
-            for key, config in self.FUSION_PARAMETER_CONFIG.items()
-        }
-
-        for i, marker in enumerate(markers):
-            current_param_vals = last_param_vals.copy()
-            if marker.get("values"):
-                marker_values = marker["values"]
-                updated_from_marker = False
-                for key, config in self.FUSION_PARAMETER_CONFIG.items():
-                    fusion_key = config["fusion_key"]
-                    if fusion_key in marker_values:
-                        try:
-                            current_param_vals[key] = config["type"](
-                                marker_values[fusion_key]
-                            )
-                            updated_from_marker = True
-                        except (ValueError, TypeError):
-                            pass
-                if updated_from_marker:
-                    applied_count += 1
-
-            # Prepare sidecar data
-            sidecar_data = {}
-            for key, config in self.FUSION_PARAMETER_CONFIG.items():
-                value = current_param_vals[key]
-                if config["type"] is bool:
-                    sidecar_data[config["sidecar_key"]] = bool(value)
-                else:
-                    sidecar_data[config["sidecar_key"]] = round(
-                        float(value), config["decimals"]
-                    )
-
-            # Determine filename
-            if len(markers) == 1:
-                target_filename = custom_save_path
-            else:
-                # Append index if multiple markers (5-digit zero-padded)
-                base, ext = os.path.splitext(custom_save_path)
-                target_filename = f"{base}_{i + 1:04d}{ext}"
-
-            if not self.sidecar_manager.save_sidecar_data(
-                target_filename, sidecar_data
-            ):
-                self.logger.error(f"Failed to save custom sidecar: {target_filename}")
-
-            last_param_vals = current_param_vals.copy()
-
-        # 4. Final Status
-        self.master_gui.status_label.config(
-            text=f"Finished: Generated {len(markers)} custom FSSIDECARs."
-        )
-        messagebox.showinfo(
-            "Custom Export Complete",
-            f"Successfully generated {len(markers)} custom FSSIDECAR files.",
-        )
 
 
-class ForwardWarpStereo(nn.Module):
-    """
-    PyTorch module for forward warping an image based on a disparity map.
-    """
+# [REFACTORED] FusionSidecarGenerator class replaced with core import
+from core.splatting import FusionSidecarGenerator
 
-    def __init__(self, eps=1e-6, occlu_map=False):
-        super(ForwardWarpStereo, self).__init__()
-        self.eps = eps
-        self.occlu_map = occlu_map
-        self.fw = forward_warp()
-
-    def forward(self, im, disp):
-        im = im.contiguous()
-        disp = disp.contiguous()
-        weights_map = disp - disp.min()
-        weights_map = (1.414) ** weights_map
-        flow = -disp.squeeze(1)
-        dummy_flow = torch.zeros_like(flow, requires_grad=False)
-        flow = torch.stack((flow, dummy_flow), dim=-1)
-        res_accum = self.fw(im * weights_map, flow)
-        mask = self.fw(weights_map, flow)
-        mask.clamp_(min=self.eps)
-        res = res_accum / mask
-        if not self.occlu_map:
-            return res
-        else:
-            ones = torch.ones_like(disp, requires_grad=False)
-            occlu_map = self.fw(ones, flow)
-            occlu_map.clamp_(0.0, 1.0)
-            occlu_map = 1.0 - occlu_map
-            return res, occlu_map
+# [REFACTORED] ForwardWarpStereo class replaced with core import
+from core.splatting import (
+    ForwardWarpStereo,
+    ConvergenceEstimatorWrapper,
+    BorderScanner,
+    BatchProcessor,
+    ProcessingSettings,
+    ProcessingTask,
+    BatchSetupResult,
+)
+from core.splatting.config_manager import ConfigManager
 
+# [REFACTORED] Video I/O and Theme functions replaced with core imports
+from core.common import ThemeManager
+from core.common.video_io import read_video_frames, _NumpyBatch
 
 class SplatterGUI(ThemedTk):
     # --- UI MINIMUM WIDTHS (tweak these numbers) ---
@@ -590,9 +175,12 @@ class SplatterGUI(ThemedTk):
         super().__init__(theme="default")
         self.title(f"Stereocrafter Splatting (Batch) {GUI_VERSION}")
 
+        self.config_manager = ConfigManager()
         self.app_config = {}
         self.help_texts = {}
         self.sidecar_manager = SidecarConfigManager()
+        self.convergence_estimator = ConvergenceEstimatorWrapper()
+        self.border_scanner = BorderScanner(gui_context=self)
         # Cache: estimated per-clip max Total(D+P) keyed by signature
         self._dp_total_est_cache = {}
 
@@ -633,194 +221,82 @@ class SplatterGUI(ThemedTk):
         # --- Variables with defaults ---
         defaults = self.APP_CONFIG_DEFAULTS  # Convenience variable
 
-        self.dark_mode_var = tk.BooleanVar(
-            value=self.app_config.get("dark_mode_enabled", False)
-        )
-        self.input_source_clips_var = tk.StringVar(
-            value=self.app_config.get("input_source_clips", "./input_source_clips")
-        )
-        self.input_depth_maps_var = tk.StringVar(
-            value=self.app_config.get("input_depth_maps", "./input_depth_maps")
-        )
-        self.multi_map_var = tk.BooleanVar(
-            value=bool(self.app_config.get("multi_map_enabled", False))
-        )
+        self.dark_mode_var = tk.BooleanVar(value=False)
+        self.input_source_clips_var = tk.StringVar(value="./input_source_clips")
+        self.input_depth_maps_var = tk.StringVar(value="./input_depth_maps")
+        self.multi_map_var = tk.BooleanVar(value=False)
         self.selected_depth_map_var = tk.StringVar(value="")
-        self.depth_map_subfolders = []  # List of valid subfolders
-        self.depth_map_radio_buttons = []  # keep list for UI management
-        self.depth_map_radio_dict = {}  # NEW: map text->widget
-        self._current_video_sidecar_map = None  # Track sidecar's selected map
-        self._suppress_sidecar_map_update = (
-            False  # Prevent overwriting manual selections
-        )
-        self._last_loaded_source_video = (
-            None  # Track source video for NEW video detection
-        )
+        self.depth_map_subfolders = []
+        self.depth_map_radio_buttons = []
+        self.depth_map_radio_dict = {}
+        self._current_video_sidecar_map = None
+        self._suppress_sidecar_map_update = False
+        self._last_loaded_source_video = None
+        
         self.input_depth_maps_var.trace_add(
             "write", lambda *args: self._on_depth_map_folder_changed()
         )
-        self.output_splatted_var = tk.StringVar(
-            value=self.app_config.get("output_splatted", "./output_splatted")
-        )
-
-        self.max_disp_var = tk.StringVar(
-            value=self.app_config.get("max_disp", defaults["MAX_DISP"])
-        )
-        self.process_length_var = tk.StringVar(
-            value=self.app_config.get("process_length", defaults["PROC_LENGTH"])
-        )
+        self.output_splatted_var = tk.StringVar(value="./output_splatted")
+        self.max_disp_var = tk.StringVar(value=defaults["MAX_DISP"])
+        self.process_length_var = tk.StringVar(value=defaults["PROC_LENGTH"])
         self.process_from_var = tk.StringVar(value="")
         self.process_to_var = tk.StringVar(value="")
-        self.batch_size_var = tk.StringVar(
-            value=self.app_config.get("batch_size", defaults["BATCH_SIZE_FULL"])
-        )
-
-        self.dual_output_var = tk.BooleanVar(
-            value=self.app_config.get("dual_output", False)
-        )
-        self.enable_global_norm_var = tk.BooleanVar(
-            value=self.app_config.get("enable_global_norm", False)
-        )
-        self.enable_full_res_var = tk.BooleanVar(
-            value=self.app_config.get("enable_full_resolution", True)
-        )
-        self.enable_low_res_var = tk.BooleanVar(
-            value=self.app_config.get("enable_low_resolution", True)
-        )
-        self.pre_res_width_var = tk.StringVar(
-            value=self.app_config.get("pre_res_width", "1024")
-        )
-        self.pre_res_height_var = tk.StringVar(
-            value=self.app_config.get("pre_res_height", "512")
-        )
-        self.low_res_batch_size_var = tk.StringVar(
-            value=self.app_config.get("low_res_batch_size", defaults["BATCH_SIZE_LOW"])
-        )
-        self.zero_disparity_anchor_var = tk.StringVar(
-            value=self.app_config.get("convergence_point", defaults["CONV_POINT"])
-        )
-        self.output_crf_var = tk.StringVar(
-            value=self.app_config.get("output_crf", defaults["CRF_OUTPUT"])
-        )
-        # Separate CRF values for Full vs Low output (fallback to legacy "output_crf")
-        _legacy_crf = self.app_config.get("output_crf", defaults["CRF_OUTPUT"])
-        self.output_crf_full_var = tk.StringVar(
-            value=self.app_config.get("output_crf_full", _legacy_crf)
-        )
-        self.output_crf_low_var = tk.StringVar(
-            value=self.app_config.get("output_crf_low", _legacy_crf)
-        )
-        # Output color metadata tags (metadata-only; does not affect splat math)
-        self.color_tags_mode_var = tk.StringVar(
-            value=self.app_config.get("color_tags_mode", "Auto")
-        )
-
-        # Dev Tools (not saved to config)
-        self.skip_lowres_preproc_var = tk.BooleanVar(
-            value=bool(self.app_config.get("skip_lowres_preproc", False))
-        )
-
-        # Dev Tools: Render-time exact Max Total(D+P) tracking (toggle for perf testing; not saved)
+        self.batch_size_var = tk.StringVar(value=defaults["BATCH_SIZE_FULL"])
+        self.dual_output_var = tk.BooleanVar(value=False)
+        self.enable_global_norm_var = tk.BooleanVar(value=False)
+        self.enable_full_res_var = tk.BooleanVar(value=True)
+        self.enable_low_res_var = tk.BooleanVar(value=False)
+        self.pre_res_width_var = tk.StringVar(value="1920")
+        self.pre_res_height_var = tk.StringVar(value="1080")
+        self.low_res_batch_size_var = tk.StringVar(value=defaults["BATCH_SIZE_LOW"])
+        self.zero_disparity_anchor_var = tk.StringVar(value=defaults["CONV_POINT"])
+        self.output_crf_var = tk.StringVar(value=defaults["CRF_OUTPUT"])
+        self.output_crf_full_var = tk.StringVar(value=defaults["CRF_OUTPUT"])
+        self.output_crf_low_var = tk.StringVar(value=defaults["CRF_OUTPUT"])
+        self.color_tags_mode_var = tk.StringVar(value="Auto")
+        self.skip_lowres_preproc_var = tk.BooleanVar(value=False)
         self.track_dp_total_true_on_render_var = tk.BooleanVar(value=False)
+        self.move_to_finished_var = tk.BooleanVar(value=True)
+        self.crosshair_enabled_var = tk.BooleanVar(value=False)
+        self.crosshair_white_var = tk.BooleanVar(value=False)
+        self.crosshair_multi_var = tk.BooleanVar(value=False)
+        self.depth_pop_enabled_var = tk.BooleanVar(value=False)
+        self.auto_convergence_mode_var = tk.StringVar(value="Off")
+        self.depth_gamma_var = tk.StringVar(value=defaults["DEPTH_GAMMA"])
+        self.depth_dilate_size_x_var = tk.StringVar(value=defaults["DEPTH_DILATE_SIZE_X"])
+        self.depth_dilate_size_y_var = tk.StringVar(value=defaults["DEPTH_DILATE_SIZE_Y"])
+        self.depth_blur_size_x_var = tk.StringVar(value=defaults["DEPTH_BLUR_SIZE_X"])
+        self.depth_blur_size_y_var = tk.StringVar(value=defaults["DEPTH_BLUR_SIZE_Y"])
+        self.depth_dilate_left_var = tk.StringVar(value=defaults["DEPTH_DILATE_LEFT"])
+        self.depth_blur_left_var = tk.StringVar(value=defaults["DEPTH_BLUR_LEFT"])
+        self.depth_blur_left_mix_var = tk.StringVar(value=defaults["DEPTH_BLUR_LEFT_MIX"])
+        self.enable_sidecar_gamma_var = tk.BooleanVar(value=True)
+        self.enable_sidecar_blur_dilate_var = tk.BooleanVar(value=True)
+        self.update_slider_from_sidecar_var = tk.BooleanVar(value=True)
+        self.auto_save_sidecar_var = tk.BooleanVar(value=False)
+        self.border_width_var = tk.StringVar(value=defaults["BORDER_WIDTH"])
+        self.border_bias_var = tk.StringVar(value=defaults["BORDER_BIAS"])
+        self.border_mode_var = tk.StringVar(value=defaults["BORDER_MODE"])
+        self.auto_border_L_var = tk.StringVar(value=defaults["AUTO_BORDER_L"])
+        self.auto_border_R_var = tk.StringVar(value=defaults["AUTO_BORDER_R"])
+        self.preview_source_var = tk.StringVar(value="Splat Result")
+        self.preview_size_var = tk.StringVar(value="75%")
+        
+        # --- NEW Sync from ConfigManager ---
+        self.config_manager.sync_to_tk_vars(self.__dict__)
+        
+        # Manual sync for non-standard mappings
+        if "convergence_point" in self.app_config:
+            self.zero_disparity_anchor_var.set(str(self.app_config["convergence_point"]))
+        if "multi_map_enabled" in self.app_config:
+            self.multi_map_var.set(bool(self.app_config["multi_map_enabled"]))
+        if "dark_mode_enabled" in self.app_config:
+            self.dark_mode_var.set(bool(self.app_config["dark_mode_enabled"]))
+        if "enable_full_resolution" in self.app_config:
+            self.enable_full_res_var.set(bool(self.app_config["enable_full_resolution"]))
+        if "enable_low_resolution" in self.app_config:
+            self.enable_low_res_var.set(bool(self.app_config["enable_low_resolution"]))
 
-        self.move_to_finished_var = tk.BooleanVar(
-            value=self.app_config.get("move_to_finished", True)
-        )
-
-        # Crosshair overlay for convergence checking (preview-only; not exported)
-        self.crosshair_enabled_var = tk.BooleanVar(
-            value=bool(self.app_config.get("crosshair_enabled", False))
-        )
-        self.crosshair_white_var = tk.BooleanVar(
-            value=bool(self.app_config.get("crosshair_white", False))
-        )
-        self.crosshair_multi_var = tk.BooleanVar(
-            value=bool(self.app_config.get("crosshair_multi", False))
-        )
-        self.depth_pop_enabled_var = tk.BooleanVar(
-            value=bool(self.app_config.get("depth_pop_enabled", False))
-        )
-
-        self.auto_convergence_mode_var = tk.StringVar(
-            value=self.app_config.get("auto_convergence_mode", "Off")
-        )
-
-        # --- Depth Pre-processing Variables ---
-        self.depth_gamma_var = tk.StringVar(
-            value=self.app_config.get("depth_gamma", defaults["DEPTH_GAMMA"])
-        )
-        _ddx = self.app_config.get(
-            "depth_dilate_size_x", defaults["DEPTH_DILATE_SIZE_X"]
-        )
-        _ddy = self.app_config.get(
-            "depth_dilate_size_y", defaults["DEPTH_DILATE_SIZE_Y"]
-        )
-        # Backward compatibility: older configs used 30..40 to represent erosion (-0..-10)
-        try:
-            _ddx_f = float(_ddx)
-            if 30.0 < _ddx_f <= 40.0:
-                _ddx = -(_ddx_f - 30.0)
-        except Exception:
-            pass
-        try:
-            _ddy_f = float(_ddy)
-            if 30.0 < _ddy_f <= 40.0:
-                _ddy = -(_ddy_f - 30.0)
-        except Exception:
-            pass
-        self.depth_dilate_size_x_var = tk.StringVar(value=str(_ddx))
-        self.depth_dilate_size_y_var = tk.StringVar(value=str(_ddy))
-        self.depth_blur_size_x_var = tk.StringVar(
-            value=self.app_config.get(
-                "depth_blur_size_x", defaults["DEPTH_BLUR_SIZE_X"]
-            )
-        )
-        self.depth_blur_size_y_var = tk.StringVar(
-            value=self.app_config.get(
-                "depth_blur_size_y", defaults["DEPTH_BLUR_SIZE_Y"]
-            )
-        )
-        self.depth_dilate_left_var = tk.StringVar(
-            value=self.app_config.get(
-                "depth_dilate_left", defaults["DEPTH_DILATE_LEFT"]
-            )
-        )
-        self.depth_blur_left_var = tk.StringVar(
-            value=self.app_config.get("depth_blur_left", defaults["DEPTH_BLUR_LEFT"])
-        )
-        self.depth_blur_left_mix_var = tk.StringVar(
-            value=self.app_config.get(
-                "depth_blur_left_mix", defaults["DEPTH_BLUR_LEFT_MIX"]
-            )
-        )
-        # --- NEW: Sidecar Control Toggle Variables ---
-        self.enable_sidecar_gamma_var = tk.BooleanVar(
-            value=self.app_config.get("enable_sidecar_gamma", True)
-        )
-        self.enable_sidecar_blur_dilate_var = tk.BooleanVar(
-            value=self.app_config.get("enable_sidecar_blur_dilate", True)
-        )
-        self.update_slider_from_sidecar_var = tk.BooleanVar(
-            value=self.app_config.get("update_slider_from_sidecar", True)
-        )
-        self.auto_save_sidecar_var = tk.BooleanVar(
-            value=self.app_config.get("auto_save_sidecar", False)
-        )
-        self.border_width_var = tk.StringVar(
-            value=self.app_config.get("border_width", defaults["BORDER_WIDTH"])
-        )
-        self.border_bias_var = tk.StringVar(
-            value=self.app_config.get("border_bias", defaults["BORDER_BIAS"])
-        )
-        self.border_mode_var = tk.StringVar(
-            value=self.app_config.get("border_mode", defaults["BORDER_MODE"])
-        )
-        self.auto_border_L_var = tk.StringVar(
-            value=self.app_config.get("auto_border_L", defaults["AUTO_BORDER_L"])
-        )
-        self.auto_border_R_var = tk.StringVar(
-            value=self.app_config.get("auto_border_R", defaults["AUTO_BORDER_R"])
-        )
         # Add traces for automatic border calculation (Auto Basic mode)
         self.zero_disparity_anchor_var.trace_add(
             "write", self._on_convergence_or_disparity_changed
@@ -828,14 +304,6 @@ class SplatterGUI(ThemedTk):
         self.max_disp_var.trace_add("write", self._on_convergence_or_disparity_changed)
         self.border_mode_var.trace_add("write", self._on_border_mode_change)
 
-        # --- NEW: Previewer Variables ---
-        self.preview_source_var = tk.StringVar(
-            value=self.app_config.get("preview_source", "Splat Result")
-        )
-        self.preview_size_var = tk.StringVar(
-            value=self.app_config.get("preview_size", "75%")
-        )
-
         # --- Variables for "Current Processing Information" display ---
         self.processing_filename_var = tk.StringVar(value="N/A")
         self.processing_resolution_var = tk.StringVar(value="N/A")
@@ -853,11 +321,17 @@ class SplatterGUI(ThemedTk):
         # --- Processing control variables ---
         self.stop_event = threading.Event()
         self.progress_queue = queue.Queue()
+        self.batch_processor = BatchProcessor(
+            progress_queue=self.progress_queue,
+            stop_event=self.stop_event,
+            sidecar_manager=self.sidecar_manager,
+        )
         self.processing_thread = None
 
         self._create_widgets()
         self._setup_keyboard_shortcuts()
         self.style = ttk.Style()
+        self.theme_manager = ThemeManager(dark_mode_var=self.dark_mode_var)
 
         self.update_idletasks()  # Ensure widgets are rendered for correct reqheight
         self._apply_theme(is_startup=True)  # Pass is_startup=True here
@@ -939,108 +413,46 @@ class SplatterGUI(ThemedTk):
         self.window_width = current_actual_width  # Update stored width
 
     def _apply_theme(self, is_startup: bool = False):
-        """Applies the selected theme (dark or light) to the GUI."""
-        # 1. Define color palettes
-        dark_colors = {
-            "bg": "#2b2b2b",
-            "fg": "white",
-            "entry_bg": "#3c3c3c",
-            "menu_bg": "#3c3c3c",
-            "menu_fg": "white",
-            "active_bg": "#555555",
-            "active_fg": "white",
-            "theme": "black",
-        }
-        light_colors = {
-            "bg": "#d9d9d9",
-            "fg": "black",
-            "entry_bg": "#ffffff",
-            "menu_bg": "#f0f0f0",
-            "menu_fg": "black",
-            "active_bg": "#dddddd",
-            "active_fg": "black",
-            "theme": "default",
-        }
-
-        # 2. Select the current palette and theme
-        if self.dark_mode_var.get():
-            colors = dark_colors
-        else:
-            colors = light_colors
+        """Applies the selected theme (dark or light) to the GUI using ThemeManager."""
+        if not hasattr(self, "theme_manager"):
+            return
 
-        self.style.theme_use(colors["theme"])
-        self.configure(bg=colors["bg"])
+        # 1. Apply styles to ttk widgets and root window
+        self.theme_manager.apply_theme_to_style(self.style, root_window=self)
 
-        # 3. Apply styles to ttk widgets
-        self.style.configure("TFrame", background=colors["bg"], foreground=colors["fg"])
-        self.style.configure(
-            "TLabelframe", background=colors["bg"], foreground=colors["fg"]
-        )
-        self.style.configure(
-            "TLabelframe.Label", background=colors["bg"], foreground=colors["fg"]
-        )
-        self.style.configure("TLabel", background=colors["bg"], foreground=colors["fg"])
-        self.style.configure(
-            "TCheckbutton", background=colors["bg"], foreground=colors["fg"]
-        )
-        self.style.map(
-            "TCheckbutton",
-            foreground=[("active", colors["fg"])],
-            background=[("active", colors["bg"])],
-        )
+        # 2. Apply theme to non-ttk widgets (Menu, Canvas, Labels)
+        colors = self.theme_manager.get_colors()
 
-        # 4. Configure Entry and Combobox widgets using style.map for robust background override
-        self.style.map(
-            "TEntry",
-            fieldbackground=[("", colors["entry_bg"])],
-            foreground=[("", colors["fg"])],
-        )
-        self.style.configure("TEntry", insertcolor=colors["fg"])
-        self.style.map(
-            "TCombobox",
-            fieldbackground=[("readonly", colors["entry_bg"])],
-            foreground=[("readonly", colors["fg"])],
-            selectbackground=[("readonly", colors["entry_bg"])],
-            selectforeground=[("readonly", colors["fg"])],
-        )
+        # Menus
+        if hasattr(self, "menubar"):
+            self.theme_manager.apply_theme_to_menus(
+                menus=[self.file_menu, self.help_menu], menubar=self.menubar
+            )
 
-        # Manually set the background for the previewer's canvas widget
+        # Previewer Canvas
         if hasattr(self, "previewer") and hasattr(self.previewer, "preview_canvas"):
-            self.previewer.preview_canvas.config(bg=colors["bg"], highlightthickness=0)
+            self.theme_manager.apply_theme_to_canvas(self.previewer.preview_canvas)
 
-        # 5. Manually configure non-ttk widgets (Menu, tk.Label)
-        if hasattr(self, "menubar"):
-            for menu in [self.menubar, self.file_menu, self.help_menu]:
-                menu.config(
-                    bg=colors["menu_bg"],
-                    fg=colors["menu_fg"],
-                    activebackground=colors["active_bg"],
-                    activeforeground=colors["active_fg"],
-                )
-        if hasattr(self, "info_frame"):
-            for label in self.info_labels:
-                label.config(bg=colors["bg"], fg=colors["fg"])
+        # Info Labels
+        if hasattr(self, "info_frame") and hasattr(self, "info_labels"):
+            self.theme_manager.apply_theme_to_labels(self.info_labels)
 
-        # 6. Handle window geometry adjustment (only after startup)
-        self.update_idletasks()  # Ensure all theme changes are rendered for accurate reqheight
-
-        # --- Apply geometry only if not during startup (NEW conditional block) ---
-        # if not is_startup:
-        #     self._adjust_window_height_for_content()
+        # 3. Handle window geometry adjustment
+        self.update_idletasks()
 
     def _auto_converge_worker(
         self, rgb_path, depth_map_path, process_length, batch_size, fallback_value, gamma, mode
     ):
         """Worker thread for running the Auto-Convergence calculation."""
 
-        # Run the NEW auto-convergence logic using Neural Estimator
-        new_anchor_avg, new_anchor_peak = self._determine_auto_convergence(
-            rgb_path,
-            depth_map_path,
-            process_length,
-            batch_size,
-            fallback_value,
-            gamma=gamma,
+        # Use the extracted ConvergenceEstimatorWrapper
+        new_anchor_avg, new_anchor_peak = self.convergence_estimator.estimate_convergence(
+            rgb_path=rgb_path,
+            depth_path=depth_map_path,
+            process_length=int(process_length),
+            gamma=float(gamma),
+            fallback_value=float(fallback_value),
+            stop_event=self.stop_event
         )
 
         # Use self.after to safely update the GUI from the worker thread
@@ -1054,127 +466,6 @@ class SplatterGUI(ThemedTk):
             ),
         )
 
-    def _determine_auto_convergence(
-        self,
-        rgb_path: str,
-        depth_map_path: str,
-        process_length: int,
-        batch_size: int,
-        fallback_value: float = 0.5,
-        gamma: float = 1.0,
-    ) -> Tuple[float, float]:
-        """
-        Uses the ConvergenceEstimator (U2NETP) to recommend a zero-parallax plane.
-        Returns: (average_convergence, peak_convergence)
-        """
-        if hasattr(self, "stop_event") and not self.stop_event:
-            self.stop_event = threading.Event()
-        
-        # Initialize Estimator
-        try:
-            # Lazy import to avoid circular dependency issues
-            from dependency.convergence_estimator import ConvergenceEstimator
-            estimator = ConvergenceEstimator()
-            if estimator.model is None:
-                 logger.error("ConvergenceEstimator model failed to load. Using fallback.")
-                 return fallback_value, fallback_value
-        except ImportError as e:
-            logger.error(f"Could not import ConvergenceEstimator: {e}")
-            return fallback_value, fallback_value
-            
-        try:
-            # Initialize Readers
-            # Use CPU context for safety and simplicity in this thread
-            vr_rgb = VideoReader(rgb_path, ctx=cpu(0))
-            vr_depth = VideoReader(depth_map_path, ctx=cpu(0))
-            
-            len_rgb = len(vr_rgb)
-            len_depth = len(vr_depth)
-            
-            # Sanity check
-            if len_rgb == 0 or len_depth == 0:
-                logger.warning("Empty video or depth map found.")
-                return fallback_value, fallback_value
-                
-            total_frames = min(len_rgb, len_depth)
-            
-            # Respect process_length if set > 0
-            if process_length > 0:
-                total_frames = min(total_frames, process_length)
-
-            # Sample frames (e.g., every 6 frames, 4 times per second at 24fps)
-            # This provides better coverage than a fixed small number
-            sample_stride = 6
-            indices = list(range(0, total_frames, sample_stride))
-            
-            # Ensure at least one frame is sampled
-            if not indices:
-                indices = [0]
-            
-            estimates = []
-            
-            logger.info(f"Auto-Converge: Sampling {len(indices)} frames from {os.path.basename(rgb_path)}...")
-
-            for idx in indices:
-                if self.stop_event.is_set():
-                    logger.info("Auto-Converge scan cancelled.")
-                    break
-                    
-                # Read RGB
-                rgb_frame = vr_rgb[idx].asnumpy() # H, W, 3 (uint8)
-                # Read Depth
-                depth_frame = vr_depth[idx].asnumpy() # H, W, C or H, W
-                
-                # Preprocess for Torch
-                # RGB: 0-255 -> 0-1, Permute to C, H, W
-                # ToTensor handles /255 but here we do manual for clarity/control over numpy
-                rgb_t = torch.from_numpy(rgb_frame).float().permute(2, 0, 1) / 255.0
-                
-                # Depth: Handle various formats (Gray8, Gray16, RGB-encoding)
-                if depth_frame.ndim == 3:
-                     # If it looks like RGB depth (grayscale repeated), take mean
-                     depth_mono = depth_frame.mean(axis=2)
-                else:
-                     depth_mono = depth_frame
-                
-                depth_t = torch.from_numpy(depth_mono).float()
-                # Normalize if not 0-1
-                if depth_t.max() > 1.0:
-                    depth_t = depth_t / 255.0
-                
-
-                # Clamp and apply the same gamma curve used by the render path:
-                # 1 - (1 - depth) ** gamma
-                try:
-                    gamma_f = float(gamma)
-                except Exception:
-                    gamma_f = 1.0
-                depth_t = torch.clamp(depth_t, 0.0, 1.0)
-                if gamma_f != 1.0:
-                    depth_t = 1.0 - torch.pow((1.0 - depth_t), gamma_f)
-
-                # Format: 1, C, H, W
-                depth_t = depth_t.unsqueeze(0).unsqueeze(0) 
-                rgb_b = rgb_t.unsqueeze(0) 
-                
-                # Predict
-                res = estimator.predict(rgb_b, depth_t)
-                estimates.extend(res)
-                
-            if not estimates:
-                return fallback_value, fallback_value
-                
-            avg_val = sum(estimates) / len(estimates)
-            # Using Max as 'Peak' estimate
-            peak_val = max(estimates)
-            
-            logger.info(f"Auto-Converge Result: Avg={avg_val:.3f}, Peak={peak_val:.3f}")
-            return avg_val, peak_val
-
-        except Exception as e:
-            logger.error(f"Auto convergence determination failed: {e}", exc_info=True)
-            return fallback_value, fallback_value
-
     def _auto_save_current_sidecar(self):
         """
         Saves the current GUI values to the sidecar file without user interaction.
@@ -1366,14 +657,7 @@ class SplatterGUI(ThemedTk):
         if r_val is None:
             r_val = self._safe_float(self.auto_border_R_var)
 
-        w = max(l_val, r_val)
-        if w > 0:
-            if l_val >= r_val:
-                b = (r_val / l_val) - 1.0
-            else:
-                b = 1.0 - (l_val / r_val)
-        else:
-            b = 0.0
+        w, b = BorderScanner.sync_sliders_to_auto_borders(l_val, r_val)
 
         self.border_width_var.set(f"{w:.2f}")
         self.border_bias_var.set(f"{b:.2f}")
@@ -1482,160 +766,54 @@ class SplatterGUI(ThemedTk):
         if not depth_path or not os.path.exists(depth_path):
             return
 
-        try:
-            vr = VideoReader(depth_path, ctx=cpu(0))
-            total_frames = len(vr)
-            if total_frames == 0:
-                return
+        # Determine current settings
+        conv = self._safe_float(self.zero_disparity_anchor_var, 0.5)
+        max_disp = self._safe_float(self.max_disp_var, 20.0)
+        gamma = self._safe_float(self.depth_gamma_var, 1.0)
 
-            # Show scanning status
-            old_status = self.status_label.cget("text")
-            self.status_label.config(
-                text=f"Scanning borders for {os.path.basename(depth_path)}..."
-            )
+        # Show scanning status and keep track of old status to revert later
+        old_status = self.status_label.cget("text")
+
+        def status_update(text):
+            self.status_label.config(text=text)
             self.update_idletasks()
 
-            step = 5
-            max_L = 0.0
-            max_R = 0.0
+        # Call the core scanner
+        scan_result = self.border_scanner.scan_current_clip(
+            depth_path=depth_path,
+            conv=conv,
+            max_disp=max_disp,
+            gamma=gamma,
+            stop_event=self.stop_event,
+            status_callback=status_update,
+        )
 
-            conv = self._safe_float(self.zero_disparity_anchor_var, 0.5)
-            max_disp = self._safe_float(self.max_disp_var, 20.0)
+        if scan_result:
+            max_L, max_R = scan_result
+            self.auto_border_L_var.set(str(max_L))
+            self.auto_border_R_var.set(str(max_R))
 
-            gamma = self._safe_float(self.depth_gamma_var, 1.0)
+            # Briefly show "Scan complete" then revert
+            self.after(2000, lambda: self.status_label.config(text=old_status))
+            return max_L, max_R
 
-            tv_disp_comp = 1.0
-            try:
-                _info = get_video_stream_info(depth_path)
-                if _infer_depth_bit_depth(_info) > 8 and str((_info or {}).get("color_range", "unknown")).lower() == "tv":
-                    tv_disp_comp = 1.0 / (DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM)
-            except Exception:
-                tv_disp_comp = 1.0
+        return None
 
-
-            for i in range(0, total_frames, step):
-                if self.stop_event.is_set():
-                    break
-
-                frame_raw = vr[i].asnumpy()
-                if frame_raw.ndim == 3:
-                    # Simple average for grayscale
-                    frame = frame_raw.mean(axis=2)
-                else:
-                    frame = frame_raw
-
-                # Sample 5px wide at each edge
-                L_sample = frame[:, :5]
-                R_sample = frame[:, -5:]
-
-                # 99th percentile to ignore noise
-                d_L = np.percentile(L_sample, 99) / 255.0
-                d_R = np.percentile(R_sample, 99) / 255.0
-
-                # Apply the same gamma curve used by the render path
-                if gamma and float(gamma) != 1.0:
-                    d_L = float(np.clip(d_L, 0.0, 1.0))
-                    d_R = float(np.clip(d_R, 0.0, 1.0))
-                    d_L = 1.0 - (1.0 - d_L) ** float(gamma)
-                    d_R = 1.0 - (1.0 - d_R) ** float(gamma)
-
-                # Scaling matches Auto Basic but localized to depth
-                b_L = max(0.0, (d_L - conv) * 2.0 * (max_disp / 20.0) * tv_disp_comp)
-                b_R = max(0.0, (d_R - conv) * 2.0 * (max_disp / 20.0) * tv_disp_comp)
-
-                max_L = max(max_L, b_L)
-                max_R = max(max_R, b_R)
-
-            max_L = min(5.0, round(float(max_L), 3))
-            max_R = min(5.0, round(float(max_R), 3))
-
-            self.auto_border_L_var.set(str(max_L))
-            self.auto_border_R_var.set(str(max_R))
-
-            logger.info(
-                f"Border scan complete: L={max_L}, R={max_R} (Conv={conv:.2f}, Disp={max_disp:.1f})"
-            )
-
-            self.status_label.config(text=f"Scan complete: L={max_L}%, R={max_R}%")
-            # Wait a bit so user can see the result before status might be clobbered
-            self.after(2000, lambda: self.status_label.config(text=old_status))
-
-            return max_L, max_R
-
-        except Exception as e:
-            logger.error(f"Border scan failed: {e}")
-            self.status_label.config(text="Border scan failed.")
-            return None
-
-
-    def _scan_borders_for_depth_path(
-        self,
-        depth_map_path: str,
-        conv: float,
-        max_disp: float,
-        gamma: float = 1.0,
-    ) -> Optional[Tuple[float, float]]:
-        """Thread-safe helper for AUTO-PASS: scans a depth-map video and returns (L, R) border %."""
-        try:
-            vr_depth = VideoReader(depth_map_path, ctx=cpu(0))
-            total_frames = len(vr_depth)
-            if total_frames <= 0:
-                return None
-
-            step = 5
-            max_L = 0.0
-            max_R = 0.0
-
-            gamma_f = 1.0
-            try:
-                gamma_f = float(gamma)
-            except Exception:
-                gamma_f = 1.0
-
-
-            tv_disp_comp = 1.0
-            try:
-                _info = get_video_stream_info(depth_map_path)
-                if _infer_depth_bit_depth(_info) > 8 and str((_info or {}).get("color_range", "unknown")).lower() == "tv":
-                    tv_disp_comp = 1.0 / (DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM)
-            except Exception:
-                tv_disp_comp = 1.0
-
-            for i in range(0, total_frames, step):
-                if hasattr(self, "stop_event") and self.stop_event and self.stop_event.is_set():
-                    break
-
-                frame_raw = vr_depth[i].asnumpy()
-                if frame_raw.ndim == 3:
-                    frame = frame_raw.mean(axis=2)
-                else:
-                    frame = frame_raw
-
-                L_sample = frame[:, :5]
-                R_sample = frame[:, -5:]
-
-                d_L = np.percentile(L_sample, 99) / 255.0
-                d_R = np.percentile(R_sample, 99) / 255.0
-
-                if gamma_f != 1.0:
-                    d_L = float(np.clip(d_L, 0.0, 1.0))
-                    d_R = float(np.clip(d_R, 0.0, 1.0))
-                    d_L = 1.0 - (1.0 - d_L) ** gamma_f
-                    d_R = 1.0 - (1.0 - d_R) ** gamma_f
-
-                b_L = max(0.0, (d_L - conv) * 2.0 * (max_disp / 20.0) * tv_disp_comp)
-                b_R = max(0.0, (d_R - conv) * 2.0 * (max_disp / 20.0) * tv_disp_comp)
-
-                max_L = max(max_L, b_L)
-                max_R = max(max_R, b_R)
-
-            max_L = min(5.0, round(float(max_L), 3))
-            max_R = min(5.0, round(float(max_R), 3))
-            return max_L, max_R
-
-        except Exception as e:
-            logger.error(f"Border scan failed: {e}")
-            return None
+    def _scan_borders_for_depth_path(
+        self,
+        depth_map_path: str,
+        conv: float,
+        max_disp: float,
+        gamma: float = 1.0,
+    ) -> Optional[Tuple[float, float]]:
+        """Thread-safe helper for AUTO-PASS: scans a depth-map video and returns (L, R) border %."""
+        return self.border_scanner.scan_depth_path(
+            depth_map_path=depth_map_path,
+            conv=conv,
+            max_disp=max_disp,
+            gamma=gamma,
+            stop_event=getattr(self, "stop_event", None),
+        )
 
     def _scan_depth_map_folders(self):
         """Scans the Input Depth Maps folder for subfolders containing *_depth.mp4 files."""
@@ -3368,1035 +2546,140 @@ class SplatterGUI(ThemedTk):
         except ValueError:
             pass  # Invalid current value
 
-    def depthSplatting(
-        self: "SplatterGUI",
-        input_video_reader: VideoReader,
-        depth_map_reader: VideoReader,
-        total_frames_to_process: int,
-        processed_fps: float,
-        output_video_path_base: str,
-        target_output_height: int,
-        target_output_width: int,
-        max_disp: float,
-        process_length: int,
-        batch_size: int,
-        dual_output: bool,
-        zero_disparity_anchor_val: float,
-        video_stream_info: Optional[dict],
-        input_bias: Optional[float],
-        assume_raw_input: bool,
-        global_depth_min: float,
-        global_depth_max: float,
-        depth_stream_info: Optional[dict],
-        user_output_crf: Optional[int] = None,
-        is_low_res_task: bool = False,
-        depth_gamma: float = 1.0,
-        depth_dilate_size_x: float = 0.0,
-        depth_dilate_size_y: float = 0.0,
-        depth_blur_size_x: float = 0.0,
-        depth_blur_size_y: float = 0.0,
-        depth_dilate_left: float = 0.0,
-        depth_blur_left: float = 0.0,
-    ):
-        logger.debug("==> Initializing ForwardWarpStereo module")
-        stereo_projector = ForwardWarpStereo(occlu_map=True).cuda()
+    # _determine_auto_convergence definition removed (moved to earlier in file)
 
-        num_frames = total_frames_to_process
-        height, width = target_output_height, target_output_width
-        os.makedirs(os.path.dirname(output_video_path_base), exist_ok=True)
+    def exit_app(self):
+        """Handles application exit, including stopping the processing thread."""
+        self._save_config()
+        self.stop_event.set()
+        if self.processing_thread and self.processing_thread.is_alive():
+            logger.info("==> Waiting for processing thread to finish...")
+            # --- NEW: Cleanup previewer resources ---
+            if hasattr(self, "previewer"):
+                self.previewer.cleanup()
+            self.processing_thread.join(timeout=5.0)
+            if self.processing_thread.is_alive():
+                logger.debug("==> Thread did not terminate gracefully within timeout.")
+        release_cuda_memory()
+        self.destroy()
 
-        # --- Determine output grid dimensions and final path ---
-        grid_height, grid_width = (
-            (height, width * 2) if dual_output else (height * 2, width * 2)
-        )
-        suffix = "_splatted2" if dual_output else "_splatted4"
-        res_suffix = f"_{width}"
-        final_output_video_path = (
-            f"{os.path.splitext(output_video_path_base)[0]}{res_suffix}{suffix}.mp4"
-        )
+    def _find_preview_sources_callback(self) -> list:
+        """
+        Callback for VideoPreviewer. Scans for matching source video and depth map pairs.
+        Handles both folder (batch) and file (single) input modes.
+        """
+        source_path = self.input_source_clips_var.get()
+        depth_raw_path = self.input_depth_maps_var.get()
 
-        # --- DEBUG: Log ffprobe-derived color metadata for this pass (Hi/Lo parity checks) ---
-        task_name = "LowRes" if is_low_res_task else "HiRes"
-        try:
-            logger.info(
-                f"[COLOR_META][{task_name}] input ffprobe: "
-                f"pix_fmt={video_stream_info.get('pix_fmt') if video_stream_info else None}, "
-                f"range={video_stream_info.get('color_range') if video_stream_info else None}, "
-                f"primaries={video_stream_info.get('color_primaries') if video_stream_info else None}, "
-                f"trc={video_stream_info.get('transfer_characteristics') if video_stream_info else None}, "
-                f"matrix={video_stream_info.get('color_space') if video_stream_info else None}"
-            )
-        except Exception:
-            pass
-        # --- END DEBUG ---
+        if not source_path or not depth_raw_path:
+            logger.warning("Preview Scan Failed: Source or depth path is empty.")
+            return []
 
-        # --- Start FFmpeg pipe process ---
-        is_test_mode = bool(
-            (getattr(self, "map_test_var", None) and self.map_test_var.get())
-            or (getattr(self, "splat_test_var", None) and self.splat_test_var.get())
-        )
-        ffmpeg_process = None
-        if is_test_mode:
-            # In test mode we don't want to create/overwrite any output video files.
+        # ------------------------------------------------------------
+        # 1) SINGLE-FILE MODE (both are actual files)
+        # ------------------------------------------------------------
+        is_source_file = os.path.isfile(source_path)
+        is_depth_file = os.path.isfile(depth_raw_path)
+
+        if is_source_file and is_depth_file:
             logger.debug(
-                "Test mode active: skipping FFmpeg encoding (no output video will be written)."
-            )
-        else:
-            # --- Start FFmpeg pipe process ---
-            # Apply output color tag mode (metadata-only; does not change splat math)
-            encode_stream_info = (
-                dict(video_stream_info) if isinstance(video_stream_info, dict) else {}
+                f"Preview Scan: Single file mode detected. "
+                f"Source: {source_path}, Depth: {depth_raw_path}"
             )
-            try:
-                _ct_mode = (
-                    self.color_tags_mode_var.get()
-                    if hasattr(self, "color_tags_mode_var")
-                    else "Auto"
-                )
-            except Exception:
-                _ct_mode = "Auto"
-
-            def _ct_defaults(d: dict) -> dict:
-                d.setdefault("color_primaries", "bt709")
-                d.setdefault("transfer_characteristics", "bt709")
-                d.setdefault("color_space", "bt709")
-                # Only metadata; used for tagging if missing. Preserve source if present.
-                d.setdefault("color_range", "tv")
-                return d
-
-            if _ct_mode == "Auto":
-                encode_stream_info = _ct_defaults(encode_stream_info)
-            elif _ct_mode == "BT.709 L":
-                encode_stream_info.update(
-                    {
-                        "color_primaries": "bt709",
-                        "transfer_characteristics": "bt709",
-                        "color_space": "bt709",
-                        "color_range": "tv",
-                    }
-                )
-            elif _ct_mode == "BT.709 F":
-                encode_stream_info.update(
-                    {
-                        "color_primaries": "bt709",
-                        "transfer_characteristics": "bt709",
-                        "color_space": "bt709",
-                        "color_range": "pc",
-                    }
-                )
-            elif _ct_mode == "BT.2020 PQ":
-                encode_stream_info.update(
-                    {
-                        "color_primaries": "bt2020",
-                        "transfer_characteristics": "smpte2084",
-                        "color_space": "bt2020nc",
-                        "color_range": "tv",
-                    }
-                )
-            elif _ct_mode == "BT.2020 HLG":
-                encode_stream_info.update(
-                    {
-                        "color_primaries": "bt2020",
-                        "transfer_characteristics": "arib-std-b67",
-                        "color_space": "bt2020nc",
-                        "color_range": "tv",
-                    }
-                )
-            else:
-                # Unknown/legacy value; keep current behavior
-                encode_stream_info = _ct_defaults(encode_stream_info)
-
-            ffmpeg_process = start_ffmpeg_pipe_process(
-                content_width=grid_width,
-                content_height=grid_height,
-                final_output_mp4_path=final_output_video_path,
-                fps=processed_fps,
-                video_stream_info=encode_stream_info,
-                user_output_crf=user_output_crf,
-                output_format_str="splatted_grid",  # Pass a placeholder for the new argument
-                debug_label=task_name,
+            return [
+                {
+                    "source_video": source_path,
+                    "depth_map": depth_raw_path,
+                }
+            ]
+
+        # ------------------------------------------------------------
+        # 2) FOLDER / BATCH MODE
+        # ------------------------------------------------------------
+        if not os.path.isdir(source_path) or not os.path.isdir(depth_raw_path):
+            logger.error(
+                "Preview Scan Failed: Inputs must either be two files or two valid directories."
             )
-            if ffmpeg_process is None:
-                logger.error("Failed to start FFmpeg pipe. Aborting splatting task.")
-                return False
+            return []
 
-        # --- DEBUG: Capture and compare encoding flags between HiRes and LowRes passes ---
-        try:
-            flags = getattr(ffmpeg_process, "sc_encode_flags", None)
-            if flags:
-                if not hasattr(self, "_sc_color_encode_flags"):
-                    self._sc_color_encode_flags = {}
-                subset_keys = [
-                    "enc_codec",
-                    "enc_pix_fmt",
-                    "enc_profile",
-                    "enc_color_primaries",
-                    "enc_color_trc",
-                    "enc_colorspace",
-                    "quality_mode",
-                    "quality_value",
-                ]
-                subset = {k: flags.get(k) for k in subset_keys}
-                self._sc_color_encode_flags[task_name] = subset
-
-                other_name = "HiRes" if task_name == "LowRes" else "LowRes"
-                if other_name in self._sc_color_encode_flags:
-                    other = self._sc_color_encode_flags[other_name]
-                    diffs = {
-                        k: (other.get(k), subset.get(k))
-                        for k in subset_keys
-                        if other.get(k) != subset.get(k)
-                    }
-                    if diffs:
-                        logger.warning(
-                            f"[COLOR_META] Encoding flags differ ({other_name} vs {task_name}): {diffs}"
-                        )
-                    else:
-                        logger.info(
-                            f"[COLOR_META] Encoding flags match between {other_name} and {task_name}."
-                        )
-        except Exception:
-            pass
-        # --- END DEBUG ---
-
-        # --- Determine max_expected_raw_value for consistent Gamma ---
-        max_expected_raw_value = 1.0
-        depth_pix_fmt = depth_stream_info.get("pix_fmt") if depth_stream_info else None
-        depth_profile = depth_stream_info.get("profile") if depth_stream_info else None
-        is_source_10bit = False
-        if depth_pix_fmt:
-            if (
-                "10" in depth_pix_fmt
-                or "gray10" in depth_pix_fmt
-                or "12" in depth_pix_fmt
-                or (depth_profile and "main10" in depth_profile)
-            ):
-                is_source_10bit = True
-        if is_source_10bit:
-            max_expected_raw_value = 1023.0
-        elif depth_pix_fmt and (
-            "8" in depth_pix_fmt or depth_pix_fmt in ["yuv420p", "yuv422p", "yuv444p"]
-        ):
-            max_expected_raw_value = 255.0
-        elif isinstance(depth_pix_fmt, str) and "float" in depth_pix_fmt:
-            max_expected_raw_value = 1.0
-        logger.debug(
-            f"Determined max_expected_raw_value: {max_expected_raw_value:.1f} (Source: {depth_pix_fmt}/{depth_profile})"
-        )
+        source_folder = source_path
+        base_depth_folder = depth_raw_path
 
-        frame_count = 0
-        encoding_successful = True  # Assume success unless an error occurs
+        # Collect all source videos
+        video_extensions = ("*.mp4", "*.avi", "*.mov", "*.mkv")
+        source_videos = []
+        for ext in video_extensions:
+            source_videos.extend(glob.glob(os.path.join(source_folder, ext)))
 
-        try:
-            # Map/Splat Test: render the *same frame* the preview is showing.
-            test_target_frame_idx = None
-            try:
-                if (
-                    getattr(self, "map_test_var", None) and self.map_test_var.get()
-                ) or (
-                    getattr(self, "splat_test_var", None) and self.splat_test_var.get()
-                ):
-                    if (
-                        hasattr(self, "previewer")
-                        and getattr(self.previewer, "last_loaded_frame_index", None)
-                        is not None
-                    ):
-                        test_target_frame_idx = int(
-                            self.previewer.last_loaded_frame_index
-                        )
-                        test_target_frame_idx = max(
-                            0, min(int(num_frames) - 1, test_target_frame_idx)
-                        )
-            except Exception:
-                test_target_frame_idx = None
+        if not source_videos:
+            logger.warning(f"No source videos found in folder: {source_folder}")
+            return []
 
-            frame_index_iter = (
-                [test_target_frame_idx]
-                if test_target_frame_idx is not None
-                else range(0, num_frames, batch_size)
-            )
-            for i in frame_index_iter:
-                t_start_batch = time.perf_counter()  # <--- TIMER START: Total Batch
-                if self.stop_event.is_set() or (
-                    ffmpeg_process is not None and ffmpeg_process.poll() is not None
-                ):
-                    if ffmpeg_process is not None and ffmpeg_process.poll() is not None:
-                        logger.error(
-                            "FFmpeg process terminated unexpectedly. Stopping frame processing."
-                        )
-                    else:
-                        logger.warning(
-                            "Stop event received. Terminating FFmpeg process."
-                        )
-                    encoding_successful = False
-                    break
+        video_source_list = []
 
-                # --- TIMER 1: Video/Depth I/O (Disk/Decode/Resize) ---
-                t_start_io = time.perf_counter()
+        # ------------------------------------------------------------
+        # 2A) MULTI-MAP PREVIEW: search all map subfolders
+        # ------------------------------------------------------------
+        if self.multi_map_var.get():
+            depth_candidate_folders = []
 
-                current_frame_indices = (
-                    [i]
-                    if test_target_frame_idx is not None
-                    else list(range(i, min(i + batch_size, num_frames)))
+            # Treat each subdirectory (except 'sidecars') as a map folder
+            try:
+                for entry in os.listdir(base_depth_folder):
+                    full_sub = os.path.join(base_depth_folder, entry)
+                    if os.path.isdir(full_sub) and entry.lower() != "sidecars":
+                        depth_candidate_folders.append(full_sub)
+            except FileNotFoundError:
+                logger.error(
+                    f"Preview Scan Failed: Depth folder not found: {base_depth_folder}"
                 )
-                if not current_frame_indices:
-                    break
-
-                batch_frames_numpy = input_video_reader.get_batch(
-                    current_frame_indices
-                ).asnumpy()
-                # This often resolves issues where Decord/FFmpeg loses the internal stream position
-                try:
-                    # Seek to the first frame of the current batch
-                    depth_map_reader.seek(current_frame_indices[0])
-                    # Then read the full batch from that position
-                    batch_depth_numpy_raw = depth_map_reader.get_batch(
-                        current_frame_indices
-                    ).asnumpy()
-                except Exception as e:
-                    logger.error(
-                        f"Error seeking/reading depth map batch starting at index {i}: {e}. Falling back to a potentially blank read."
-                    )
-                    batch_depth_numpy_raw = depth_map_reader.get_batch(
-                        current_frame_indices
-                    ).asnumpy()
-                t_end_io = time.perf_counter()
+                return []
 
-                file_frame_idx = current_frame_indices[0]
-                task_name = "LowRes" if is_low_res_task else "HiRes"
+            if not depth_candidate_folders:
+                logger.warning(
+                    f"Preview Scan: No map subfolders found in Multi-Map base folder: {base_depth_folder}"
+                )
 
-                if batch_depth_numpy_raw.min() == batch_depth_numpy_raw.max() == 0:
-                    logger.warning(
-                        f"Depth map batch starting at index {i} is entirely blank/zero after read. **Seeking failed to resolve.**"
-                    )
+            for video_path in sorted(source_videos):
+                base_name = os.path.splitext(os.path.basename(video_path))[0]
+                matched = False
 
-                if batch_depth_numpy_raw.min() == batch_depth_numpy_raw.max():
-                    logger.warning(
-                        f"Depth map batch starting at index {i} is entirely uniform/flat after read. Min/Max: {batch_depth_numpy_raw.min():.2f}"
-                    )
+                for dpath in depth_candidate_folders:
+                    mp4 = os.path.join(dpath, f"{base_name}_depth.mp4")
+                    npz = os.path.join(dpath, f"{base_name}_depth.npz")
 
-                # Use the FIRST frame index for the file name (e.g., 00000.png)
-                file_frame_idx = current_frame_indices[0]
-
-                # self._save_debug_numpy(batch_depth_numpy_raw, "01_RAW_INPUT", i, file_frame_idx, task_name)
-                # --- TIMER 2: CPU Pre-processing (Dilate, Blur, Grayscale, Gamma, Min/Max Calc) ---
-                # --- 8-bit resize parity (Map Test Preview vs Render) ---
-                # For 8-bit depth videos, Decord's decode-time scaling can differ by ~1-2 code values vs OpenCV.
-                # To match the Preview path, decode at native res (see load_pre_rendered_depth) and resize here
-                # BEFORE preprocessing using the same OpenCV interpolation policy.
-                _bd = (
-                    _infer_depth_bit_depth(depth_stream_info)
-                    if depth_stream_info
-                    else 8
-                )
-                if _bd <= 8:
-                    # Determine pre-processing resolution (matches preview/render ordering).
-                    _target_w = int(batch_frames_numpy.shape[2])
-                    _target_h = int(batch_frames_numpy.shape[1])
-                    if is_low_res_task:
-                        _skip_lr = bool(
-                            getattr(self, "skip_lowres_preproc_var", None) is not None
-                            and self.skip_lowres_preproc_var.get()
-                        )
-                        if not _skip_lr:
-                            # For Low-Res tasks we want to apply preprocessing at the incoming depth-reader resolution
-                            # (typically clip-sized when match_resolution_to_target is enabled), then downscale after preprocessing.
-                            _target_w = int(batch_depth_numpy_raw.shape[2])
-                            _target_h = int(batch_depth_numpy_raw.shape[1])
-
-                    # Reduce to a single channel (depth maps are grayscale-encoded).
-                    if batch_depth_numpy_raw.ndim == 4:
-                        if batch_depth_numpy_raw.shape[-1] == 3:
-                            _depth_1c = batch_depth_numpy_raw[..., 0]
-                        elif batch_depth_numpy_raw.shape[-1] == 1:
-                            _depth_1c = batch_depth_numpy_raw[..., 0]
-                        else:
-                            _depth_1c = batch_depth_numpy_raw.squeeze(-1)
-                    else:
-                        _depth_1c = batch_depth_numpy_raw
-
-                    if (_depth_1c.shape[1] != _target_h) or (
-                        _depth_1c.shape[2] != _target_w
-                    ):
-                        _interp = (
-                            cv2.INTER_LINEAR
-                            if (
-                                _target_w > _depth_1c.shape[2]
-                                or _target_h > _depth_1c.shape[1]
-                            )
-                            else cv2.INTER_AREA
+                    if os.path.exists(mp4):
+                        video_source_list.append(
+                            {
+                                "source_video": video_path,
+                                "depth_map": mp4,
+                            }
                         )
-                        _resized = np.empty(
-                            (_depth_1c.shape[0], _target_h, _target_w),
-                            dtype=_depth_1c.dtype,
+                        matched = True
+                        break
+                    elif os.path.exists(npz):
+                        video_source_list.append(
+                            {
+                                "source_video": video_path,
+                                "depth_map": npz,
+                            }
                         )
-                        for _di in range(_depth_1c.shape[0]):
-                            _resized[_di] = cv2.resize(
-                                _depth_1c[_di],
-                                (_target_w, _target_h),
-                                interpolation=_interp,
-                            )
-                        batch_depth_numpy_raw = _resized[..., None]
-                    else:
-                        batch_depth_numpy_raw = _depth_1c[..., None]
-
-                t_start_preproc = time.perf_counter()
-
-                batch_depth_numpy = self._process_depth_batch(
-                    batch_depth_numpy_raw=batch_depth_numpy_raw,
-                    depth_stream_info=depth_stream_info,
-                    depth_gamma=depth_gamma,
-                    depth_dilate_size_x=depth_dilate_size_x,
-                    depth_dilate_size_y=depth_dilate_size_y,
-                    depth_blur_size_x=depth_blur_size_x,
-                    depth_blur_size_y=depth_blur_size_y,
-                    depth_dilate_left=depth_dilate_left,
-                    depth_blur_left=depth_blur_left,
-                    is_low_res_task=is_low_res_task,
-                    max_raw_value=max_expected_raw_value,
-                    global_depth_min=global_depth_min,
-                    global_depth_max=global_depth_max,
-                    debug_batch_index=i,
-                    debug_frame_index=file_frame_idx,
-                    debug_task_name=task_name,
-                )
+                        matched = True
+                        break
 
-                # If the depth batch resolution differs from the source frames,
-                # resize AFTER preprocessing so dilation/erosion/blur parity is preserved.
-                if (batch_depth_numpy.shape[1] != batch_frames_numpy.shape[1]) or (
-                    batch_depth_numpy.shape[2] != batch_frames_numpy.shape[2]
-                ):
-                    target_h = batch_frames_numpy.shape[1]
-                    target_w = batch_frames_numpy.shape[2]
-                    # INTER_AREA seems best for downscaling depth maps (minimizes aliasing/jaggies-cv2.INTER_LANCZOS4 works too).
-                    if (
-                        target_w < batch_depth_numpy.shape[2]
-                        and target_h < batch_depth_numpy.shape[1]
-                    ):
-                        interp = cv2.INTER_AREA
-                    else:
-                        interp = cv2.INTER_LINEAR
-                    resized_depth = np.empty(
-                        (batch_depth_numpy.shape[0], target_h, target_w),
-                        dtype=batch_depth_numpy.dtype,
+                if not matched:
+                    logger.debug(
+                        f"Preview Scan: No depth map found in any map folder for '{base_name}'."
                     )
-                    for _di in range(batch_depth_numpy.shape[0]):
-                        resized_depth[_di] = cv2.resize(
-                            batch_depth_numpy[_di],
-                            (target_w, target_h),
-                            interpolation=interp,
-                        )
-                    batch_depth_numpy = resized_depth
 
-                # self._save_debug_numpy(batch_depth_numpy, "02_PROCESSED_PRE_NORM", i, file_frame_idx, task_name)
+        # ------------------------------------------------------------
+        # 2B) NORMAL MODE PREVIEW: single depth folder
+        # ------------------------------------------------------------
+        else:
+            depth_folder = base_depth_folder
 
-                batch_frames_float = batch_frames_numpy.astype("float32") / 255.0
-
-                # 1. Normalize based on mode
-                if assume_raw_input:
-                    if global_depth_max > 1.0:
-                        batch_depth_normalized = batch_depth_numpy / global_depth_max
-                    else:
-                        batch_depth_normalized = batch_depth_numpy.copy()
-                else:
-                    depth_range = global_depth_max - global_depth_min
-                    if depth_range > 1e-5:
-                        batch_depth_normalized = (
-                            batch_depth_numpy - global_depth_min
-                        ) / depth_range
-                    else:
-                        batch_depth_normalized = np.full_like(
-                            batch_depth_numpy,
-                            fill_value=zero_disparity_anchor_val,
-                            dtype=np.float32,
-                        )
-                        logger.warning(
-                            f"Normalization collapsed to zero range ({global_depth_min:.4f} - {global_depth_max:.4f})."
-                        )
-
-                # 2. Unified Inverted Gamma (Matches Preview Math exactly)
-                if round(float(depth_gamma), 2) != 1.0:
-                    # Math: 1.0 - (1.0 - depth)^gamma
-                    batch_depth_normalized = 1.0 - np.power(
-                        1.0 - np.clip(batch_depth_normalized, 0, 1), depth_gamma
-                    )
-                    # (Normal) Depth gamma applied in normalized space; no warning log.
-
-                batch_depth_normalized = np.clip(batch_depth_normalized, 0, 1)
-
-                # --- START OF DIAGNOSTIC SYNC BLOCK ---
-                # Keeps depth visualization at 16-bit to prevent banding in the Render output
-                batch_depth_vis_list = []
-                for d_frame in batch_depth_normalized:
-                    # We keep this as float32 for the assembly to prevent 8-bit quantization
-                    vis_gray_3ch = np.stack([d_frame] * 3, axis=-1)
-                    # Render-time: track measured per-clip max Total(D+P) (percent of width)
-                    # (Uses the same depth frames already being processed; no extra decode pass.)
-                    try:
-                        sig = getattr(self, "_dp_total_true_active_sig", None)
-                        _track_true = (
-                            getattr(self, "track_dp_total_true_on_render_var", None) is not None
-                            and self.track_dp_total_true_on_render_var.get()
-                        )
-                        if sig and _track_true:
-                            # Exclude hard holes (0 and 1) from min/max to avoid bogus extremes
-                            _min_arr = d_frame[d_frame > 0.001]
-                            _max_arr = d_frame[d_frame < 0.999]
-                            if _min_arr.size and _max_arr.size:
-                                dmin = float(_min_arr.min())
-                                dmax = float(_max_arr.max())
-                                # True max Total(D+P) (percent of width) for this frame
-                                # Uses full-frame min/max (holes excluded) with the same math as the preview overlay.
-                                _scale = 2.0 * float(max_disp) * 100.0 / float(d_frame.shape[1])
-                                _min_pct = (dmin - float(zero_disparity_anchor_val)) * _scale
-                                _max_pct = (dmax - float(zero_disparity_anchor_val)) * _scale
-                                _depth_pct = max(0.0, -_min_pct)
-                                _pop_pct = _max_pct
-                                total_pct = float(_depth_pct) + float(_pop_pct)
-
-                                cur = getattr(self, "_dp_total_true_active_val", None)
-                                if cur is None or total_pct > cur:
-                                    self._dp_total_true_active_val = float(total_pct)
-                                    try:
-                                        self._dp_total_true_cache[sig] = float(
-                                            self._dp_total_true_active_val
-                                        )
-                                    except Exception:
-                                        pass
-                    except Exception:
-                        pass
-                    batch_depth_vis_list.append(vis_gray_3ch.astype("float32"))
-
-                batch_depth_vis = np.stack(batch_depth_vis_list, axis=0)
-                # --- END OF DIAGNOSTIC SYNC BLOCK ---
-
-                t_end_preproc = time.perf_counter()
-                # --- END TIMER 2 ---
-
-                # --- TIMER 3: HtoD Transfer (CPU to GPU) ---
-                t_start_transfer_HtoD = time.perf_counter()
-
-                left_video_tensor = (
-                    torch.from_numpy(batch_frames_numpy)
-                    .permute(0, 3, 1, 2)
-                    .float()
-                    .cuda()
-                    / 255.0
-                )
-                disp_map_tensor = (
-                    torch.from_numpy(batch_depth_normalized).unsqueeze(1).float().cuda()
-                )
-                disp_map_tensor = (disp_map_tensor - zero_disparity_anchor_val) * 2.0
-                disp_map_tensor = disp_map_tensor * max_disp
-
-                torch.cuda.synchronize()  # Force synchronization before compute
-                t_end_transfer_HtoD = time.perf_counter()
-                # --- END TIMER 3 ---
-
-                # --- TIMER 4: GPU Compute (Core Splatting) ---
-                t_start_compute = time.perf_counter()
-
-                with torch.no_grad():
-                    right_video_tensor_raw, occlusion_mask_tensor = stereo_projector(
-                        left_video_tensor, disp_map_tensor
-                    )
-                    right_video_tensor = right_video_tensor_raw
-
-                right_video_numpy = right_video_tensor.cpu().permute(0, 2, 3, 1).numpy()
-                occlusion_mask_numpy = (
-                    occlusion_mask_tensor.cpu()
-                    .permute(0, 2, 3, 1)
-                    .numpy()
-                    .repeat(3, axis=-1)
-                )
-
-                t_end_transfer_DtoH = time.perf_counter()
-                # --- END TIMER 5 ---
-
-                # --- TIMER 6: FFmpeg Write (Blocking I/O) ---
-                t_start_write = time.perf_counter()
-
-                # --- START OF DIAGNOSTIC TEST ASSEMBLY ---
-                test_generated_this_task = False
-                if self.map_test_var.get() or self.splat_test_var.get():
-                    for j in range(len(batch_frames_numpy)):
-                        # Capture logic
-                        clean_base = (
-                            os.path.splitext(output_video_path_base)[0]
-                            .replace("_splatted2", "")
-                            .replace("_splatted4", "")
-                        )
-                        base_img_path = f"{clean_base}_{width}"
-
-                        # Emergency pump to ensure the 100% scale update_preview finished
-                        for _ in range(25):
-                            self.update_idletasks()
-                            self.update()
-                            if self.previewer.pil_image_for_preview is not None:
-                                break
-                            time.sleep(0.1)
-
-                        if self.previewer.pil_image_for_preview is None:
-                            logger.error(
-                                "Diagnostic Capture Failed: Preview image is missing."
-                            )
-                            return False
-
-                        preview_pil = self.previewer.pil_image_for_preview.convert(
-                            "RGB"
-                        )
-                        # Resize preview to match RENDER resolution for SBS parity
-                        preview_pil = preview_pil.resize(
-                            (width, height), Image.Resampling.LANCZOS
-                        )
-                        preview_side = np.array(preview_pil).astype(np.float32) / 255.0
-
-                        if self.map_test_var.get():
-                            # Render depth visualization (apply the same visualization-only TV->full expansion for 10-bit, when tagged 'tv')
-                            render_map_vis = batch_depth_vis[j]
-                            try:
-                                if DEPTH_VIS_APPLY_TV_RANGE_EXPANSION_10BIT:
-                                    bd = (
-                                        int(_infer_depth_bit_depth(depth_stream_info))
-                                        if depth_stream_info
-                                        else 8
-                                    )
-                                    rng = str(
-                                        (depth_stream_info or {}).get("color_range")
-                                        or (depth_stream_info or {}).get("range")
-                                        or ""
-                                    ).lower()
-                                    if bd >= 10 and rng == "tv":
-                                        render_map_vis = (
-                                            render_map_vis - DEPTH_VIS_TV10_BLACK_NORM
-                                        ) / (
-                                            DEPTH_VIS_TV10_WHITE_NORM
-                                            - DEPTH_VIS_TV10_BLACK_NORM
-                                        )
-                            except Exception:
-                                pass
-
-                            # Save labeled PREVIEW / RENDER single images
-                            try:
-                                prev_u8 = (np.clip(preview_side, 0, 1) * 255).astype(
-                                    np.uint8
-                                )
-                                prev_bgr = cv2.cvtColor(prev_u8, cv2.COLOR_RGB2BGR)
-                                cv2.putText(
-                                    prev_bgr,
-                                    "PREVIEW",
-                                    (10, 35),
-                                    cv2.FONT_HERSHEY_SIMPLEX,
-                                    1.2,
-                                    (0, 0, 255),
-                                    2,
-                                )
-                                cv2.imwrite(
-                                    f"{base_img_path}_map_test_preview.png", prev_bgr
-                                )
-
-                                rend_u8 = (np.clip(render_map_vis, 0, 1) * 255).astype(
-                                    np.uint8
-                                )
-                                rend_bgr = cv2.cvtColor(rend_u8, cv2.COLOR_RGB2BGR)
-                                cv2.putText(
-                                    rend_bgr,
-                                    "RENDER",
-                                    (10, 35),
-                                    cv2.FONT_HERSHEY_SIMPLEX,
-                                    1.2,
-                                    (0, 0, 255),
-                                    2,
-                                )
-                                cv2.imwrite(
-                                    f"{base_img_path}_map_test_render.png", rend_bgr
-                                )
-                            except Exception:
-                                pass
-
-                            # SBS (kept for convenience)
-                            sbs_map = np.concatenate(
-                                [preview_side, render_map_vis], axis=1
-                            )
-                            sbs_map_uint8 = (np.clip(sbs_map, 0, 1) * 255).astype(
-                                np.uint8
-                            )
-                            sbs_map_bgr = cv2.cvtColor(sbs_map_uint8, cv2.COLOR_RGB2BGR)
-                            cv2.putText(
-                                sbs_map_bgr,
-                                "PREVIEW",
-                                (10, 35),
-                                cv2.FONT_HERSHEY_SIMPLEX,
-                                1.2,
-                                (0, 0, 255),
-                                2,
-                            )
-                            cv2.putText(
-                                sbs_map_bgr,
-                                "RENDER",
-                                (width + 10, 35),
-                                cv2.FONT_HERSHEY_SIMPLEX,
-                                1.2,
-                                (0, 0, 255),
-                                2,
-                            )
-                            cv2.imwrite(f"{base_img_path}_map_test.png", sbs_map_bgr)
-                            logger.info(
-                                f"Saved Map Test: {os.path.basename(base_img_path)}_map_test.png"
-                            )
-
-                        if self.splat_test_var.get():
-                            # Save labeled PREVIEW / RENDER single images
-                            try:
-                                prev_u8 = (np.clip(preview_side, 0, 1) * 255).astype(
-                                    np.uint8
-                                )
-                                prev_bgr = cv2.cvtColor(prev_u8, cv2.COLOR_RGB2BGR)
-                                cv2.putText(
-                                    prev_bgr,
-                                    "PREVIEW",
-                                    (10, 35),
-                                    cv2.FONT_HERSHEY_SIMPLEX,
-                                    1.2,
-                                    (0, 0, 255),
-                                    2,
-                                )
-                                cv2.imwrite(
-                                    f"{base_img_path}_splat_test_preview.png", prev_bgr
-                                )
-
-                                rend_u8 = (
-                                    np.clip(right_video_numpy[j], 0, 1) * 255
-                                ).astype(np.uint8)
-                                rend_bgr = cv2.cvtColor(rend_u8, cv2.COLOR_RGB2BGR)
-                                cv2.putText(
-                                    rend_bgr,
-                                    "RENDER",
-                                    (10, 35),
-                                    cv2.FONT_HERSHEY_SIMPLEX,
-                                    1.2,
-                                    (0, 0, 255),
-                                    2,
-                                )
-                                cv2.imwrite(
-                                    f"{base_img_path}_splat_test_render.png", rend_bgr
-                                )
-                            except Exception:
-                                pass
-
-                            # SBS (kept for convenience)
-                            sbs_splat = np.concatenate(
-                                [preview_side, right_video_numpy[j]], axis=1
-                            )
-                            sbs_splat_uint8 = (np.clip(sbs_splat, 0, 1) * 255).astype(
-                                np.uint8
-                            )
-                            sbs_splat_bgr = cv2.cvtColor(
-                                sbs_splat_uint8, cv2.COLOR_RGB2BGR
-                            )
-                            cv2.putText(
-                                sbs_splat_bgr,
-                                "PREVIEW",
-                                (10, 35),
-                                cv2.FONT_HERSHEY_SIMPLEX,
-                                1.2,
-                                (0, 0, 255),
-                                2,
-                            )
-                            cv2.putText(
-                                sbs_splat_bgr,
-                                "RENDER",
-                                (width + 10, 35),
-                                cv2.FONT_HERSHEY_SIMPLEX,
-                                1.2,
-                                (0, 0, 255),
-                                2,
-                            )
-                            cv2.imwrite(
-                                f"{base_img_path}_splat_test.png", sbs_splat_bgr
-                            )
-                            logger.info(
-                                f"Saved Splat Test: {os.path.basename(base_img_path)}_splat_test.png"
-                            )
-
-                        test_generated_this_task = True
-                        break  # Only process one frame for images
-
-                    if test_generated_this_task:
-                        return True  # Exit early: NO VIDEO encoding occurs
-
-                # --- STANDARD VIDEO ENCODING (Only reached if NO tests enabled) ---
-                for j in range(len(batch_frames_numpy)):
-                    if dual_output:
-                        video_grid = np.concatenate(
-                            [occlusion_mask_numpy[j], right_video_numpy[j]], axis=1
-                        )
-                    else:
-                        video_grid_top = np.concatenate(
-                            [batch_frames_float[j], batch_depth_vis[j]], axis=1
-                        )
-                        video_grid_bottom = np.concatenate(
-                            [occlusion_mask_numpy[j], right_video_numpy[j]], axis=1
-                        )
-                        video_grid = np.concatenate(
-                            [video_grid_top, video_grid_bottom], axis=0
-                        )
-
-                    video_grid_uint16 = (
-                        np.clip(video_grid, 0.0, 1.0) * 65535.0
-                    ).astype(np.uint16)
-                    ffmpeg_process.stdin.write(
-                        cv2.cvtColor(video_grid_uint16, cv2.COLOR_RGB2BGR).tobytes()
-                    )
-                    frame_count += 1
-                # --- END OF FRAME ASSEMBLY ---
-
-                t_end_write = time.perf_counter()
-                # --- END TIMER 6 ---
-
-                del (
-                    left_video_tensor,
-                    disp_map_tensor,
-                    right_video_tensor,
-                    occlusion_mask_tensor,
-                )
-                torch.cuda.empty_cache()
-                draw_progress_bar(frame_count, num_frames, prefix=f"  Encoding:")
-
-                t_end_batch = time.perf_counter()  # <--- TIMER END: Total Batch
-
-                # --- LOG RESULTS: Conditionally log at DEBUG level ---
-                if logger.isEnabledFor(logging.DEBUG):
-                    batch_size_actual = len(current_frame_indices)
-                    task_tag = "LowRes" if is_low_res_task else "HiRes"
-
-                    io_time = t_end_io - t_start_io
-                    preproc_time = t_end_preproc - t_start_preproc
-                    htod_time = t_end_transfer_HtoD - t_start_transfer_HtoD
-                    compute_time = t_end_compute - t_start_compute
-                    dtoh_time = t_end_transfer_DtoH - t_start_transfer_DtoH
-                    write_time = t_end_write - t_start_write
-                    total_batch_time = t_end_batch - t_start_batch
-
-                    logger.info(
-                        f"[{task_tag} Batch {i // batch_size_actual + 1}] Frames={batch_size_actual} Total={total_batch_time * 1000:.0f}ms | "
-                        f"IO={io_time * 1000:.0f}ms | CPU_Proc={preproc_time * 1000:.0f}ms | HtoD={htod_time * 1000:.0f}ms | "
-                        f"GPU_Comp={compute_time * 1000:.0f}ms | DtoH={dtoh_time * 1000:.0f}ms | FFmpeg_Write={write_time * 1000:.0f}ms"
-                    )
-                # --- END LOG RESULTS ---
-
-        except (IOError, BrokenPipeError) as e:
-            logger.error(f"FFmpeg pipe error: {e}. Encoding may have failed.")
-            encoding_successful = False
-        finally:
-            del stereo_projector
-            torch.cuda.empty_cache()
-            gc.collect()
-
-            # --- Finalize FFmpeg process ---
-            if ffmpeg_process is not None:
-                if ffmpeg_process.stdin:
-                    ffmpeg_process.stdin.close()  # Close the pipe to signal end of input
-
-                # Wait for the process to finish and get output
-                stdout, stderr = ffmpeg_process.communicate(timeout=120)
-
-                if self.stop_event.is_set():
-                    ffmpeg_process.terminate()
-                    logger.warning(
-                        f"FFmpeg encoding stopped by user for {os.path.basename(final_output_video_path)}."
-                    )
-                    encoding_successful = False
-                elif ffmpeg_process.returncode != 0:
-                    logger.error(
-                        f"FFmpeg encoding failed for {os.path.basename(final_output_video_path)} (return code {ffmpeg_process.returncode}):\n{stderr.decode()}"
-                    )
-                    encoding_successful = False
-                else:
-                    logger.info(
-                        f"Successfully encoded video to {final_output_video_path}"
-                    )
-                    logger.debug(f"FFmpeg stderr log:\n{stderr.decode()}")
-            else:
-                # Test mode: no encoding step was started, so nothing to finalize.
-                pass
-
-        if not encoding_successful:
-            return False
-
-        # --- Check for Low-Res Task BEFORE writing sidecar ---
-        if is_low_res_task:
-            # --- Write sidecar JSON after successful encoding ---
-            output_sidecar_data = {}
-
-            # Check and include frame_overlap and input_bias
-            has_non_zero_setting = False
-
-            if input_bias is not None and input_bias != 0.0:
-                output_sidecar_data["input_bias"] = input_bias
-                has_non_zero_setting = True
-
-            # Use the combined condition: non-zero setting AND is low-res
-            if has_non_zero_setting:
-                sidecar_ext = self.APP_CONFIG_DEFAULTS.get(
-                    "OUTPUT_SIDECAR_EXT", ".spsidecar"
-                )
-                output_sidecar_path = (
-                    f"{os.path.splitext(final_output_video_path)[0]}{sidecar_ext}"
-                )
-                try:
-                    with open(output_sidecar_path, "w", encoding="utf-8") as f:
-                        json.dump(output_sidecar_data, f, indent=4)
-                    logger.info(f"Created output sidecar file: {output_sidecar_path}")
-                except Exception as e:
-                    logger.error(
-                        f"Error creating output sidecar file '{output_sidecar_path}': {e}"
-                    )
-            else:
-                logger.debug(
-                    "Skipping output sidecar creation: frame_overlap and input_bias are zero."
-                )
-        else:
-            logger.debug(
-                "Skipping output sidecar creation: High-resolution output does not require spsidecar."
-            )
-
-        return True
-
-    # _determine_auto_convergence definition removed (moved to earlier in file)
-
-    def exit_app(self):
-        """Handles application exit, including stopping the processing thread."""
-        self._save_config()
-        self.stop_event.set()
-        if self.processing_thread and self.processing_thread.is_alive():
-            logger.info("==> Waiting for processing thread to finish...")
-            # --- NEW: Cleanup previewer resources ---
-            if hasattr(self, "previewer"):
-                self.previewer.cleanup()
-            self.processing_thread.join(timeout=5.0)
-            if self.processing_thread.is_alive():
-                logger.debug("==> Thread did not terminate gracefully within timeout.")
-        release_cuda_memory()
-        self.destroy()
-
-    def _find_preview_sources_callback(self) -> list:
-        """
-        Callback for VideoPreviewer. Scans for matching source video and depth map pairs.
-        Handles both folder (batch) and file (single) input modes.
-        """
-        source_path = self.input_source_clips_var.get()
-        depth_raw_path = self.input_depth_maps_var.get()
-
-        if not source_path or not depth_raw_path:
-            logger.warning("Preview Scan Failed: Source or depth path is empty.")
-            return []
-
-        # ------------------------------------------------------------
-        # 1) SINGLE-FILE MODE (both are actual files)
-        # ------------------------------------------------------------
-        is_source_file = os.path.isfile(source_path)
-        is_depth_file = os.path.isfile(depth_raw_path)
-
-        if is_source_file and is_depth_file:
-            logger.debug(
-                f"Preview Scan: Single file mode detected. "
-                f"Source: {source_path}, Depth: {depth_raw_path}"
-            )
-            return [
-                {
-                    "source_video": source_path,
-                    "depth_map": depth_raw_path,
-                }
-            ]
-
-        # ------------------------------------------------------------
-        # 2) FOLDER / BATCH MODE
-        # ------------------------------------------------------------
-        if not os.path.isdir(source_path) or not os.path.isdir(depth_raw_path):
-            logger.error(
-                "Preview Scan Failed: Inputs must either be two files or two valid directories."
-            )
-            return []
-
-        source_folder = source_path
-        base_depth_folder = depth_raw_path
-
-        # Collect all source videos
-        video_extensions = ("*.mp4", "*.avi", "*.mov", "*.mkv")
-        source_videos = []
-        for ext in video_extensions:
-            source_videos.extend(glob.glob(os.path.join(source_folder, ext)))
-
-        if not source_videos:
-            logger.warning(f"No source videos found in folder: {source_folder}")
-            return []
-
-        video_source_list = []
-
-        # ------------------------------------------------------------
-        # 2A) MULTI-MAP PREVIEW: search all map subfolders
-        # ------------------------------------------------------------
-        if self.multi_map_var.get():
-            depth_candidate_folders = []
-
-            # Treat each subdirectory (except 'sidecars') as a map folder
-            try:
-                for entry in os.listdir(base_depth_folder):
-                    full_sub = os.path.join(base_depth_folder, entry)
-                    if os.path.isdir(full_sub) and entry.lower() != "sidecars":
-                        depth_candidate_folders.append(full_sub)
-            except FileNotFoundError:
-                logger.error(
-                    f"Preview Scan Failed: Depth folder not found: {base_depth_folder}"
-                )
-                return []
-
-            if not depth_candidate_folders:
-                logger.warning(
-                    f"Preview Scan: No map subfolders found in Multi-Map base folder: {base_depth_folder}"
-                )
-
-            for video_path in sorted(source_videos):
-                base_name = os.path.splitext(os.path.basename(video_path))[0]
-                matched = False
-
-                for dpath in depth_candidate_folders:
-                    mp4 = os.path.join(dpath, f"{base_name}_depth.mp4")
-                    npz = os.path.join(dpath, f"{base_name}_depth.npz")
-
-                    if os.path.exists(mp4):
-                        video_source_list.append(
-                            {
-                                "source_video": video_path,
-                                "depth_map": mp4,
-                            }
-                        )
-                        matched = True
-                        break
-                    elif os.path.exists(npz):
-                        video_source_list.append(
-                            {
-                                "source_video": video_path,
-                                "depth_map": npz,
-                            }
-                        )
-                        matched = True
-                        break
-
-                if not matched:
-                    logger.debug(
-                        f"Preview Scan: No depth map found in any map folder for '{base_name}'."
-                    )
-
-        # ------------------------------------------------------------
-        # 2B) NORMAL MODE PREVIEW: single depth folder
-        # ------------------------------------------------------------
-        else:
-            depth_folder = base_depth_folder
-
-            for video_path in sorted(source_videos):
-                base_name = os.path.splitext(os.path.basename(video_path))[0]
+            for video_path in sorted(source_videos):
+                base_name = os.path.splitext(os.path.basename(video_path))[0]
 
                 candidates = [
                     os.path.join(depth_folder, f"{base_name}_depth.mp4"),
@@ -4432,63 +2715,34 @@ class SplatterGUI(ThemedTk):
         return video_source_list
 
     def _get_current_config(self):
-        """Collects all current GUI variable values into a single dictionary."""
-        config = {
-            # Folder Configurations
-            "input_source_clips": self.input_source_clips_var.get(),
-            "input_depth_maps": self.input_depth_maps_var.get(),
-            "output_splatted": self.output_splatted_var.get(),
-            "dark_mode_enabled": self.dark_mode_var.get(),
-            "window_width": self.winfo_width(),
-            "window_height": self.winfo_height(),
-            "window_x": self.winfo_x(),
-            "window_y": self.winfo_y(),
-            "update_slider_from_sidecar": self.update_slider_from_sidecar_var.get(),
-            "auto_save_sidecar": self.auto_save_sidecar_var.get(),
-            "multi_map_enabled": self.multi_map_var.get(),
-            "skip_lowres_preproc": self.skip_lowres_preproc_var.get(),
-            "crosshair_enabled": self.crosshair_enabled_var.get(),
-            "crosshair_white": self.crosshair_white_var.get(),
-            "crosshair_multi": self.crosshair_multi_var.get(),
-            "depth_pop_enabled": self.depth_pop_enabled_var.get(),
-            "debug_logging_enabled": self.debug_logging_var.get(),
-            "loop_playback": bool(
-                getattr(
-                    getattr(self, "previewer", None),
-                    "loop_playback_var",
-                    tk.BooleanVar(value=False),
-                ).get()
-            ),
-            "enable_full_resolution": self.enable_full_res_var.get(),
-            "batch_size": self.batch_size_var.get(),
-            "enable_low_resolution": self.enable_low_res_var.get(),
-            "pre_res_width": self.pre_res_width_var.get(),
-            "pre_res_height": self.pre_res_height_var.get(),
-            "low_res_batch_size": self.low_res_batch_size_var.get(),
-            "depth_dilate_size_x": self.depth_dilate_size_x_var.get(),
-            "depth_dilate_size_y": self.depth_dilate_size_y_var.get(),
-            "depth_blur_size_x": self.depth_blur_size_x_var.get(),
-            "depth_blur_size_y": self.depth_blur_size_y_var.get(),
-            "depth_dilate_left": self.depth_dilate_left_var.get(),
-            "depth_blur_left": self.depth_blur_left_var.get(),
-            "depth_blur_left_mix": self.depth_blur_left_mix_var.get(),
-            "preview_size": self.preview_size_var.get(),
-            "preview_source": self.preview_source_var.get(),
-            "process_length": self.process_length_var.get(),
-            "output_crf": self.output_crf_full_var.get(),  # legacy
-            "output_crf_full": self.output_crf_full_var.get(),
-            "output_crf_low": self.output_crf_low_var.get(),
-            "color_tags_mode": getattr(
-                self, "color_tags_mode_var", tk.StringVar(value="Auto")
-            ).get(),
-            "dual_output": self.dual_output_var.get(),
-            "auto_convergence_mode": self.auto_convergence_mode_var.get(),
-            "depth_gamma": self.depth_gamma_var.get(),
-            "max_disp": self.max_disp_var.get(),
-            "convergence_point": self.zero_disparity_anchor_var.get(),
-            "enable_global_norm": self.enable_global_norm_var.get(),  # Renamed
-            "move_to_finished": self.move_to_finished_var.get(),
-        }
+        """Collects all current GUI variable values into a single dictionary using ConfigManager."""
+        from core.splatting.config_manager import get_current_config
+        config = get_current_config(self.__dict__, self.config_manager.defaults)
+
+        # Map non-standard variable names to expected config keys
+        config["convergence_point"] = self.zero_disparity_anchor_var.get()
+        config["multi_map_enabled"] = self.multi_map_var.get()
+        config["dark_mode_enabled"] = self.dark_mode_var.get()
+        config["enable_full_resolution"] = self.enable_full_res_var.get()
+        config["enable_low_resolution"] = self.enable_low_res_var.get()
+
+        # Add special cases not directly mapped to _var
+        config["window_width"] = self.winfo_width()
+        config["window_height"] = self.winfo_height()
+        config["window_x"] = self.winfo_x()
+        config["window_y"] = self.winfo_y()
+
+        config["loop_playback"] = bool(
+            getattr(
+                getattr(self, "previewer", None),
+                "loop_playback_var",
+                tk.BooleanVar(value=False),
+            ).get()
+        )
+
+        # Legacy CRF mapping
+        config["output_crf"] = self.output_crf_full_var.get()
+
         # Avoid persisting test-forced preview settings
         try:
             test_active = bool(self.splat_test_var.get()) or bool(
@@ -4504,6 +2758,50 @@ class SplatterGUI(ThemedTk):
 
         return config
 
+    def _get_processing_settings(self):
+        """Converts current GUI configuration to BatchProcessor settings object."""
+        config = self._get_current_config()
+        return ProcessingSettings(
+            input_source_clips=config["input_source_clips"],
+            input_depth_maps=config["input_depth_maps"],
+            output_splatted=config["output_splatted"],
+            max_disp=float(config["max_disp"]),
+            process_length=int(config["process_length"]),
+            enable_full_resolution=config["enable_full_resolution"],
+            full_res_batch_size=int(config["batch_size"]),
+            enable_low_resolution=config["enable_low_resolution"],
+            low_res_width=int(config["pre_res_width"]),
+            low_res_height=int(config["pre_res_height"]),
+            low_res_batch_size=int(config["low_res_batch_size"]),
+            dual_output=config["dual_output"],
+            zero_disparity_anchor=float(config["convergence_point"]),
+            enable_global_norm=config["enable_global_norm"],
+            move_to_finished=config["move_to_finished"],
+            output_crf_full=int(config["output_crf_full"]),
+            output_crf_low=int(config["output_crf_low"]),
+            depth_gamma=float(config["depth_gamma"]),
+            depth_dilate_size_x=float(config["depth_dilate_size_x"]),
+            depth_dilate_size_y=float(config["depth_dilate_size_y"]),
+            depth_blur_size_x=float(config["depth_blur_size_x"]),
+            depth_blur_size_y=float(config["depth_blur_size_y"]),
+            depth_dilate_left=float(config["depth_dilate_left"]),
+            depth_blur_left=float(config["depth_blur_left"]),
+            depth_blur_left_mix=float(config["depth_blur_left_mix"]),
+            auto_convergence_mode=config["auto_convergence_mode"],
+            enable_sidecar_gamma=self.enable_sidecar_gamma_var.get(),
+            enable_sidecar_blur_dilate=self.enable_sidecar_blur_dilate_var.get(),
+            # NEW fields
+            multi_map=self.multi_map_var.get(),
+            selected_depth_map=self.selected_depth_map_var.get().strip(),
+            color_tags_mode=self.color_tags_mode_var.get() if hasattr(self, "color_tags_mode_var") else "Auto",
+            is_test_mode=False, # Standard batch mode
+            test_target_frame_idx=None,
+            skip_lowres_preproc=bool(getattr(self, "skip_lowres_preproc_var", None) and self.skip_lowres_preproc_var.get()),
+            sidecar_ext=self.APP_CONFIG_DEFAULTS["SIDECAR_EXT"],
+            sidecar_folder=self._get_sidecar_base_folder(),
+            track_dp_total_true_on_render=bool(getattr(self, "track_dp_total_true_on_render_var", None) and self.track_dp_total_true_on_render_var.get()),
+        )
+
     def get_current_preview_settings(self) -> dict:
         """Gathers settings from the GUI needed for the preview callback."""
         try:
@@ -4583,34 +2881,9 @@ class SplatterGUI(ThemedTk):
 
         return json_sidecar_path, depth_map_path, current_data
 
-    def _get_defined_tasks(self, settings):
-        """Helper to return a list of processing tasks based on GUI settings."""
-        processing_tasks = []
-        if settings["enable_full_resolution"]:
-            processing_tasks.append(
-                {
-                    "name": "Full-Resolution",
-                    "output_subdir": "hires",
-                    "set_pre_res": False,
-                    "target_width": -1,
-                    "target_height": -1,
-                    "batch_size": settings["full_res_batch_size"],
-                    "is_low_res": False,
-                }
-            )
-        if settings["enable_low_resolution"]:
-            processing_tasks.append(
-                {
-                    "name": "Low-Resolution",
-                    "output_subdir": "lowres",
-                    "set_pre_res": True,
-                    "target_width": settings["low_res_width"],
-                    "target_height": settings["low_res_height"],
-                    "batch_size": settings["low_res_batch_size"],
-                    "is_low_res": True,
-                }
-            )
-        return processing_tasks
+    def _get_defined_tasks(self, settings: ProcessingSettings) -> list[ProcessingTask]:
+        """Helper to return a list of processing tasks based on settings."""
+        return self.batch_processor.get_defined_tasks(settings)
 
     def _get_video_specific_settings(
         self,
@@ -4967,13 +3240,13 @@ class SplatterGUI(ThemedTk):
             ) = read_video_frames(
                 video_path,
                 process_length,
-                set_pre_res=task_settings["set_pre_res"],
-                pre_res_width=task_settings["target_width"],
-                pre_res_height=task_settings["target_height"],
+                set_pre_res=task_settings.set_pre_res,
+                pre_res_width=task_settings.target_width,
+                pre_res_height=task_settings.target_height,
             )
         except Exception as e:
             logger.error(
-                f"==> Error initializing input video reader for {os.path.basename(video_path)} {task_settings['name']} pass: {e}. Skipping this pass."
+                f"==> Error initializing input video reader for {os.path.basename(video_path)} {task_settings.name} pass: {e}. Skipping this pass."
             )
             return (
                 None,
@@ -5015,7 +3288,7 @@ class SplatterGUI(ThemedTk):
             depth_target_h = current_processed_height
             depth_target_w = current_processed_width
             depth_match = match_depth_res
-            if task_settings.get("is_low_res"):
+            if task_settings.is_low_res:
                 if (
                     getattr(self, "skip_lowres_preproc_var", None) is not None
                     and self.skip_lowres_preproc_var.get()
@@ -5107,25 +3380,8 @@ class SplatterGUI(ThemedTk):
         )
 
     def _load_config(self):
-        """Loads configuration from default config file."""
-        config_filename = self.APP_CONFIG_DEFAULTS["DEFAULT_CONFIG_FILENAME"]
-        # --- MODIFIED: Use the new dictionary constant ---
-        if os.path.exists(config_filename):
-            try:
-                with open(config_filename, "r") as f:
-                    self.app_config = json.load(f)
-
-                # --- BACKWARD COMPATIBILITY FIX: Handle the old 'enable_autogain' key ---
-                # Old meaning: True = Raw Input / Disable Normalization (GN OFF)
-                # New meaning: True = Enable Global Normalization (GN ON)
-                if "enable_autogain" in self.app_config:
-                    old_value = self.app_config.pop("enable_autogain")  # Remove old key
-                    # New value is the inverse of the old value
-                    self.app_config["enable_global_norm"] = not bool(old_value)
-                # --- END FIX ---
-            except Exception as e:
-                logger.error(f"Failed to load config file: {e}. Using defaults.")
-                self.app_config = {}
+        """Loads configuration using the ConfigManager."""
+        self.app_config = self.config_manager.load()
 
     def _load_help_texts(self):
         """Loads help texts from a JSON file."""
@@ -5146,47 +3402,18 @@ class SplatterGUI(ThemedTk):
             self.help_texts = {}
 
     def load_settings(self):
-        """Loads settings from a user-selected JSON file."""
+        """Loads settings from a user-selected JSON file using ConfigManager."""
         filename = filedialog.askopenfilename(
-            defaultextension=".json",
-            filetypes=[("JSON files", "*.json")],
+            defaultextension=".splatcfg",
+            filetypes=[("Splat Config", "*.splatcfg"), ("JSON files", "*.json")],
             title="Load Settings from File",
         )
         if not filename:
             return
 
         try:
-            with open(filename, "r") as f:
-                loaded_config = json.load(f)
-            # Apply loaded config values to the variables
-            for (
-                config_key,
-                config_value,
-            ) in loaded_config.items():  # Iterate over loaded keys
-                # --- NEW MAPPING LOGIC ---
-                # Construct the expected name of the Tkinter variable
-                tk_var_attr_name = config_key + "_var"
-
-                if hasattr(self, tk_var_attr_name):
-                    tk_var_object = getattr(self, tk_var_attr_name)
-
-                    if isinstance(tk_var_object, tk.BooleanVar):
-                        # Ensure value is converted to a proper boolean/int before setting BooleanVar
-                        tk_var_object.set(bool(config_value))
-                    elif isinstance(tk_var_object, tk.StringVar):
-                        # Set StringVar directly
-                        tk_var_object.set(str(config_value))
-
-            # Apply loaded config values to the variables
-            for key, var in self.__dict__.items():
-                if key.endswith("_var") and key in loaded_config:
-                    # Logic to safely set values:
-                    # For tk.StringVar, set()
-                    # For tk.BooleanVar, use set() with the bool/int value
-                    if isinstance(var, tk.BooleanVar):
-                        var.set(bool(loaded_config[key]))
-                    elif isinstance(var, tk.StringVar):
-                        var.set(str(loaded_config[key]))
+            from core.splatting.config_manager import load_settings_from_file
+            load_settings_from_file(filename, tk_vars=self.__dict__)
 
             self._apply_theme()  # Re-apply theme in case dark mode setting was loaded
             self.toggle_processing_settings_fields()  # Update state of dependent fields
@@ -5536,843 +3763,66 @@ class SplatterGUI(ThemedTk):
                         padding=(0, band_half),
                     )
                     > 0.5
-                ).float()
-
-                # Feather the band so the blend ramps on/off smoothly.
-                alpha = custom_blur(edge_band, 7, 1, False, 1.0)
-                alpha = torch.clamp(alpha, 0.0, 1.0)
-
-                # Two-pass blur for Blur Left:
-                # - Horizontal-only blur helps anti-alias along X (like your regular Blur X behavior),
-                # - Vertical-only blur helps smooth stair-steps along the edge.
-                # We blend horizontal/vertical Blur Left based on a compact UI selector:
-                #   0.0 = all horizontal, 1.0 = all vertical, 0.5 = 50/50.
-                try:
-                    mix_f = float(self.depth_blur_left_mix_var.get())
-                except Exception:
-                    mix_f = 0.5
-                mix_f = max(0.0, min(1.0, mix_f))
-
-                BLUR_LEFT_V_WEIGHT = mix_f
-                BLUR_LEFT_H_WEIGHT = 1.0 - mix_f
-
-                blurred_h = None
-                blurred_v = None
-                if BLUR_LEFT_H_WEIGHT > 1e-6:
-                    blurred_h = custom_blur(tensor_4d, k_blur, 1, False, max_raw_value)
-                if BLUR_LEFT_V_WEIGHT > 1e-6:
-                    blurred_v = custom_blur(tensor_4d, 1, k_blur, False, max_raw_value)
-
-                if blurred_h is not None and blurred_v is not None:
-                    wsum = BLUR_LEFT_H_WEIGHT + BLUR_LEFT_V_WEIGHT
-                    blurred = (
-                        blurred_h * BLUR_LEFT_H_WEIGHT + blurred_v * BLUR_LEFT_V_WEIGHT
-                    ) / max(wsum, 1e-6)
-                elif blurred_h is not None:
-                    blurred = blurred_h
-                elif blurred_v is not None:
-                    blurred = blurred_v
-                else:
-                    blurred = tensor_4d
-
-                tensor_4d = tensor_4d * (1.0 - alpha) + blurred * alpha
-            if abs(render_dilate_x) > 1e-5 or abs(render_dilate_y) > 1e-5:
-                tensor_4d = custom_dilate(
-                    tensor_4d,
-                    float(render_dilate_x),
-                    float(render_dilate_y),
-                    False,
-                    max_raw_value,
-                )
-            if render_blur_x > 0 or render_blur_y > 0:
-                tensor_4d = custom_blur(
-                    tensor_4d,
-                    float(render_blur_x),
-                    float(render_blur_y),
-                    False,
-                    max_raw_value,
-                )
-            batch_depth_numpy_float = tensor_4d.squeeze(1).cpu().numpy()
-            release_cuda_memory()
-
-        return batch_depth_numpy_float
-
-    def _process_single_video_tasks(
-        self,
-        video_path,
-        settings,
-        initial_overall_task_counter,
-        is_single_file_mode,
-        finished_source_folder=None,
-        finished_depth_folder=None,
-    ):
-        """
-        Handles the full processing lifecycle (sidecar, auto-conv, task loop, move-to-finished)
-        for a single video and its depth map.
-
-        Returns: (tasks_processed_count: int, any_task_completed_successfully: bool)
-        """
-        # Initialize task-local variables (some of these were local in the old _run_batch_process loop)
-        current_depth_dilate_size_x = 0
-        current_depth_dilate_size_y = 0
-        current_depth_blur_size_x = 0
-        current_depth_blur_size_y = 0
-        current_depth_dilate_left = 0.0
-        current_depth_blur_left = 0.0
-        current_depth_blur_left_mix = 0.0
-
-        video_name = os.path.splitext(os.path.basename(video_path))[0]
-        logger.info(f"==> Processing Video: {video_name}")
-        self.progress_queue.put(("update_info", {"filename": video_name}))
-
-        # Keep a local counter for tasks processed in this function
-        local_task_counter = initial_overall_task_counter
-
-        video_specific_settings = self._get_video_specific_settings(
-            video_path,
-            settings["input_depth_maps"],
-            settings["zero_disparity_anchor"],
-            settings["max_disp"],
-            is_single_file_mode,
-        )
-
-        processing_tasks = self._get_defined_tasks(settings)
-        has_low_res_task = any(t.get("is_low_res") for t in processing_tasks)
-        has_full_res_task = any(not t.get("is_low_res") for t in processing_tasks)
-        expected_task_count = len(processing_tasks)
-        processed_tasks_count = 0
-        any_task_completed_successfully_for_this_video = False
-
-        if video_specific_settings.get("error"):
-            logger.error(
-                f"Error getting video specific settings for {video_name}: {video_specific_settings['error']}. Skipping."
-            )
-            # Skip the expected task count in the progress bar
-            local_task_counter += expected_task_count
-            self.progress_queue.put(("processed", local_task_counter))
-            return expected_task_count, False
-
-        actual_depth_map_path = video_specific_settings["actual_depth_map_path"]
-        current_zero_disparity_anchor = video_specific_settings["convergence_plane"]
-        current_max_disparity_percentage = video_specific_settings[
-            "max_disparity_percentage"
-        ]
-        current_input_bias = video_specific_settings["input_bias"]
-        anchor_source = video_specific_settings["anchor_source"]
-        max_disp_source = video_specific_settings["max_disp_source"]
-        gamma_source = video_specific_settings["gamma_source"]
-        map_source = video_specific_settings.get("map_source", "N/A")
-        current_depth_gamma = video_specific_settings["depth_gamma"]
-        current_depth_dilate_size_x = video_specific_settings["depth_dilate_size_x"]
-        current_depth_dilate_size_y = video_specific_settings["depth_dilate_size_y"]
-        current_depth_blur_size_x = video_specific_settings["depth_blur_size_x"]
-        current_depth_blur_size_y = video_specific_settings["depth_blur_size_y"]
-        current_depth_dilate_left = float(
-            video_specific_settings.get(
-                "depth_dilate_left", settings.get("depth_dilate_left", 0.0)
-            )
-        )
-        current_depth_blur_left = float(
-            video_specific_settings.get(
-                "depth_blur_left", settings.get("depth_blur_left", 0.0)
-            )
-        )
-        current_depth_blur_left_mix = float(
-            video_specific_settings.get(
-                "depth_blur_left_mix",
-                getattr(self, "depth_blur_left_mix_var", None).get()
-                if getattr(self, "depth_blur_left_mix_var", None)
-                else 0.0,
-            )
-        )
-
-        if not processing_tasks:
-            logger.debug(
-                f"==> No processing tasks configured for {video_name}. Skipping."
-            )
-            return 0, False
-
-        # --- Auto-Convergence Logic (BEFORE initializing readers) ---
-        auto_conv_mode = settings["auto_convergence_mode"]
-
-        # --- NEW: Global Normalization Policy variables ---
-        enable_global_norm_policy = video_specific_settings["enable_global_norm"]
-        gn_source = video_specific_settings["gn_source"]
-
-        if (
-            video_specific_settings["sidecar_found"]
-            and self.enable_global_norm_var.get()
-        ):
-            # Policy: Sidecar exists AND GUI toggle is ON. Policy forces GN OFF.
-            if not self._gn_warning_shown:
-                messagebox.showwarning(
-                    "GN Policy Warning",
-                    f"Sidecar found for '{video_name}'.\n"
-                    f"Global Normalization is DISABLED for this clip, overriding the GUI setting.\n"
-                    f"Further warnings will be logged to console only.",
-                )
-                self._gn_warning_shown = (
-                    True  # Set flag to log to console only next time
-                )
-            else:
-                logger.warning(
-                    f"GN Policy: Sidecar found for {video_name}. GN forced OFF (console log only)."
-                )
-
-        # --- NEW LOGIC: Sidecar overrides Auto-Convergence ---
-        if anchor_source == "Sidecar" and auto_conv_mode != "Off":
-            logger.info(
-                f"Sidecar found for {video_name}. Convergence Point locked to Sidecar value ({current_zero_disparity_anchor:.4f}). Auto-Convergence SKIPPED."
-            )
-            auto_conv_mode = "Off"
-
-        if auto_conv_mode != "Off":
-            logger.info(
-                f"Auto-Convergence is ENABLED (Mode: {auto_conv_mode}). Running pre-pass..."
-            )
-
-            try:
-                anchor_float = float(current_zero_disparity_anchor)
-            except (ValueError, TypeError):
-                logger.error(
-                    f"Invalid convergence anchor value found: {current_zero_disparity_anchor}. Defaulting to 0.5."
-                )
-                anchor_float = 0.5
-
-            new_anchor_avg, new_anchor_peak = self._determine_auto_convergence(
-                video_path,
-                actual_depth_map_path,
-                settings["process_length"],
-                settings["full_res_batch_size"],
-                anchor_float,
-                gamma=settings.get("depth_gamma", 1.0),
-            )
-            if auto_conv_mode == "Average":
-                new_anchor_val = new_anchor_avg
-            elif auto_conv_mode == "Peak":
-                new_anchor_val = new_anchor_peak
-            elif auto_conv_mode == "Hybrid":
-                new_anchor_val = (new_anchor_avg + new_anchor_peak) / 2.0
-            else:
-                new_anchor_val = float(current_zero_disparity_anchor)
-
-            if new_anchor_val != current_zero_disparity_anchor:
-                current_zero_disparity_anchor = new_anchor_val
-                anchor_source = "Auto"
-
-            logger.info(
-                f"Using Convergence Point: {current_zero_disparity_anchor:.4f} (Source: {anchor_source})"
-            )
-
-        for task in processing_tasks:
-            if self.stop_event.is_set():
-                logger.info(
-                    f"==> Stopping {task['name']} processing for {video_name} due to user request"
-                )
-                # Increment the global counter for all remaining, skipped tasks
-                remaining_tasks_to_increment = (
-                    expected_task_count - processed_tasks_count
-                )
-                local_task_counter += remaining_tasks_to_increment
-                self.progress_queue.put(("processed", local_task_counter))
-                return (
-                    expected_task_count,
-                    any_task_completed_successfully_for_this_video,
-                )
-
-            logger.debug(f"\n==> Starting {task['name']} pass for {video_name}")
-            self.progress_queue.put(
-                ("status", f"Processing {task['name']} for {video_name}")
-            )
-
-            # Decide what to show in the Map field
-            if self.multi_map_var.get():
-                # Multi-Map mode
-                if actual_depth_map_path and map_source not in ("", "N/A"):
-                    map_folder = os.path.basename(
-                        os.path.dirname(actual_depth_map_path)
-                    ).strip()
-                    map_label = f"{map_folder} ({map_source})"
-                else:
-                    map_label = "N/A"
-            else:
-                # Normal mode
-                map_label = "Direct file" if is_single_file_mode else "Direct folder"
-
-            self.progress_queue.put(
-                (
-                    "update_info",
-                    {
-                        "task_name": task["name"],
-                        "convergence": f"{current_zero_disparity_anchor:.2f} ({anchor_source})",
-                        "disparity": f"{current_max_disparity_percentage:.1f}% ({max_disp_source})",
-                        "gamma": f"{current_depth_gamma:.2f} ({gamma_source})",
-                        "map": map_label,
-                    },
-                )
-            )
-
-            (
-                video_reader_input,
-                depth_reader_input,
-                processed_fps,
-                original_vid_h,
-                original_vid_w,
-                current_processed_height,
-                current_processed_width,
-                video_stream_info,
-                total_frames_input,
-                total_frames_depth,
-                actual_depth_height,
-                actual_depth_width,
-                depth_stream_info,
-            ) = self._initialize_video_and_depth_readers(
-                video_path,
-                actual_depth_map_path,
-                settings["process_length"],
-                task,
-                settings["match_depth_res"],
-            )
-
-            # Explicitly check for None for critical components before proceeding
-            if (
-                video_reader_input is None
-                or depth_reader_input is None
-                or video_stream_info is None
-            ):
-                logger.error(
-                    f"Skipping {task['name']} pass for {video_name} due to reader initialization error, frame count mismatch, or missing stream info."
-                )
-                local_task_counter += 1
-                processed_tasks_count += 1
-                self.progress_queue.put(("processed", local_task_counter))
-                release_cuda_memory()
-                continue
-
-            # --- MODIFIED: Use the policy to determine the mode ---
-            assume_raw_input_mode = (
-                not enable_global_norm_policy
-            )  # If GN is OFF, assume RAW Input
-            global_depth_min = 0.0
-            global_depth_max = 1.0
-
-            # --- UNCONDITIONAL Max Content Value Scan for RAW/Normalization Modes ---
-            max_content_value = 1.0
-            raw_depth_reader_temp = None
-            try:
-                depth_info_tmp = get_video_stream_info(actual_depth_map_path)
-
-                bit_depth_tmp = _infer_depth_bit_depth(depth_info_tmp)
-
-                pix_fmt_tmp = str((depth_info_tmp or {}).get("pix_fmt", ""))
-
-                # Use source/processing dimensions for the content scan (avoids NameError and keeps preview/render parity)
-                scan_w = original_vid_w or current_processed_width or actual_depth_width
-                scan_h = (
-                    original_vid_h or current_processed_height or actual_depth_height
-                )
-
-                if bit_depth_tmp > 8:
-                    raw_depth_reader_temp = FFmpegDepthPipeReader(
-                        actual_depth_map_path,
-                        out_w=scan_w,
-                        out_h=scan_h,
-                        bit_depth=bit_depth_tmp,
-                        num_frames=total_frames_depth,
-                        pix_fmt=pix_fmt_tmp,
-                    )
-
-                else:
-                    raw_depth_reader_temp = VideoReader(
-                        actual_depth_map_path, ctx=cpu(0), width=scan_w, height=scan_h
-                    )
-
-                if len(raw_depth_reader_temp) > 0:
-                    _, max_content_value = compute_global_depth_stats(
-                        depth_map_reader=raw_depth_reader_temp,
-                        total_frames=total_frames_depth,
-                        chunk_size=task["batch_size"],
-                    )
-                    logger.debug(f"Max content depth scanned: {max_content_value:.3f}.")
-                else:
-                    logger.error("RAW depth reader has no frames for content scan.")
-            except Exception as e:
-                logger.error(f"Failed to scan max content depth: {e}")
-            finally:
-                if raw_depth_reader_temp:
-                    if hasattr(raw_depth_reader_temp, "close"):
-                        try:
-                            raw_depth_reader_temp.close()
-                        except Exception:
-                            pass
-                    del raw_depth_reader_temp
-                    gc.collect()
-            # --- END UNCONDITIONAL SCAN ---
-
-            if not assume_raw_input_mode:
-                logger.info(
-                    "==> Global Depth Normalization selected. Starting global depth stats pre-pass with RAW reader."
-                )
-
-                raw_depth_reader_temp = None
-                try:
-                    depth_info_tmp = get_video_stream_info(actual_depth_map_path)
-
-                    bit_depth_tmp = _infer_depth_bit_depth(depth_info_tmp)
-
-                    pix_fmt_tmp = str((depth_info_tmp or {}).get("pix_fmt", ""))
-
-                    if bit_depth_tmp > 8:
-                        raw_depth_reader_temp = FFmpegDepthPipeReader(
-                            actual_depth_map_path,
-                            out_w=actual_depth_width,
-                            out_h=actual_depth_height,
-                            bit_depth=bit_depth_tmp,
-                            num_frames=total_frames_depth,
-                            pix_fmt=pix_fmt_tmp,
-                        )
-
-                    else:
-                        raw_depth_reader_temp = VideoReader(
-                            actual_depth_map_path,
-                            ctx=cpu(0),
-                            width=actual_depth_width,
-                            height=actual_depth_height,
-                        )
-
-                    if len(raw_depth_reader_temp) > 0:
-                        global_depth_min, global_depth_max = compute_global_depth_stats(
-                            depth_map_reader=raw_depth_reader_temp,
-                            total_frames=total_frames_depth,
-                            chunk_size=task["batch_size"],
-                        )
-                        logger.debug(
-                            "Successfully computed global stats from RAW reader."
-                        )
-                    else:
-                        logger.error("RAW depth reader has no frames.")
-                except Exception as e:
-                    logger.error(
-                        f"Failed to initialize/read RAW depth reader for global stats: {e}"
-                    )
-                    global_depth_min = 0.0
-                    global_depth_max = 1.0
-                finally:
-                    if raw_depth_reader_temp:
-                        if hasattr(raw_depth_reader_temp, "close"):
-                            try:
-                                raw_depth_reader_temp.close()
-                            except Exception:
-                                pass
-                        del raw_depth_reader_temp
-                        gc.collect()
-            else:
-                logger.debug(
-                    "==> No Normalization (Assume Raw 0-1 Input) selected. Skipping global stats pre-pass."
-                )
-
-                # --- RAW INPUT MODE SCALING ---
-                final_scaling_factor = 1.0
-
-                if max_content_value <= 256.0 and max_content_value > 1.0:
-                    final_scaling_factor = 255.0
-                    logger.debug(
-                        f"Content Max {max_content_value:.2f} <= 8-bit. SCALING BY 255.0."
-                    )
-                elif max_content_value > 256.0 and max_content_value <= 1024.0:
-                    final_scaling_factor = 1023.0
-                    logger.debug(
-                        f"Content Max {max_content_value:.2f} (9-10bit). SCALING BY 1023.0 (expected 10-bit max) for preview parity."
-                    )
-                else:
-                    final_scaling_factor = 1023.0
-                    logger.warning(
-                        f"Max content value is too high/low ({max_content_value:.2f}). Using fallback 1023.0."
-                    )
-
-                global_depth_max = final_scaling_factor
-                global_depth_min = 0.0
-
-                logger.debug(
-                    f"Raw Input Final Scaling Factor set to: {global_depth_max:.3f}"
-                )
-
-            if not (
-                actual_depth_height == current_processed_height
-                and actual_depth_width == current_processed_width
-            ):
-                # Low-res parity path intentionally loads depth at clip resolution so dilation/blur happens pre-resize.
-                # The depth frames are resized to match the (possibly low-res) video frames *after* preprocessing inside depthSplatting().
-                if task.get("is_low_res"):
-                    skip_lr_preproc = bool(
-                        getattr(self, "skip_lowres_preproc_var", None) is not None
-                        and self.skip_lowres_preproc_var.get()
-                    )
-                    if (not skip_lr_preproc) and (
-                        actual_depth_width == original_vid_w
-                        and actual_depth_height == original_vid_h
-                    ):
-                        logger.info(
-                            f"==> Low-Resolution pass: Upscaling depthmap to clip resolution ({actual_depth_width}x{actual_depth_height}) "
-                            f"before preprocessing for parity; it will be resized to ({current_processed_width}x{current_processed_height}) after preprocessing."
-                        )
-                    else:
-                        _bd_warn = (
-                            _infer_depth_bit_depth(depth_stream_info)
-                            if depth_stream_info
-                            else 8
-                        )
-                        if _bd_warn > 8:
-                            logger.warning(
-                                f"==> Warning: Depth map reader output resolution ({actual_depth_width}x{actual_depth_height}) does not match processed "
-                                f"video resolution ({current_processed_width}x{current_processed_height}) for {task['name']} pass. "
-                                f"This indicates an issue with `load_pre_rendered_depth`'s `width`/`height` parameters. Processing may proceed but results might be misaligned."
-                            )
-                else:
-                    _bd_warn = (
-                        _infer_depth_bit_depth(depth_stream_info)
-                        if depth_stream_info
-                        else 8
-                    )
-                    if _bd_warn > 8:
-                        logger.warning(
-                            f"==> Warning: Depth map reader output resolution ({actual_depth_width}x{actual_depth_height}) does not match processed "
-                            f"video resolution ({current_processed_width}x{current_processed_height}) for {task['name']} pass. "
-                            f"This indicates an issue with `load_pre_rendered_depth`'s `width`/`height` parameters. Processing may proceed but results might be misaligned."
-                        )
-
-            tv_disp_comp = 1.0
-            if assume_raw_input_mode:
-                try:
-                    if _infer_depth_bit_depth(depth_stream_info) > 8 and str((depth_stream_info or {}).get("color_range", "unknown")).lower() == "tv":
-                        tv_disp_comp = 1.0 / (DEPTH_VIS_TV10_WHITE_NORM - DEPTH_VIS_TV10_BLACK_NORM)
-                except Exception:
-                    tv_disp_comp = 1.0
-
-            actual_percentage_for_calculation = (current_max_disparity_percentage / 20.0) * tv_disp_comp
-            actual_max_disp_pixels = (
-                actual_percentage_for_calculation / 100.0
-            ) * current_processed_width
-            logger.debug(
-                f"==> Max Disparity Input: {current_max_disparity_percentage:.1f}% -> Calculated Max Disparity for splatting ({task['name']}): {actual_max_disp_pixels:.2f} pixels"
-            )
-
-            self.progress_queue.put(
-                (
-                    "update_info",
-                    {
-                        "disparity": f"{current_max_disparity_percentage:.1f}% ({actual_max_disp_pixels:.2f} pixels)"
-                    },
-                )
-            )
-
-            current_output_subdir = os.path.join(
-                settings["output_splatted"], task["output_subdir"]
-            )
-            os.makedirs(current_output_subdir, exist_ok=True)
-            output_video_path_base = os.path.join(
-                current_output_subdir, f"{video_name}.mp4"
-            )
-
-            # Prepare signature for render-time Total(D+P) capture (for CSV/sidecar exact max update)
-            try:
-                _track_true = (
-                    getattr(self, "track_dp_total_true_on_render_var", None) is not None
-                    and self.track_dp_total_true_on_render_var.get()
-                )
-                if _track_true and ((has_full_res_task and (not task.get("is_low_res"))) or ((not has_full_res_task) and task.get("is_low_res"))):
-                    self._dp_total_true_active_sig = self._dp_total_signature(
-                        actual_depth_map_path,
-                        float(current_zero_disparity_anchor),
-                        float(current_max_disparity_percentage),
-                        float(current_depth_gamma),
-                    )
-                    self._dp_total_true_active_val = None
-                else:
-                    self._dp_total_true_active_sig = None
-                    self._dp_total_true_active_val = None
-            except Exception:
-                self._dp_total_true_active_sig = None
-                self._dp_total_true_active_val = None
-            completed_splatting_task = self.depthSplatting(
-                input_video_reader=video_reader_input,
-                depth_map_reader=depth_reader_input,
-                total_frames_to_process=total_frames_input,
-                processed_fps=processed_fps,
-                output_video_path_base=output_video_path_base,
-                target_output_height=current_processed_height,
-                target_output_width=current_processed_width,
-                max_disp=actual_max_disp_pixels,
-                process_length=settings["process_length"],
-                batch_size=task["batch_size"],
-                dual_output=settings["dual_output"],
-                zero_disparity_anchor_val=current_zero_disparity_anchor,
-                video_stream_info=video_stream_info,
-                input_bias=current_input_bias,
-                assume_raw_input=assume_raw_input_mode,
-                global_depth_min=global_depth_min,
-                global_depth_max=global_depth_max,
-                depth_stream_info=depth_stream_info,
-                user_output_crf=(
-                    settings.get("output_crf_low")
-                    if task["is_low_res"]
-                    else settings.get("output_crf_full", settings.get("output_crf"))
-                ),
-                is_low_res_task=task["is_low_res"],
-                depth_gamma=current_depth_gamma,
-                depth_dilate_size_x=(
-                    0.0
-                    if (
-                        task["is_low_res"]
-                        and getattr(self, "skip_lowres_preproc_var", None) is not None
-                        and self.skip_lowres_preproc_var.get()
-                    )
-                    else current_depth_dilate_size_x
-                ),
-                depth_dilate_size_y=(
-                    0.0
-                    if (
-                        task["is_low_res"]
-                        and getattr(self, "skip_lowres_preproc_var", None) is not None
-                        and self.skip_lowres_preproc_var.get()
-                    )
-                    else current_depth_dilate_size_y
-                ),
-                depth_blur_size_x=(
-                    0.0
-                    if (
-                        task["is_low_res"]
-                        and getattr(self, "skip_lowres_preproc_var", None) is not None
-                        and self.skip_lowres_preproc_var.get()
-                    )
-                    else current_depth_blur_size_x
-                ),
-                depth_blur_size_y=(
-                    0.0
-                    if (
-                        task["is_low_res"]
-                        and getattr(self, "skip_lowres_preproc_var", None) is not None
-                        and self.skip_lowres_preproc_var.get()
-                    )
-                    else current_depth_blur_size_y
-                ),
-                depth_dilate_left=(
-                    0.0
-                    if (
-                        task["is_low_res"]
-                        and getattr(self, "skip_lowres_preproc_var", None) is not None
-                        and self.skip_lowres_preproc_var.get()
-                    )
-                    else float(current_depth_dilate_left)
-                ),
-                depth_blur_left=(
-                    0.0
-                    if (
-                        task["is_low_res"]
-                        and getattr(self, "skip_lowres_preproc_var", None) is not None
-                        and self.skip_lowres_preproc_var.get()
-                    )
-                    else float(current_depth_blur_left)
-                ),
-            )
-
-            if self.stop_event.is_set():
-                logger.info(
-                    f"==> Stopping {task['name']} pass for {video_name} due to user request"
-                )
-                break
-
-            if completed_splatting_task:
-                logger.debug(
-                    f"==> Splatted {task['name']} video saved for {video_name}."
-                )
-                any_task_completed_successfully_for_this_video = True
-
-                # Persist per-clip settings/metadata + (optional) render-time True Max into sidecar/CSV
-                try:
-                    # Capture any render-measured True Max Total(D+P)
-                    sig = getattr(self, "_dp_total_true_active_sig", None)
-                    dp_true = getattr(self, "_dp_total_true_active_val", None)
-                    if sig and dp_true is not None:
-                        self._dp_total_true_cache[sig] = float(dp_true)
-
-                    sidecar_ext = self.APP_CONFIG_DEFAULTS.get("SIDECAR_EXT", ".fssidecar")
-                    depth_bn_noext = os.path.splitext(os.path.basename(actual_depth_map_path))[0]
-                    sidecar_folder = self._get_sidecar_base_folder()
-                    json_sidecar_path = os.path.join(sidecar_folder, f"{depth_bn_noext}{sidecar_ext}")
-
-                    sidecar_data = {}
-                    try:
-                        sidecar_data = self.sidecar_manager.load_sidecar_data(json_sidecar_path) or {}
-                    except Exception:
-                        sidecar_data = {}
-
-                    # Always overwrite with the EFFECTIVE values used for THIS clip (avoid defaults leaking in)
-                    sidecar_data["convergence_plane"] = float(current_zero_disparity_anchor)
-                    sidecar_data["max_disparity"] = float(current_max_disparity_percentage)
-                    sidecar_data["gamma"] = float(f"{float(current_depth_gamma):.2f}")
-
-                    sidecar_data["depth_dilate_size_x"] = float(current_depth_dilate_size_x)
-                    sidecar_data["depth_dilate_size_y"] = float(current_depth_dilate_size_y)
-                    sidecar_data["depth_blur_size_x"] = float(current_depth_blur_size_x)
-                    sidecar_data["depth_blur_size_y"] = float(current_depth_blur_size_y)
-                    sidecar_data["depth_dilate_left"] = float(current_depth_dilate_left)
-                    sidecar_data["depth_blur_left"] = float(current_depth_blur_left)
-
-                    # Selected depth map (Multi-Map mode)
-                    try:
-                        if self.multi_map_var.get():
-                            base_depth = os.path.normpath(settings.get("input_depth_maps", "") or "")
-                            if base_depth:
-                                rel = os.path.relpath(os.path.normpath(actual_depth_map_path), base_depth)
-                                parts = rel.split(os.sep)
-                                if len(parts) > 1 and parts[0] not in (".", ""):
-                                    sidecar_data["selected_depth_map"] = parts[0]
-                    except Exception:
-                        pass
-
-                    # Auto Border: only compute/save when Auto is selected; otherwise preserve any existing per-clip borders.
-                    try:
-                        border_mode = self.border_mode_var.get()
-                    except Exception:
-                        border_mode = "Off"
-
-                    if border_mode in ("Auto Basic", "Auto Adv."):
-                        try:
-                            if border_mode == "Auto Basic":
-                                width = max(
-                                    0.0,
-                                    (1.0 - float(current_zero_disparity_anchor))
-                                    * 2.0
-                                    * (float(current_max_disparity_percentage) / 20.0) * tv_disp_comp,
-                                )
-                                sidecar_data["left_border"] = round(width, 3)
-                                sidecar_data["right_border"] = round(width, 3)
-                                sidecar_data["auto_border_L"] = sidecar_data["left_border"]
-                                sidecar_data["auto_border_R"] = sidecar_data["right_border"]
-                                sidecar_data["border_mode"] = border_mode
-                                logger.info(
-                                    f"Auto Border (Basic): left_border={sidecar_data['left_border']}, right_border={sidecar_data['right_border']}"
-                                )
-                            else:
-                                l_val, r_val = self._scan_borders_for_depth_path(
-                                    actual_depth_map_path,
-                                    float(current_zero_disparity_anchor),
-                                    float(current_max_disparity_percentage),
-                                    float(current_depth_gamma),
-                                )
-                                if l_val is not None and r_val is not None:
-                                    sidecar_data["left_border"] = round(float(l_val), 3)
-                                    sidecar_data["right_border"] = round(float(r_val), 3)
-                                    sidecar_data["auto_border_L"] = sidecar_data["left_border"]
-                                    sidecar_data["auto_border_R"] = sidecar_data["right_border"]
-                                    sidecar_data["border_mode"] = border_mode
-                                    logger.info(
-                                        f"Auto Border (Adv.): left_border={sidecar_data['left_border']}, right_border={sidecar_data['right_border']}"
-                                    )
-                        except Exception:
-                            pass
-
-                    # Persist render-measured True Max Total(D+P) into sidecar (and log the value) if available
-                    if dp_true is not None:
-                        try:
-                            sidecar_data["dp_total_max_true"] = round(float(dp_true), 3)
-                            logger.info(
-                                f"True Max Total(D+P): dp_total_max_true={sidecar_data['dp_total_max_true']:.3f}"
-                            )
-                        except Exception:
-                            pass
-
-                    # Save sidecar (create if missing)
-                    try:
-                        if not self.sidecar_manager.save_sidecar_data(json_sidecar_path, sidecar_data):
-                            logger.error(f"Failed to save sidecar: {os.path.basename(json_sidecar_path)}")
-                    except Exception:
-                        pass
+                ).float()
 
-                    # Update CSV row (creates file on demand)
-                    self._auto_pass_csv_update_row_for_paths(
-                        source_video_path=video_path,
-                        depth_map_path=actual_depth_map_path,
-                        current_data=sidecar_data,
-                        dp_total_max_true=(float(dp_true) if dp_true is not None else None),
-                    )
+                # Feather the band so the blend ramps on/off smoothly.
+                alpha = custom_blur(edge_band, 7, 1, False, 1.0)
+                alpha = torch.clamp(alpha, 0.0, 1.0)
+
+                # Two-pass blur for Blur Left:
+                # - Horizontal-only blur helps anti-alias along X (like your regular Blur X behavior),
+                # - Vertical-only blur helps smooth stair-steps along the edge.
+                # We blend horizontal/vertical Blur Left based on a compact UI selector:
+                #   0.0 = all horizontal, 1.0 = all vertical, 0.5 = 50/50.
+                try:
+                    mix_f = float(self.depth_blur_left_mix_var.get())
                 except Exception:
-                    pass
+                    mix_f = 0.5
+                mix_f = max(0.0, min(1.0, mix_f))
 
-                if video_reader_input is not None:
-                    del video_reader_input
-                if depth_reader_input is not None:
-                    del depth_reader_input
-                torch.cuda.empty_cache()
-                gc.collect()
-                logger.debug(
-                    "Explicitly deleted VideoReader objects and forced garbage collection to release file handles."
-                )
-            else:
-                logger.info(
-                    f"==> Splatting task '{task['name']}' for '{video_name}' was skipped or failed. Files will NOT be moved."
-                )
-                if video_reader_input:
-                    del video_reader_input
-                if depth_reader_input:
-                    del depth_reader_input
-                torch.cuda.empty_cache()
-                gc.collect()
-
-            local_task_counter += 1
-            processed_tasks_count += 1
-            self.progress_queue.put(("processed", local_task_counter))
-            logger.debug(f"==> Completed {task['name']} pass for {video_name}.")
-
-        # After all tasks for the current video are processed or stopped
-        if self.stop_event.is_set():
-            return expected_task_count, any_task_completed_successfully_for_this_video
+                BLUR_LEFT_V_WEIGHT = mix_f
+                BLUR_LEFT_H_WEIGHT = 1.0 - mix_f
 
-        # Move to finished logic
-        move_enabled = settings[
-            "move_to_finished"
-        ]  # Use the setting from the dictionary
+                blurred_h = None
+                blurred_v = None
+                if BLUR_LEFT_H_WEIGHT > 1e-6:
+                    blurred_h = custom_blur(tensor_4d, k_blur, 1, False, max_raw_value)
+                if BLUR_LEFT_V_WEIGHT > 1e-6:
+                    blurred_v = custom_blur(tensor_4d, 1, k_blur, False, max_raw_value)
 
-        if is_single_file_mode:
-            # CRITICAL FIX: Get the finished folders directly from settings (set in start_single_processing)
-            single_finished_src = settings.get("single_finished_source_folder")
-            single_finished_depth = settings.get("single_finished_depth_folder")
+                if blurred_h is not None and blurred_v is not None:
+                    wsum = BLUR_LEFT_H_WEIGHT + BLUR_LEFT_V_WEIGHT
+                    blurred = (
+                        blurred_h * BLUR_LEFT_H_WEIGHT + blurred_v * BLUR_LEFT_V_WEIGHT
+                    ) / max(wsum, 1e-6)
+                elif blurred_h is not None:
+                    blurred = blurred_h
+                elif blurred_v is not None:
+                    blurred = blurred_v
+                else:
+                    blurred = tensor_4d
 
-            # --- Check move_enabled setting ---
-            if (
-                single_finished_src
-                and single_finished_depth
-                and move_enabled
-                and any_task_completed_successfully_for_this_video
-            ):
-                self._move_processed_files(
-                    video_path,
-                    actual_depth_map_path,
-                    single_finished_src,
-                    single_finished_depth,
+                tensor_4d = tensor_4d * (1.0 - alpha) + blurred * alpha
+            if abs(render_dilate_x) > 1e-5 or abs(render_dilate_y) > 1e-5:
+                tensor_4d = custom_dilate(
+                    tensor_4d,
+                    float(render_dilate_x),
+                    float(render_dilate_y),
+                    False,
+                    max_raw_value,
                 )
-            else:
-                logger.debug(
-                    f"Single file move skipped. Enabled={move_enabled}, Success={any_task_completed_successfully_for_this_video}, PathsValid={bool(single_finished_src)}"
+            if render_blur_x > 0 or render_blur_y > 0:
+                tensor_4d = custom_blur(
+                    tensor_4d,
+                    float(render_blur_x),
+                    float(render_blur_y),
+                    False,
+                    max_raw_value,
                 )
+            batch_depth_numpy_float = tensor_4d.squeeze(1).cpu().numpy()
+            release_cuda_memory()
 
-        elif (
-            any_task_completed_successfully_for_this_video
-            and finished_source_folder
-            and finished_depth_folder
-            and move_enabled
-        ):
-            # Batch mode move (uses the arguments passed from _run_batch_process)
-            self._move_processed_files(
-                video_path,
-                actual_depth_map_path,
-                finished_source_folder,
-                finished_depth_folder,
-            )
-
-        # Return the number of tasks actually processed for the global counter update
-        return processed_tasks_count, any_task_completed_successfully_for_this_video
+        return batch_depth_numpy_float
 
     def _preview_processing_callback(
         self, source_frames: dict, params: dict
@@ -6941,38 +4391,15 @@ class SplatterGUI(ThemedTk):
         return pil_img
 
     def reset_to_defaults(self):
-        """Resets all GUI parameters to their default hardcoded values."""
+        """Resets all GUI parameters to their default values using ConfigManager."""
         if not messagebox.askyesno(
             "Reset Settings",
             "Are you sure you want to reset all settings to their default values?",
         ):
             return
 
-        self.input_source_clips_var.set("./input_source_clips")
-        self.input_depth_maps_var.set("./input_depth_maps")
-        self.output_splatted_var.set("./output_splatted")
-        self.max_disp_var.set("20.0")
-        self.process_length_var.set("-1")
-        self.enable_full_res_var.set(True)
-        self.batch_size_var.set("10")
-        self.enable_low_res_var.set(False)
-        self.pre_res_width_var.set("1920")
-        self.pre_res_height_var.set("1080")
-        self.low_res_batch_size_var.set("50")
-        self.dual_output_var.set(False)
-        self.enable_global_norm_var.set(False)
-        self.zero_disparity_anchor_var.set("0.5")
-        self.enable_sidecar_gamma_var.set(True)
-        self.enable_sidecar_blur_dilate_var.set(True)
-
-        self.border_manual_var.set(False)
-        self.border_width_var.set("0.0")
-        self.border_bias_var.set("0.0")
-
-        self.output_crf_var.set("23")
-        self.output_crf_full_var.set("23")
-        self.output_crf_low_var.set("23")
-        self.move_to_finished_var.set(True)
+        from core.splatting.config_manager import reset_to_defaults
+        reset_to_defaults(self.__dict__, self.config_manager.defaults)
 
         # Ensure UI/Toggles match the new reset states
         self.toggle_processing_settings_fields()
@@ -7077,204 +4504,40 @@ class SplatterGUI(ThemedTk):
         except ValueError:
             pass
 
-    def _run_batch_process(self, settings):
+    def _run_batch_process(self, settings: ProcessingSettings):
         """
-        Batch processing entry point.
-
-        In multi-file mode:
-          - 'From' and 'To' are treated as 1-based indices into the *GUI list*
-            (self.previewer.video_list), i.e. the same numbers you see when
-            jumping between clips.
-        In single-file mode:
-          - The From/To fields are ignored and the single video is processed.
+        Batch processing entry point. Delegates to core BatchProcessor.
         """
-        try:
-            # --- 1. Basic setup (folder paths, discovered videos, etc.) ---
-            setup_result = self._setup_batch_processing(settings)
-            if "error" in setup_result:
-                logger.error(setup_result["error"])
-                messagebox.showerror("Batch Processing Error", setup_result["error"])
-                return
-
-            input_videos = setup_result["input_videos"]
-            is_single_file_mode = setup_result["is_single_file_mode"]
-            finished_source_folder = setup_result["finished_source_folder"]
-            finished_depth_folder = setup_result["finished_depth_folder"]
-
-            if not input_videos:
-                logger.error("No input videos found for processing.")
-                messagebox.showerror(
-                    "Processing Error", "No input videos found for processing."
-                )
-                return
-
-            # --- 2. Apply From/To range on the *preview list* when available ---
-            # In single-file mode, we always process the one file and ignore From/To.
-            if (
-                not is_single_file_mode
-                and hasattr(self, "previewer")
-                and getattr(self.previewer, "video_list", None)
-            ):
-                # The previewer list is what you see in the GUI (1/XXXX, 2/XXXX, ...).
-                available_entries = self.previewer.video_list
-                total_videos = len(available_entries)
-
-                # Defaults: full range
-                start_index_0 = 0  # 0-based
-                end_index_0 = total_videos  # exclusive
-
-                # Parse "From" (1-based in UI)
-                from_str = self.process_from_var.get().strip()
-                if from_str:
-                    try:
-                        from_val = int(from_str)
-                        if from_val > 0:
-                            # convert to 0-based, clamp to bounds
-                            start_index_0 = max(0, min(total_videos, from_val - 1))
-                    except ValueError:
-                        logger.warning(f"Invalid 'From' value '{from_str}', ignoring.")
-
-                # Parse "To" (1-based in UI, inclusive)
-                to_str = self.process_to_var.get().strip()
-                if to_str:
-                    try:
-                        to_val = int(to_str)
-                        if to_val > 0:
-                            # convert to exclusive end index, clamp, and ensure at least 1 video
-                            end_index_0 = max(
-                                start_index_0 + 1, min(total_videos, to_val)
-                            )
-                    except ValueError:
-                        logger.warning(f"Invalid 'To' value '{to_str}', ignoring.")
-
-                # Log the range in GUI-style indices if we're not using the full list
-                if start_index_0 > 0 or end_index_0 < total_videos:
-                    logger.info(
-                        f"Processing range: videos {start_index_0 + 1} to {end_index_0} "
-                        f"(out of {total_videos} total)"
-                    )
-
-                # Slice the preview list and build the actual video path list
-                selected_entries = available_entries[start_index_0:end_index_0]
-                sliced_videos = [
-                    entry.get("source_video")
-                    for entry in selected_entries
-                    if entry.get("source_video")
-                ]
-
-                input_videos = sliced_videos
-
-            else:
-                # Multi-file mode with no previewer/video_list: treat From/To as simple
-                # 1-based indices over the discovered input_videos list (old behavior).
-                # In *single-file* mode, we intentionally ignore From/To and leave
-                # input_videos unchanged so the current preview clip always runs.
-                if not is_single_file_mode:
-                    total_videos = len(input_videos)
-                    start_index_0 = 0
-                    end_index_0 = total_videos
-
-                    from_str = self.process_from_var.get().strip()
-                    if from_str:
-                        try:
-                            from_val = int(from_str)
-                            if from_val > 0:
-                                start_index_0 = max(0, min(total_videos, from_val - 1))
-                        except ValueError:
-                            logger.warning(
-                                f"Invalid 'From' value '{from_str}', ignoring."
-                            )
-
-                    to_str = self.process_to_var.get().strip()
-                    if to_str:
-                        try:
-                            to_val = int(to_str)
-                            if to_val > 0:
-                                end_index_0 = max(
-                                    start_index_0 + 1, min(total_videos, to_val)
-                                )
-                        except ValueError:
-                            logger.warning(f"Invalid 'To' value '{to_str}', ignoring.")
-
-                    if start_index_0 > 0 or end_index_0 < total_videos:
-                        logger.info(
-                            f"Processing range: videos {start_index_0 + 1} to {end_index_0} "
-                            f"(out of {total_videos} total)"
-                        )
-                    input_videos = input_videos[start_index_0:end_index_0]
-                # else: single-file mode -> From/To boxes are ignored on purpose
-
-            # After applying the range, make sure we still have something to do
-            if not input_videos:
-                logger.error(
-                    "No input videos left to process after applying From/To range."
-                )
-                messagebox.showerror(
-                    "Processing Error",
-                    "No input videos left to process after applying the From/To range.",
-                )
-                return
-
-            # --- 3. Determine total tasks for the progress bar ---
-            processing_tasks = self._get_defined_tasks(settings)
-            if not processing_tasks:
-                logger.error(
-                    "No processing tasks defined. Please enable at least one output resolution."
-                )
-                messagebox.showerror(
-                    "Processing Error",
-                    "No processing tasks defined. Please enable at least one output resolution.",
-                )
-                return
-
-            tasks_per_video = len(processing_tasks)
-            total_tasks = len(input_videos) * tasks_per_video
-            logger.info(
-                f"Total tasks to process: {total_tasks} "
-                f"({len(input_videos)} videos √ó {tasks_per_video} tasks each)"
-            )
-            self.progress_queue.put(("total", total_tasks))
-
-            overall_task_counter = 0
-
-            # --- 4. Main processing loop ---
-            for idx, video_path in enumerate(input_videos):
-                if self.stop_event.is_set():
-                    logger.info("==> Stopping processing due to user request")
-                    break
+        # Parse From/To range
+        from_idx = 0
+        to_idx = None
+        
+        from_str = self.process_from_var.get().strip()
+        if from_str:
+            try:
+                from_idx = int(from_str) - 1
+            except ValueError:
+                pass
 
-                # Delegates all per-video work to the helper
-                tasks_processed, _ = self._process_single_video_tasks(
-                    video_path=video_path,
-                    settings=settings,
-                    initial_overall_task_counter=overall_task_counter,
-                    is_single_file_mode=is_single_file_mode,
-                    finished_source_folder=finished_source_folder,
-                    finished_depth_folder=finished_depth_folder,
-                )
+        to_str = self.process_to_var.get().strip()
+        if to_str:
+            try:
+                to_idx = int(to_str)
+            except ValueError:
+                pass
 
-                overall_task_counter += tasks_processed
+        video_list = None
+        if hasattr(self, "previewer") and getattr(self.previewer, "video_list", None):
+            video_list = self.previewer.video_list
 
-        except Exception as e:
-            logger.error(
-                f"An unexpected error occurred during batch processing: {e}",
-                exc_info=True,
-            )
-            self.progress_queue.put(("status", f"Error: {e}"))
-            error_message = str(e)
-            self.after(
-                0,
-                lambda msg=error_message: messagebox.showerror(
-                    "Processing Error",
-                    f"An unexpected error occurred during batch processing: {msg}",
-                ),
-            )
-        finally:
-            release_cuda_memory()
-            self.progress_queue.put("finished")
-            self.after(0, self.clear_processing_info)
+        self.batch_processor.run_batch_process(
+            settings=settings,
+            from_index=from_idx,
+            to_index=to_idx,
+            video_list=video_list,
+        )
 
-    def run_fusion_sidecar_generator(self):
+    def run_fusion_sidecar_generator(self) -> None:
         """Initializes and runs the FusionSidecarGenerator tool."""
 
         # Use an external thread to prevent the GUI from freezing during the file scan
@@ -7710,7 +4973,6 @@ class SplatterGUI(ThemedTk):
 
         return max_total
 
-
     def run_estimate_dp_total_max(self):
         """Compute a quick sampled estimate of this clip's Max Total(D+P) and seed the overlay."""
         try:
@@ -7735,7 +4997,7 @@ class SplatterGUI(ThemedTk):
 
             if isinstance(params, dict):
                 conv = float(params.get("zero_disparity_anchor", self.zero_disparity_anchor_var.get()))
-                max_disp = float(params.get("max_disp", self.max_disp_var.get()))
+                max_disp = float(params.get("max_disparity", self.max_disp_var.get()))
                 gamma = float(params.get("depth_gamma", self.depth_gamma_var.get()))
             else:
                 conv = float(self.zero_disparity_anchor_var.get())
@@ -8104,7 +5366,6 @@ class SplatterGUI(ThemedTk):
                 "Save Error", f"Failed to save settings to {config_filename}:\n{e}"
             )
 
-
     def run_auto_pass(self) -> None:
         """Run AUTO-PASS over the preview list (or From/To range) without rendering."""
         if not getattr(self, "previewer", None) or not getattr(self.previewer, "video_list", None):
@@ -8485,11 +5746,9 @@ class SplatterGUI(ThemedTk):
             pass
 
     def _save_config(self):
-        """Saves current GUI settings to the default file."""
-        config = self._get_current_config()
-        config_filename = self.APP_CONFIG_DEFAULTS["DEFAULT_CONFIG_FILENAME"]
-        with open(config_filename, "w") as f:
-            json.dump(config, f, indent=4)
+        """Saves current GUI settings using the ConfigManager."""
+        self.config_manager.config = self._get_current_config()
+        self.config_manager.save()
 
     def _save_current_sidecar_data(
         self,
@@ -8715,19 +5974,19 @@ class SplatterGUI(ThemedTk):
         self._save_debug_image(data, filename_tag, batch_index, frame_index, task_name)
 
     def save_settings(self):
-        """Saves current GUI settings to a user-selected JSON file."""
+        """Saves current GUI settings to a user-selected JSON file using ConfigManager."""
         filename = filedialog.asksaveasfilename(
-            defaultextension=".json",
-            filetypes=[("JSON files", "*.json")],
+            defaultextension=".splatcfg",
+            filetypes=[("Splat Config", "*.splatcfg"), ("JSON files", "*.json")],
             title="Save Settings to File",
         )
         if not filename:
             return
 
         try:
+            from core.splatting.config_manager import save_settings_to_file
             config_to_save = self._get_current_config()
-            with open(filename, "w") as f:
-                json.dump(config_to_save, f, indent=4)
+            save_settings_to_file(config_to_save, filename)
 
             messagebox.showinfo(
                 "Settings Saved",
@@ -8847,94 +6106,9 @@ class SplatterGUI(ThemedTk):
         # Store the actual width that was applied (which is current_width) for save_config
         self.window_width = current_width  # Update instance variable for save_config
 
-    def _setup_batch_processing(self, settings):
-        """
-        Handles input path validation, mode determination (single file vs batch),
-        and creates necessary 'finished' folders.
-
-        Returns a dict:
-
-            {
-                "input_videos": [...],
-                "is_single_file_mode": bool,
-                "finished_source_folder": str or None,
-                "finished_depth_folder": str or None,
-            }
-
-        or, on error:
-
-            { "error": "message" }
-        """
-        input_source_clips_path = settings["input_source_clips"]
-        input_depth_maps_path = settings["input_depth_maps"]
-        output_splatted = settings["output_splatted"]
-
-        is_source_file = os.path.isfile(input_source_clips_path)
-        is_source_dir = os.path.isdir(input_source_clips_path)
-        is_depth_file = os.path.isfile(input_depth_maps_path)
-        is_depth_dir = os.path.isdir(input_depth_maps_path)
-
-        input_videos = []
-        finished_source_folder = None
-        finished_depth_folder = None
-        is_single_file_mode = False
-
-        if is_source_file and is_depth_file:
-            # Single-file mode
-            is_single_file_mode = True
-            logger.debug(
-                "==> Running in single file mode. Files will not be moved to "
-                "'finished' folders (unless specifically enabled in Single Process mode)."
-            )
-            input_videos.append(input_source_clips_path)
-            os.makedirs(output_splatted, exist_ok=True)
-
-        elif is_source_dir and is_depth_dir:
-            # Batch (folder) mode
-            logger.debug("==> Running in batch (folder) mode.")
-
-            if settings["move_to_finished"]:
-                finished_source_folder = os.path.join(
-                    input_source_clips_path, "finished"
-                )
-                finished_depth_folder = os.path.join(input_depth_maps_path, "finished")
-                os.makedirs(finished_source_folder, exist_ok=True)
-                os.makedirs(finished_depth_folder, exist_ok=True)
-                logger.debug("Finished folders enabled for batch mode.")
-            else:
-                logger.debug(
-                    "Finished folders DISABLED by user setting. "
-                    "Files will remain in input folders."
-                )
-
-            os.makedirs(output_splatted, exist_ok=True)
-
-            video_extensions = ("*.mp4", "*.avi", "*.mov", "*.mkv")
-            for ext in video_extensions:
-                input_videos.extend(
-                    glob.glob(os.path.join(input_source_clips_path, ext))
-                )
-            input_videos = sorted(input_videos)
-
-        else:
-            msg = (
-                "==> Error: Input Source Clips and Input Depth Maps must both be "
-                "either files or directories. Skipping processing."
-            )
-            logger.error(msg)
-            return {"error": msg}
-
-        if not input_videos:
-            msg = f"No video files found in {input_source_clips_path}"
-            logger.error(msg)
-            return {"error": msg}
-
-        return {
-            "input_videos": input_videos,
-            "is_single_file_mode": is_single_file_mode,
-            "finished_source_folder": finished_source_folder,
-            "finished_depth_folder": finished_depth_folder,
-        }
+    def _setup_batch_processing(self, settings: ProcessingSettings) -> BatchSetupResult:
+        """Handles input path validation and determine mode via core BatchProcessor."""
+        return self.batch_processor.setup_batch_processing(settings)
 
     def show_about(self):
         """Displays the 'About' message box."""
@@ -9101,42 +6275,10 @@ class SplatterGUI(ThemedTk):
             self.stop_button.config(state="disabled")
             return
 
-        settings = {
-            "input_source_clips": self.input_source_clips_var.get(),
-            "input_depth_maps": self.input_depth_maps_var.get(),
-            "output_splatted": self.output_splatted_var.get(),
-            "max_disp": float(self.max_disp_var.get()),
-            "process_length": int(self.process_length_var.get()),
-            "enable_full_resolution": self.enable_full_res_var.get(),
-            "full_res_batch_size": int(self.batch_size_var.get()),
-            "enable_low_resolution": self.enable_low_res_var.get(),
-            "low_res_width": int(self.pre_res_width_var.get()),
-            "low_res_height": int(self.pre_res_height_var.get()),
-            "low_res_batch_size": int(self.low_res_batch_size_var.get()),
-            "dual_output": self.dual_output_var.get(),
-            "zero_disparity_anchor": float(self.zero_disparity_anchor_var.get()),
-            "enable_global_norm": self.enable_global_norm_var.get(),  # Renamed
-            "match_depth_res": True,
-            "move_to_finished": self.move_to_finished_var.get(),
-            "output_crf": int(self.output_crf_full_var.get()),  # legacy
-            "output_crf_full": int(self.output_crf_full_var.get()),
-            "output_crf_low": int(self.output_crf_low_var.get()),
-            # --- Depth Pre-processing & Auto-Convergence Settings ---
-            "depth_gamma": depth_gamma_val,
-            "depth_dilate_size_x": depth_dilate_size_x_val,
-            "depth_dilate_size_y": depth_dilate_size_y_val,
-            "depth_blur_size_x": depth_blur_size_x_val,
-            "depth_blur_size_y": depth_blur_size_y_val,
-            "depth_dilate_left": float(self.depth_dilate_left_var.get()),
-            "depth_blur_left": int(round(float(self.depth_blur_left_var.get()))),
-            "auto_convergence_mode": self.auto_convergence_mode_var.get(),
-            "enable_sidecar_gamma": self.enable_sidecar_gamma_var.get(),
-            "enable_sidecar_blur_dilate": self.enable_sidecar_blur_dilate_var.get(),
-        }
-        self.processing_thread = threading.Thread(
-            target=self._run_batch_process, args=(settings,)
-        )
-        self.processing_thread.start()
+        settings = self._get_processing_settings()
+
+        # Start processing in a new thread
+        threading.Thread(target=self._run_batch_process, args=(settings,)).start()
         self.check_queue()
 
     def start_single_processing(self):
@@ -9292,43 +6434,12 @@ class SplatterGUI(ThemedTk):
                 f"Single Process: Finished folders set to: {single_finished_source_folder}"
             )
 
-        settings = {
-            # --- OVERRIDDEN INPUTS FOR SINGLE MODE ---
-            "input_source_clips": single_video_path,
-            "input_depth_maps": single_depth_path,
-            "output_splatted": self.output_splatted_var.get(),  # Use the batch output folder
-            # --- END OVERRIDE ---
-            "max_disp": float(self.max_disp_var.get()),
-            "process_length": int(self.process_length_var.get()),
-            "enable_full_resolution": self.enable_full_res_var.get(),
-            "full_res_batch_size": int(self.batch_size_var.get()),
-            "enable_low_resolution": self.enable_low_res_var.get(),
-            "low_res_width": int(self.pre_res_width_var.get()),
-            "low_res_height": int(self.pre_res_height_var.get()),
-            "low_res_batch_size": int(self.low_res_batch_size_var.get()),
-            "dual_output": self.dual_output_var.get(),
-            "zero_disparity_anchor": float(self.zero_disparity_anchor_var.get()),
-            "enable_global_norm": self.enable_global_norm_var.get(),  # Renamed
-            "match_depth_res": True,
-            "output_crf": int(self.output_crf_full_var.get()),  # legacy
-            "output_crf_full": int(self.output_crf_full_var.get()),
-            "output_crf_low": int(self.output_crf_low_var.get()),
-            # --- Depth Pre-processing Settings ---
-            "depth_gamma": float(self.depth_gamma_var.get()),
-            "depth_dilate_size_x": int(float(self.depth_dilate_size_x_var.get())),
-            "depth_dilate_size_y": int(float(self.depth_dilate_size_y_var.get())),
-            "depth_blur_size_x": int(float(self.depth_blur_size_x_var.get())),
-            "depth_blur_size_y": int(float(self.depth_blur_size_y_var.get())),
-            "depth_dilate_left": float(self.depth_dilate_left_var.get()),
-            "depth_blur_left": int(round(float(self.depth_blur_left_var.get()))),
-            "depth_blur_left_mix": float(self.depth_blur_left_mix_var.get()),
-            "auto_convergence_mode": self.auto_convergence_mode_var.get(),
-            "enable_sidecar_gamma": self.enable_sidecar_gamma_var.get(),
-            "enable_sidecar_blur_dilate": self.enable_sidecar_blur_dilate_var.get(),
-            "single_finished_source_folder": single_finished_source_folder,
-            "single_finished_depth_folder": single_finished_depth_folder,
-            "move_to_finished": self.move_to_finished_var.get(),
-        }
+        settings = self._get_processing_settings()
+        # Single-mode overrides
+        settings.input_source_clips = single_video_path
+        settings.input_depth_maps = single_depth_path
+        settings.single_finished_source_folder = single_finished_source_folder
+        settings.single_finished_depth_folder = single_finished_depth_folder
 
         # 4. Start the processing thread
         self.stop_event.clear()
@@ -9847,7 +6958,7 @@ def compute_global_depth_stats(
         if chunk_max > global_max:
             global_max = chunk_max
 
-        # draw_progress_bar(i + len(current_indices), total_frames, prefix="  Depth Stats:", suffix="Complete")
+        # draw_progress_bar(i + len(current_indices), total_frames, prefix="  Depth Stats:", suffix="Complete    ")
 
     logger.info(
         f"==> Global depth stats computed: min_raw={global_min:.3f}, max_raw={global_max:.3f}"
@@ -9855,323 +6966,12 @@ def compute_global_depth_stats(
     return float(global_min), float(global_max)
 
 
-def read_video_frames(
-    video_path: str,
-    process_length: int,
-    set_pre_res: bool,
-    pre_res_width: int,
-    pre_res_height: int,
-    dataset: str = "open",
-) -> Tuple[VideoReader, float, int, int, int, int, Optional[dict], int]:
-    """
-    Initializes a VideoReader for chunked reading.
-    Returns: (video_reader, fps, original_height, original_width, actual_processed_height, actual_processed_width, video_stream_info, total_frames_to_process)
-    """
-    if dataset == "open":
-        logger.debug(f"==> Initializing VideoReader for: {video_path}")
-        vid_info_only = VideoReader(
-            video_path, ctx=cpu(0)
-        )  # Use separate reader for info
-        original_height, original_width = vid_info_only.get_batch([0]).shape[1:3]
-        total_frames_original = len(vid_info_only)
-        logger.debug(
-            f"==> Original video shape: {total_frames_original} frames, {original_height}x{original_width} per frame"
-        )
-
-        height_for_reader = original_height
-        width_for_reader = original_width
-
-        if set_pre_res and pre_res_width > 0 and pre_res_height > 0:
-            height_for_reader = pre_res_height
-            width_for_reader = pre_res_width
-            logger.debug(
-                f"==> Pre-processing resolution set to: {width_for_reader}x{height_for_reader}"
-            )
-        else:
-            logger.debug(
-                f"==> Using original video resolution for reading: {width_for_reader}x{height_for_reader}"
-            )
-
-    else:
-        raise NotImplementedError(f"Dataset '{dataset}' not supported.")
-
-    # decord automatically resizes if width/height are passed to VideoReader
-    video_reader = VideoReader(
-        video_path, ctx=cpu(0), width=width_for_reader, height=height_for_reader
-    )
-
-    # Verify the actual shape after Decord processing, using the first frame
-    first_frame_shape = video_reader.get_batch([0]).shape
-    actual_processed_height, actual_processed_width = first_frame_shape[1:3]
-
-    fps = video_reader.get_avg_fps()  # Use actual FPS from the reader
-
-    total_frames_available = len(video_reader)
-    total_frames_to_process = total_frames_available  # Use available frames directly
-    if process_length != -1 and process_length < total_frames_available:
-        total_frames_to_process = process_length
-
-    logger.debug(
-        f"==> VideoReader initialized. Final processing dimensions: {actual_processed_width}x{actual_processed_height}. Total frames for processing: {total_frames_to_process}"
-    )
-
-    video_stream_info = get_video_stream_info(
-        video_path
-    )  # Get stream info for FFmpeg later
-
-    return (
-        video_reader,
-        fps,
-        original_height,
-        original_width,
-        actual_processed_height,
-        actual_processed_width,
-        video_stream_info,
-        total_frames_to_process,
-    )
-
-
-# -----------------------------
-# 10-bit+ depth decode helpers
-# -----------------------------
-class _NumpyBatch:
-    """Minimal wrapper to match Decord's get_batch(...).asnumpy() API."""
-
-    def __init__(self, arr: np.ndarray):
-        self._arr = arr
-
-    def asnumpy(self) -> np.ndarray:
-        return self._arr
-
-
-def _infer_depth_bit_depth(depth_stream_info: Optional[dict]) -> int:
-    """Best-effort bit-depth inference from ffprobe info."""
-    if not depth_stream_info:
-        return 8
-    pix_fmt = str(depth_stream_info.get("pix_fmt", "")).lower()
-    profile = str(depth_stream_info.get("profile", "")).lower()
-
-    # Common patterns: yuv420p10le, yuv444p12le, gray16le, etc.
-    m = re.search(r"(?:p|gray)(\d+)", pix_fmt)
-    if m:
-        try:
-            return int(m.group(1))
-        except Exception:
-            pass
-    if "main10" in profile or "10" in profile:
-        return 10
-    return 8
-
-
-def _build_depth_vf(pix_fmt: str, out_w: int, out_h: int) -> str:
-    pix_fmt = (pix_fmt or "").lower()
-    # If it's already gray*, don't try to extract planes.
-    if pix_fmt.startswith("gray"):
-        return f"scale={out_w}:{out_h}:flags=bilinear,format=gray16le"
-    # Otherwise, extract luma plane (Y) and convert to gray16.
-    return f"extractplanes=y,scale={out_w}:{out_h}:flags=bilinear,format=gray16le"
-
-
-class FFmpegDepthPipeReader:
-    """
-    Sequential ffmpeg-backed depth reader that preserves 10-bit+ values.
-    Implements a small subset of Decord's VideoReader API used by the splatter:
-      - __len__()
-      - seek(idx)
-      - get_batch([idx0, idx1, ...]).asnumpy()
-    NOTE: This reader is optimized for sequential access (render path).
-    """
-
-    def __init__(
-        self,
-        path: str,
-        out_w: int,
-        out_h: int,
-        bit_depth: int,
-        num_frames: int,
-        pix_fmt: str = "",
-    ):
-        self.path = path
-        self.out_w = int(out_w)
-        self.out_h = int(out_h)
-        self.bit_depth = int(bit_depth) if bit_depth else 16
-        self._num_frames = int(num_frames) if num_frames is not None else 0
-        self._pix_fmt = pix_fmt or ""
-        self._proc: Optional[subprocess.Popen] = None
-        self._next_index = 0
-        self._frame_bytes = self.out_w * self.out_h * 2  # gray16le
-        self._msb_shift: Optional[int] = None
-        self._use_16_to_n_scale: bool = False
-        self._start_process()
-
-    def __len__(self) -> int:
-        return self._num_frames
-
-    def _start_process(self):
-        self.close()
-        vf = _build_depth_vf(self._pix_fmt, self.out_w, self.out_h)
-        cmd = [
-            "ffmpeg",
-            "-hide_banner",
-            "-loglevel",
-            "error",
-            "-nostdin",
-            "-i",
-            self.path,
-            "-an",
-            "-sn",
-            "-dn",
-            "-vframes",
-            str(self._num_frames)
-            if self._num_frames and self._num_frames > 0
-            else "999999999",
-            "-vf",
-            vf,
-            "-f",
-            "rawvideo",
-            "pipe:1",
-        ]
-        self._proc = subprocess.Popen(
-            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
-        )
-        self._next_index = 0
-        self._msb_shift = None
-
-    def close(self):
-        try:
-            if self._proc is not None:
-                if self._proc.stdout:
-                    try:
-                        self._proc.stdout.close()
-                    except Exception:
-                        pass
-                if self._proc.stderr:
-                    try:
-                        self._proc.stderr.close()
-                    except Exception:
-                        pass
-                try:
-                    self._proc.terminate()
-                except Exception:
-                    pass
-        finally:
-            self._proc = None
-
-    def __del__(self):
-        self.close()
-
-    def seek(self, idx: int):
-        """Best-effort seek. Efficient only if seeking forward sequentially."""
-        idx = int(idx)
-        if idx == self._next_index:
-            return
-        if idx < self._next_index:
-            # Restart and fast-forward by discarding frames (rare in render path).
-            self._start_process()
-        # Discard until we reach idx
-        to_skip = idx - self._next_index
-        if to_skip <= 0:
-            self._next_index = idx
-            return
-        if self._proc is None or self._proc.stdout is None:
-            self._start_process()
-        discard_bytes = to_skip * self._frame_bytes
-        _ = self._proc.stdout.read(discard_bytes)
-        self._next_index = idx
-
-    def _read_exact(self, nbytes: int) -> bytes:
-        if self._proc is None or self._proc.stdout is None:
-            self._start_process()
-        buf = b""
-        while len(buf) < nbytes:
-            chunk = self._proc.stdout.read(nbytes - len(buf))
-            if not chunk:
-                break
-            buf += chunk
-        return buf
-
-    def _maybe_apply_shift(self, arr_u16: np.ndarray) -> np.ndarray:
-        """Normalize decoded gray16 values back into the native bit-depth range (e.g., 0..1023 for 10-bit).
-
-        ffmpeg's format conversion to gray16le may either:
-          1) left-shift the original N-bit values into the MSBs (exact multiples of 2^(16-N)), or
-          2) scale values to the full 16-bit range (0..65535).
-
-        We detect the likely behavior once and then apply either a right-shift or a scale-down so the
-        output values land back in 0..(2^bit_depth-1).
-        """
-        if self._msb_shift is None:
-            expected_max = (
-                (1 << self.bit_depth) - 1 if 0 < self.bit_depth < 16 else None
-            )
-            if expected_max is None:
-                self._msb_shift = 0
-                self._use_16_to_n_scale = False
-            else:
-                # Sample to avoid full-frame scans on large batches
-                flat = arr_u16.reshape(-1)
-                step = max(1, flat.size // 50000)
-                sample = flat[::step]
-                max_val = int(sample.max(initial=0))
-
-                if max_val <= expected_max:
-                    self._msb_shift = 0
-                    self._use_16_to_n_scale = False
-                else:
-                    shift = 16 - self.bit_depth
-                    if shift <= 0:
-                        self._msb_shift = 0
-                        self._use_16_to_n_scale = False
-                    else:
-                        low_mask = (1 << shift) - 1
-                        # If the sample's low bits are consistently zero, it's very likely MSB-aligned (pure left shift).
-                        low_bits_max = int((sample & low_mask).max(initial=0))
-                        if low_bits_max == 0:
-                            self._msb_shift = shift
-                            self._use_16_to_n_scale = False
-                        else:
-                            # Otherwise assume scaled-to-16bit; scale back down into expected range.
-                            self._msb_shift = 0
-                            self._use_16_to_n_scale = True
-
-        expected_max = (1 << self.bit_depth) - 1 if 0 < self.bit_depth < 16 else None
-        if expected_max is None:
-            return arr_u16
-
-        if self._msb_shift and self._msb_shift > 0:
-            return (arr_u16 >> self._msb_shift).astype(np.uint16)
-
-        if self._use_16_to_n_scale:
-            # Map 0..65535 -> 0..expected_max with rounding.
-            arr32 = arr_u16.astype(np.uint32)
-            return ((arr32 * expected_max + 32767) // 65535).astype(np.uint16)
-
-        return arr_u16
-
-    def get_batch(self, indices):
-        indices = list(indices)
-        if not indices:
-            return _NumpyBatch(
-                np.zeros((0, self.out_h, self.out_w, 1), dtype=np.uint16)
-            )
-
-        # Render path calls are contiguous and increasing. Enforce best-effort alignment.
-        first = int(indices[0])
-        if first != self._next_index:
-            self.seek(first)
-
-        n = len(indices)
-        expected = self._frame_bytes * n
-        buf = self._read_exact(expected)
-        if len(buf) != expected:
-            raise EOFError(
-                f"FFmpegDepthPipeReader: expected {expected} bytes, got {len(buf)}"
-            )
-
-        arr = np.frombuffer(buf, dtype=np.uint16).reshape(n, self.out_h, self.out_w, 1)
-        arr = self._maybe_apply_shift(arr)
-        self._next_index = first + n
-        return _NumpyBatch(arr.copy())
+# [REFACTORED] Depth processing functions replaced with core imports
+from core.splatting.depth_processing import (
+    _infer_depth_bit_depth,
+    _build_depth_vf,
+    FFmpegDepthPipeReader,
+)
 
 
 def load_pre_rendered_depth(
